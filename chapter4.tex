% !TeX root = main.tex
%===================================== CHAP 4 =================================

\chapter{B-spline MINMPC}\label{chap:b-spline-minmpc}

\section{Dynamic Model}
\subsection{Double Integrator}\label{sec:double-integrator}
The double integrator model is a simple model that ensures a continuous position, velocity, and acceleration for the ship. The model is described by the following equations:
\begin{subequations}\label{eq:double-integrator}
    \begin{align}
        \dot{\mathbf{p}}(t) &= \mathbf{v}(t), \label{eq:double-integrator-x} \\
        \dot{\mathbf{v}}(t) &= \mathbf{a}(t), \label{eq:double-integrator-v} \\
        \|\mathbf{v}(t)\|_2 &\leq v_{\max}, \label{eq:double-integrator-vmax} \\
        \|\mathbf{a}(t)\|_2 &\leq a_{\max}, \label{eq:double-integrator-a}
    \end{align}
\end{subequations}
where $\mathbf{p} = [p_N, p_E]^\top$ denotes the position of the ship, $\mathbf{v} = [v_N, v_E]^\top$ is the velocity, and $\mathbf{a} = [a_N, a_E]^\top$ is the acceleration in North-East coordinates. The $\|\cdot\|_2$ notation denotes the Euclidean norm, and $v_{\max}$ and $a_{\max}$ are the maximum speed and acceleration of the ship, respectively.

This continuous model can be relaxed to a B-spline model by letting $\mathbf{p}(x)$, $\mathbf{v}(x)$, and $\mathbf{a}(x)$ be spline functions on a chosen B-spline basis. It is desirable to let time be a variable in the optimization problem so that minimum time trajectories can be found. It is impractical to let the knot values of the B-spline representation of $\mathbf{p}(x)$ be optimization variables as an analysis of the Cox-de Boor recursion formula in \cref{eq:b-spline-recurrence} shows that the spline function is non-linear in these knot values. More specifically a degree $p$ B-spline with the parameter $x$ fixed, will be a polynomial of degree $p$ in the knot values. Instead, the time variable $t$ is introduced as a function of the parameter $x$, which is a common approach in these types of problems \citep{mercy2017spline,ShortestPathsConvexSets}. 

With this approach, the velocity and acceleration can now be expressed as
\begin{subequations}\label{eq:double-integrator-spline}
    \begin{align}
        \mathbf{v}(t) &= \frac{\partial \mathbf{p}}{\partial x} \frac{\partial x}{\partial t} = \frac{\mathbf{p}'}{t'},
        \label{eq:double-integrator-v-spline} \\
        \mathbf{a}(t) &= \frac{\partial \mathbf{v}}{\partial t} = \frac{\partial}{\partial t} \left(\frac{\mathbf{p}'}{t'}\right) = 
        \frac{
            t'\frac{\partial \mathbf{p}'}{\partial t} - \mathbf{p}'\frac{\partial t'}{\partial t}
            }{(t')^2} = 
        \frac{\mathbf{p}'' - \mathbf{p}'\frac{t''}{t'}}{(t')^2}.
        \label{eq:double-integrator-a-spline}
    \end{align}
\end{subequations}
Here $(\cdot)'$ and $(\cdot)''$ denotes the first and second derivative with respect to the spline parameter $x$, respectively. In \cref{eq:double-integrator-spline}, the inverse function theorem was used to flip the relation between $t$ and $x$. This is only possible if $t$ is a strictly increasing function of $x$, which is the case if the constraint 
\begin{equation}\label{eq:constraint-t}
    t'(x) > 0
\end{equation}
holds.  
If we in addition require the time to be positive, the velocity constraint in \cref{eq:double-integrator-vmax} becomes
\begin{equation}\label{eq:double-integrator-vmax-spline}
    \begin{aligned}
        &&\|\mathbf{v}(t)\|_2 = \left\|\frac{\mathbf{p}'}{t'}\right\|_2 &\leq v_{\max} \\
        &\implies& \|\mathbf{p}'(x)\|_2 &\leq t'(x)v_{\max} \\
        &\implies& \mathbf{p}'(x)\cdot\mathbf{p}'(x) &\leq (t'(x))^2v_{\max}^2.
    \end{aligned}
\end{equation}
To simplify the model, yet another restriction is put on the time function $t(x)$ by requiring it to be a 1st degree B-spline function. This means that the time variable is a piecewise linear function of the parameter $x$, and the second derivative of $t$ is zero, giving the simplified acceleration constraint
\begin{equation}\label{eq:double-integrator-a-spline-simplified}
    \begin{aligned}
        &&\|\mathbf{a}(t)\|_2 = \left\|\frac{\mathbf{p}''}{(t')^2}\right\|_2 &\leq a_{\max} \\
        &\implies& \|\mathbf{p}''(x)\|_2 &\leq t'(x)^2a_{\max} \\
        &\implies& \mathbf{p}''(x)\cdot\mathbf{p}''(x) &\leq t'(x)^4a_{\max}^2.
    \end{aligned}
\end{equation}

Now the B-Spline relaxation is fully given by the following equations:
\begin{subequations}\label{eq:double-integrator-spline-complete}
    \begin{align}
        \mathbf{p}(x) &= \sum_{i=0}^{n} \mathbf{p}_i B_{i,p,\mathbf{t}}(x), \label{eq:double-integrator-spline-complete-p} \\
        t(x) &= \sum_{i=0}^{m} t_i B_{i,1,\boldsymbol{\tau}}(x), \label{eq:double-integrator-spline-complete-t} \\
        \mathbf{p}'(x) \cdot \mathbf{p}'(x) &\leq t'(x)^2v_{\max}^2, \label{eq:double-integrator-spline-complete-vmax} \\
        \mathbf{p}''(x) \cdot \mathbf{p}''(x) &\leq t'(x)^4a_{\max}^2, \label{eq:double-integrator-spline-complete-a} \\
        t(x) &\geq 0, \label{eq:double-integrator-spline-complete-t-constraint} \\
        t'(x) &> 0. \label{eq:double-integrator-spline-complete-t-derivative} \end{align}
\end{subequations}
The only optimization variables needed are the B-Spline coefficients $\mathbf{p}_i\in\mathbb{R}^2$ and $t_i\in\mathbb{R}^+$ from \cref{eq:double-integrator-spline-complete-p,eq:double-integrator-spline-complete-t}, which are subject to the constraints \cref{eq:double-integrator-spline-complete-vmax,eq:double-integrator-spline-complete-a,eq:double-integrator-spline-complete-t-constraint,eq:double-integrator-spline-complete-t-derivative}.


\subsection{Dubins Model}\label{sec:dubins-model}
\todo{Legg til tid på samme måte som i dobbeltintegratormodellen}

The double integrator model is a simple model that ensures a continuous position, velocity, and acceleration for the ship. However, it does not take into account the ship's heading and turning radius. Dubins model (also referred to as the unicycle model) is a more realistic model for ship dynamics, as it describes a vehicle moving in a plane where the velocity points in the direction of the heading. In the motion control literature, there exists a variety of parameterizations for this kind of model. Notably \citet{mercy2017spline}, which uses the tangent half-angle substitution to represent the heading angle in a B-spline framework and \citet{Wang2020}, which uses a convex relaxation technique with sequential convex programming to exchange nonlinear equality constraints with convex inequalities. 

The model in \cite{Wang2020} technique was discarded as the assumtions for the relaxation technique are not valid under a reference following scenario. \cite{mercy2017spline} used the tangent half-angle to parameterize the bicycle model, which also includes the steering angle as a control input. In this work, the simplified Dubins model was tested, but ultimately discarded as the Pythogerean-hodograph (PH) B-spline model has the same properties as the tangent half-angle model, but has in addition all the nice properties described in \cref{sec:pythogerean-hodograph}. See \cref{app:tangent-half-angle} for a more detailed description of the Dubins model and the tangent half-angle substitution.

Dubins model can be described by the following equations:
\begin{subequations}\label{eq:dubins-model}
    \begin{align}
        \dot p_N &= V \cos(\chi),       \label{eq:dubins-x} \\
        \dot p_E &= V \sin(\chi),       \label{eq:dubins-y} \\
        \dot \chi &= \omega,          \label{eq:dubins-chi} \\
        |\omega| &\leq \omega_{\max}, \label{eq:dubins-omega} \\
        0 \leq V &\leq V_{\max},      \label{eq:dubins-V} 
    \end{align}
\end{subequations}
where $p_N$ and $p_E$ are the North and East position coordinates of the ship, and $\chi$ is the heading angle. The control input is the speed $V$ and turn rate $\omega$. Using the PH B-spline model from \cref{sec:pythogerean-hodograph}, the Dubins model can be expressed as
\begin{subequations}\label{eq:dubins-model-ph}
    \begin{align}
        \dot p_N(x) &= u(x)^2 - v(x)^2, \label{eq:dubins-ph-x} \\
        \dot p_E(x) &= 2u(x)v(x),       \label{eq:dubins-ph-y} \\
        \dot \chi(x) &= \omega(x) = \frac{2\big(u(x)v'(x) - u'(x)v(x)\big)}{u(x)^2 + v(x)^2},  \label{eq:dubins-ph-chi} \\
        |\omega(x)| &\leq \omega_{\max},  \\
        V(x) &= u(x)^2 + v(x)^2 \\
        0 \leq V(x) &\leq V_{\max}, 
    \end{align}
\end{subequations}
where \cref{eq:dubins-ph-x,eq:dubins-ph-y} correspond to \cref{eq:hodograph-derivatives-a,eq:hodograph-derivatives-b} and \cref{eq:dubins-ph-chi} follows from \cref{eq:turn-rate}.

To implement this model in an optimization framework, $u(x)$ and $v(x)$ are represented as B-spline functions, and the turn-rate constraints are derived as follows
\begin{equation*}
    \begin{aligned}
        |\omega(x)| &\le \omega_{\max} \\
        \implies \left|\frac{2\big(u(x)v'(x) - u'(x)v(x)\big)}{u(x)^2 + v(x)^2}\right| &\le \omega_{\max} \\
        \implies \left|u(x)v'(x) - u'(x)v(x)\right| &\le \frac{\omega_{\max}}{2}\left(u(x)^2 + v(x)^2\right),
    \end{aligned}
\end{equation*}
yeilding the polynomial constraints
\begin{subequations}\label{eq:dubins-ph-turn-rate}
    \begin{align}
        u(x)v'(x) - u'(x)v(x) &\le \frac{\omega_{\max}}{2}\left(u(x)^2 + v(x)^2\right), \\
        u(x)v'(x) - u'(x)v(x) &\ge -\frac{\omega_{\max}}{2}\left(u(x)^2 + v(x)^2\right).
    \end{align}
\end{subequations}

The constraint in \cref{eq:dubins-ph-chi} is a rational B-spline function and could in theory be implemented as a NURBS directly in an optimization framework. However, this is not done as unlike the bilinear terms in \cref{eq:dubins-ph-turn-rate}, the rational term in \cref{eq:dubins-ph-chi} has a singularity when $u(x)$ and $v(x)$ are both zero.


\section{Target Ships}
\todo{skriv introduksjon til dette avsnittet}

\subsection{Hyperplane Separation Theorem}
The standard way to enforce collision constraints between the OS and TS is to apply a minumum distance constraint between the two ships as
\begin{equation}\label{eq:minimum-distance}
    (\mathbf p_{\text{OS}} - \mathbf p_{\text{TS}}) \cdot (\mathbf p_{\text{OS}} - \mathbf p_{\text{TS}}) \geq d_{\text{min}}^2,
\end{equation}
where $\mathbf p_{\text{OS}}$ and $\mathbf p_{\text{TS}}$ are the positions of the OS and TS, respectively, and $d_{\text{min}}$ is the minimum distance between the two ships. Examples of this type of constraint can be found in \cite{Thyri2022-MPC}.\todo{flere kilder} This constraint is non-convex, as the feasible region for the OS lies outside the circle with radius $d_{\text{min}}$ centered at the TS.

Another method of enforcing collision avoidance is to use the hyperplane separation theorem. Although still non-convex, this method allows for a more flexible formulation of the constraint, as it is not only restricted to circular shapes. 
The hyperplane separation theorem states that for two disjoint convex sets $\mathcal A$ and $\mathcal B$, there exists $\mathbf n\in \mathbb R^n\backslash\{\mathbf0\}$ and $b\in\mathbb R$ such that the hyperplane $H=\{\mathbf x\in\mathbb R^n \mid \mathbf n^\top \mathbf x = b\}$ separates $\mathcal A$ and $\mathcal B$ \citep{Boyd2004-ih}. In other words, there exists a function $a^\top x - b$ that is non-negative for all $x \in \mathcal A$ and non-positive for all $x \in \mathcal B$. 

Using this theorem, the collision avoidance task can be transformed to a classification problem. The objective is to find a line that separates the OS and TS, such that the distance between the line and the ships is greater than a given threshold. This is done in the B-spline framework by introducing a hyperplane $\mathcal H$ with normal $\mathbf n(t)\in\mathbb{S}^2_{p,\mathbf t}$ and offset $b(t)\in\mathbb{S}_{p,\mathbf t}$, which separates the OS and TS. This hyperplane is implemented using the following constraints:
\begin{subnumcases}{\label{eq:minimum-distance-hyperplane}\mathcal{H}:}
    \mathbf p_\text{OS}(t) \cdot{\mathbf n}(t) & $\ge b(t) + d_\text{OS}$,
    \label{eq:hyperplane-os} \\
    \mathbf p_\text{TS}(t) \cdot{\mathbf n}(t) & $\le b(t) - d_\text{TS}$,
    \label{eq:hyperplane-ts} \\
    \|{\mathbf n}(t)\|_\infty & $\le 1$.
    \label{eq:hyperplane-norm}
\end{subnumcases}
The constraints in \zcref{eq:hyperplane-os,eq:hyperplane-ts} are enforced by
letting $\mathbf n(t)$ and $b(t)$ be optimization variables. Equation \zcref{eq:hyperplane-norm} is a box constraint on $\mathbf n(t)$, ensuring that the hyperplane normal doesn't become too large. The minimum distance constraint in \cref{eq:minimum-distance} can be equivalently expressed using the hyperplane separation theorem by letting $d_\text{OS} = 0$ and $d_\text{TS} = d_\text{min}$ in \zcref{eq:hyperplane-os,eq:hyperplane-ts}. As the following section shows,

\subsection{Collision Constraints}\label{sec:collision-constraints}

Collision avoidance is non-convex: with fixed start/end points one cannot deform a trajectory from one side of the obstacle to the other without intersection. A convex solver will therefore remain on the same side (see \cref{fig:non-convex-obstacle}), potentially missing the globally optimal path.
\begin{figure}
    \centering
    \includesvg[width=0.8\textwidth]{fig/b-spline/non-convex-obstacle.svg}
    \caption{Black dots: fixed start/end points. Dotted line: current trajectory. Solid line: reference. Red circle: obstacle. A convex solver cannot switch sides without violating the obstacle constraint.}
    \label{fig:non-convex-obstacle}
\end{figure}

For designing a COLREGS-compliant trajectory, being able to decide which side of a given target ship to pass on is essential. So to address the aforementioned issue, which side to pass the obstacle on is made a decision variable in a mixed integer programming (MIP) problem as follows:

The idea is that each target ship is represented by two obstacles, each obstacle having only one feasible side to be passed on. The passing side of the target ship is enforced by adding additional points to each obstacle on the opposide side of the passing side using the normal $\mathbf{\hat n}_\text{ref}$ of the reference trajectory $\mathbf p_\text{ref}$. The two obstacles are then moved into and out of the feasible region by introducing a binary variable $z_j\in\{0,1\}$ for each target ship $j$ which controls an offset $M \mathbf{\hat n}_\text{ref}$, where $M$ is a large positive number. More formally, the constraints are given by

\begin{subnumcases}{\label{eq:colregs-bigM}\mathcal{O}_j:}
    \mathbf{n}_{p,j}(t)\cdot\mathbf{p}_{\mathrm{OS}}(t)
      & \label{eq:colregs-bigM-os-p}$\ge b_{p,j}(t) + d_{\text{OS}}$\\
    \mathbf{n}_{p,j}(t)\cdot\mathbf{p}_{\mathrm{TS},j}(t)
      & \label{eq:colregs-bigM-ts-p}$\le b_{p,j}(t) - d_{\text{TS},j} + M\,z_j$\\
    \mathbf{n}_{p,j}(t)\cdot\bigl(\mathbf{p}_{\mathrm{TS},j}(t)+2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)\bigr)
      & \label{eq:colregs-bigM-ts-o-p}$\le b_{p,j}(t) - M$\\[1.5ex]
      %
    \mathbf{n}_{s,j}(t)\cdot\mathbf{p}_{\mathrm{OS}}(t)
      & \label{eq:colregs-bigM-os-s}$\ge b_{s,j}(t) + d_{\text{OS}}$\\
    \mathbf{n}_{s,j}(t)\cdot\mathbf{p}_{\mathrm{TS},j}(t)
      & \label{eq:colregs-bigM-ts-s}$\le b_{s,j}(t) - d_{\text{TS},j} + M\,(1-z_j)$\\
    \mathbf{n}_{s,j}(t)\cdot\bigl(\mathbf{p}_{\mathrm{TS},j}(t)-2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)\bigr)
      & \label{eq:colregs-bigM-ts-o-s}$\le b_{s,j}(t) - M$\\[1.5ex]
    z_j \in \{0,1\} & \label{eq:colregs-bigM-z} \\
    \|\mathbf{n}_{p,j}(t)\|_\infty\le 1 & \label{eq:colregs-bigM-np} \\
    \|\mathbf{n}_{s,j}(t)\|_\infty\le 1 & \label{eq:colregs-bigM-ns}
\end{subnumcases}
    
Here,
\begin{itemize}
    \item $\mathbf{n}_{p,j}(t)$ and $b_{p,j}(t)$ are the port‐side hyperplane normal and offset,
    \item $\mathbf{n}_{s,j}(t)$ and $b_{s,j}(t)$ are the starboard‐side normal and offset,
    \item $d_{\text{OS}}$ and $d_{\text{TS},j}$ are the required minimum separations for the OS and TS from the hyperplane, 
    \item $z_j\in\{0,1\}$ selects port‐side ($z_j=0$) or starboard‐side ($z_j=1$) passage,
    \item $\mathbf{p}_{\mathrm{TS},j}(t)+2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)$ and $\mathbf{p}_{\mathrm{TS},j}(t)-2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)$ are the two obstacles on the opposite side of the passing side,
    \item $M$ is a large constant that renders inactive constraints nonbinding.
    \item and $\mathbf{\hat n}_{\mathrm{ref}}(t)$ is the unit normal of the reference trajectory $\mathbf p_\text{ref}(t)$, defined as
    \begin{equation}\label{eq:reference-normal}
        \mathbf{\hat n}_{\mathrm{ref}}(t) = \mathbf R\frac{\mathbf p_\text{ref}'(t)}{\|\mathbf p_\text{ref}'(t)\|_2}.
    \end{equation}
    where $\mathbf R$ is a rotation matrix that rotates 90 degrees counterclockwise around the down axis in a North-East-Down coordinate system.
\end{itemize}
The first three inequalities
(\zcref{eq:colregs-bigM-os-p,eq:colregs-bigM-ts-p,eq:colregs-bigM-ts-o-p})
apply when passing on the port side, while the last three
(\zcref{eq:colregs-bigM-os-s,eq:colregs-bigM-ts-s,eq:colregs-bigM-ts-o-s})
enforce the starboard‐side conditions.


Including all ships’ constraints $\mathcal{O}_j$ guarantees that a passing side is chosen for each TS and that the required separation is maintained.
    
In the NLP relaxation of the MINLP (letting $z_j$ be continuous), there is now a way to continuously move the trajectory from one side of the obstacle to the other without violating any constraints. To be clear, all of the obstacles and points are active at the same time, but with a large enough chosen constant $M$, the inactive obstacles are so far away from the trajectory that they do not affect the optimization problem. This strategy is commonly referred to as the big-M method in Mixed Integer Programming (MIP) literature \citep{gan2012adaptive,Cococcioni2020}.

\begin{figure}
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\scriptsize]{fig/b-spline/non-convex-obstacle-mi-sb.svg}
        \caption{Starboard maneuver ($z_j=1$)}
        \label{fig:non-convex-obstacle-mi-sb}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\scriptsize]{fig/b-spline/non-convex-obstacle-mi-port.svg}
        \caption{Port maneuver ($z_j=0$)}
        \label{fig:non-convex-obstacle-mi-port}
    \end{subfigure}
    \caption{Visualization of the Big–$M$ collision-avoidance constraints in \cref{eq:colregs-bigM}: the top panel shows a starboard maneuver and the bottom panel a port maneuver. The red/orange polygon represents the target ship (TS) and the blue polygon the own ship (OS). Grey areas indicate infeasible regions (not to scale). The solid line denotes the reference trajectory, while the dashed line depicts the planned trajectory under the current optimization variables.}
    \label{fig:non-convex-obstacle-mi}
\end{figure}



\section{Objective Function}
The goals of the objective function should be 1) to find a safe, COLREGS-compliant trajectory, and 2) to follow a reference trajectory. These objectives are conflicting as rule 8b) of the COLREGS (see \cref{sec:colregs}) states that any alteration of course should be large enough to be readily apparent to another vessel. Following a reference trajectory on the other hand often involves the squared error between the vessel and the reference trajectory, which inherently penalizes sudden deviations in course. 
Thus a careful choice of the objective function is important to ensure robust and predictable behaviour. 
The following section will examine the behavior of the B-spline method when applied to a basic reference-following task. The subsequent section will then address COLREGS considerations.


\subsection{Reference Following Metric}
\label{sec:reference-following-metric}


\begin{figure}
    \centering
    \includesvg[width=0.8\textwidth]{fig/illustrations/cross-track- and along-track- error.svg}
    \caption{Cross-track error and total track error for a vessel trying its best to follow a reference trajectory. Figure adapted from \cite{prosjektoppgave}.}
    \label{fig:cross-track-along-track-error}
\end{figure}

Literature on motion planning distinguishes between path following, where only geometric relations are considered, and trajectory following, which considers both geometric and temporal relations. 
Path following is often expressed in terms of the cross-track error (XTE) \citep{Fossen2011-Handbook}. The XTE is the closest distance between the \acrshort{OS} and the reference path. In reactive methods such as pure pursuit and line-of-sight guidance, the XTE is used to determine the steering angle of the \acrshort{OS} at any given time \citep{Fossen2011-Handbook}. 
Using these costs in motion planning problems, where the entire trajectory is considered, is not straightforward. Depending on the geometry of the reference trajectory, the XTE can be a non-convex function of the OS position. This also holds true in the B-spline framework, where numerical search methods are generally needed to compute the closest distance from a point to a spline \citep{johnson2005distance,hu2005second,chen2009computing}.

The \acrfull{TTE} will now be defined as the distance between a time parameterized OS and a time parameterized reference trajectory. This is a more practical cost function, as now only a distance between two points is considered at any given time. The TTE between the position of the OS $\mathbf p_\text{OS}(t)$ and the position of the reference trajectory $\mathbf p_\text{ref}(t)$ is given by
\begin{equation}\label{eq:total-track-error}
    \mathbf e(t) = \mathbf p_\text{OS}(t) - \mathbf p_\text{ref}(t).
\end{equation}
A limitation of employing the TTE as a cost function arises when the OS lags the reference trajectory, such as during collision avoidance maneuvers. In such instances, the OS tends to converge towards a point ahead of the geometrically closest point on the reference trajectory, resulting in prolonged convergence times. 

A reference‐following metric that balances geometric and temporal alignment is the pseudo cross–track error (PXTE), defined by
\begin{equation}\label{eq:pxte}
  \bar e(t)
  = \mathbf e(t)^\top
    \mathbf{\hat n}_{\mathrm{ref}}(t),
\end{equation}
where
\begin{equation}\label{eq:reference-normal-pxte}
  \mathbf{\hat n}_{\mathrm{ref}}(t)
  = \mathbf R\,\frac{\mathbf p_{\mathrm{ref}}'(t)}{\sqrt{\mathbf p_{\mathrm{ref}}'(t) \cdot \mathbf p_{\mathrm{ref}}'(t)}}
\end{equation}
is the unit normal to the reference path (with $\mathbf R$ a $+\pi/2$ rotation).  On straight segments \cref{eq:pxte} reduces to the standard XTE, yet---unlike the XTE---remains closed‐form and differentiable.


In classical optimal control with time grids, a straightforward approach to design an objective function based on the PXTE is to minimize the sum of the squared PXTE at each time step, i.e., to minimize
\begin{equation}\label{eq:total-track-error-time-grid}
    J_\text{ref} = \sum_{k=0}^{n} \bar e(t_k)^2,
\end{equation}
where $t_k$ is the time at the $k$-th time step and $n$ is the number of time steps.

A similar approach can be used in the B-spline framework, where both the OS and the reference trajectory are represented as B-spline functions. The resulting PXTE is a NURBS, as it is a rational function of two B-splines. The natural extension of \cref{eq:total-track-error-time-grid} to the B-spline framework is then to minimize the integral of the squared PXTE over the parameter domain of the B-spline functions, i.e., to minimize
\begin{equation}\label{eq:total-track-error-b-spline}
J_\text{ref} = \int_\mathbb R \bar e(x)^2 \, dx.
\end{equation}
This integral, however, does not yield a rational function, as the integral of a rational polynomial function includes $\log$ and $\arctan$ functions in its solution. Instead, this integral can be approximated by dividing each coefficient of $\mathbf p'_\text{ref}(x)$ by the average speed over each basis function's support. This gives an approximate normalized normal vector of the reference trajectory at parameter $x$:
\begin{equation}\label{eq:approx-reference-normal-pxte}
    \mathbf{\tilde n}_{\mathrm{ref}}(x) = \sum_{i=0}^{N} \frac{\mathbf c_\text{ref,i} }{\sqrt{\mathbf p_\text{ref}'(\xi_i)^\top \mathbf p_\text{ref}'(\xi_i)}}B_{\mathrm{ref},i}^\top(x).
\end{equation}
Here $\mathbf c_\text{ref,i}$ are the control points of the reference trajectory B-spline, $B_{\mathrm{ref},i}(x)$ is the $i$-th basis function in the $N$-element basis, and $\xi_i$ is the greville abscissa of the $i$-th basis function. Replacing $\mathbf{\hat n}_{\mathrm{ref}}(t)$ in \cref{eq:pxte} with $\mathbf{\tilde n}_{\mathrm{ref}}(x)$, the integral in \cref{eq:total-track-error-b-spline} reduces to a B-spline function, which can easily be used as a cost function in the B-spline optimization framework. 
This approximiation has the advantage that it retains the property of being equivalent to the XTE on straight segments, while the accuracy on curved segments depends on the change in the curvature of the reference trajectory over the supports of the basis functions in that particular segment.


\subsection{Reference Following Weighting}\label{sec:oscillations}
During the development of the B-spline framework, it was discovered that the choice of cost function for reference following has a significant impact on the resulting trajectory. Overlapping support of B-spline basis functions can induce oscillations in the resulting optimized trajectory when employing an integral function over the spline parameter $x$ like the one in \cref{eq:total-track-error-b-spline} as a cost. (see \cref{fig:conservativeness-traj-integral} for a solution to an optimization problem using this cost). Similar results have been found for other integral-based costs as well. Therefore, careful consideration is required when designing the cost within the B-spline framework.

To illustrate this, consider the following optimization problem, which is a simple reference-following problem where the OS is constrained to a maximum velocity $v_\text{max}$:
\begin{equation}\label{eq:conservativeness-optimization}
    \begin{aligned}
        \min_{\mathbf c} \quad & J_\text{ref} \\
        \text{subject to} \quad &\mathbf p_\text{OS}(0) = \mathbf p_0, \\
                    &\mathbf p_\text{OS} = \mathbf B^\top(x) \mathbf c, \\
                    &\mathbf p_\text{ref} = \mathbf B_\text{ref}^\top(x) \mathbf c_\text{ref}, \\
                    & \mathbf p_\text{OS}'(x) \cdot \mathbf p_\text{OS}'(x) \le v_\text{max}^2.
    \end{aligned}
\end{equation}
Here, $\mathbf p_0$ is the initial position of the OS and the $\cdot$ operator denotes the pointwise dot product as described in \cref{sec:b-spline-theory}. The B-spline basis functions $\mathbf B$ and $\mathbf B_\text{ref}$ are defined as uniform B-splines of degree $p$ and 1, respectively, with the parameter domain $x\in[0,1]$. The control points $\mathbf c$ of the OS B-spline function are optimized to minimize the TTE to a reference trajectory $\mathbf p_\text{ref}$, which is defined by the control points $\mathbf c_\text{ref}$.
This optimization problem is then analyzied for different choices of $p$ and number $N$ of uniform B-spline basis functions $\mathbf B$. The resulting trajectories are shown in \cref{fig:conservativeness} with the parameters for the optimization problem set to
\begin{equation}\label{eq:conservativeness-parameters}
    \begin{aligned}
        \mathbf p_0 &= \begin{bmatrix} 1 & 0 \end{bmatrix} \\
        \mathbf B &= \mathbf B_{p, \mathbf u(p,n)} \\
        \mathbf B_\text{ref} &= \mathbf B_{1,\{0, 0, 1, 1\}} \\
        \mathbf c_\text{ref} &= \begin{bmatrix}
            [0 & 0] \\
            [0 & 3]
        \end{bmatrix} \\
        v_\text{max} &= 6, \\
    \end{aligned}
\end{equation}
where $\mathbf u(p,n) = \{\{0\}^{(p)}, \{i/N\}_{i=0}^{N}, \{1\}^{(p)}\}$ is a function that generates the knot vector for a uniform B-spline of degree $p$ with $N$ basis functions. The resulting trajectories are shown in \cref{fig:conservativeness-traj-integral}.

% $\|\cdot\|_\text{F}^2$ is the squared Frobenius norm, given by
% \begin{equation}
%     \|\mathbf A\|_\text{F}^2 = \sum_{i,j} a_{ij}^2,
% \end{equation}
% where $\mathbf A$ is a matrix and $a_{ij}$ is the element in the $i$-th row and $j$-th column of $\mathbf A$. All other operations on the B-spline functions are done using the algorithms in \cref{sec:b-spline-theory}. 


\begin{figure}
    % \centering
    % \begin{subfigure}[b]{\textwidth}
    % \centering
    % \includesvg[width=\textwidth]{fig/conservativeness/conservativeness_traj_coeffs_degree_3.svg}
    %     \caption{Discrete cost: Trajectory using the sum of squared coefficients of the reference error as a cost function.}
    %     \label{fig:conservativeness-traj-coeffs}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{\textwidth}
    %     \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/conservativeness_traj_integral_degree_2.svg}
        \caption{Optimal trajectory in \cref{eq:conservativeness-optimization} with parameters \cref{eq:conservativeness-parameters} for $p=2$,$N\in\{3,\ldots,20\}$.}
        \label{fig:conservativeness-traj-integral}
    % \end{subfigure}
    % \caption{Comparison of optimal trajectory in \cref{eq:conservativeness-optimization} with parameters \cref{eq:conservativeness-parameters} for $N\in\{4,\ldots,20\}$.}
    % \label{fig:conservativeness-traj}
\end{figure}

The oscillatory behavior observed in \cref{fig:conservativeness-traj-integral} can be attributed to the integral cost function's emphasis on the global trajectory shape, which consequently diminishes its sensitivity to high-frequency oscillations. To substantiate this claim, and to find a solution to the problem, a rigorous analysis of the eigenvectors and eigenvalues of the Hessian matrix associated with this cost function will be conducted.

To find the Hessian of the cost function, one can observe that the integral of the squared error can be expressed as a quadratic form in the coefficients of the B-spline functions. The cost function can be rewritten as
\begin{equation}
    \begin{aligned}
        J_\text{ref} &= \int_\mathbb R \mathbf e(x) \cdot \mathbf e(x) \, dx \\
        &= \int_\mathbb R \sum_i \mathbf c_{\mathbf e,i}^\top \mathbf B(x) \mathbf B^\top(x) \mathbf c_{\mathbf e,i}  dx \\
        &= \sum_i \mathbf c_{\mathbf e,i}^\top \underbrace{\int_\mathbb R \mathbf B(x) \mathbf B^\top(x) dx}_{\mathbf G} \;\mathbf c_{\mathbf e,i},
    \end{aligned}
\end{equation}
where $\mathbf c_{\mathbf e,i}$ are the coefficients of the B-spline function $\mathbf e(x)$, which represents the TTE. The subscript $i\in\{N,E\}$ denotes the North and East components of the position error, respectively.
The basis $\mathbf B(x)$ is the same as for $\mathbf p_\text{OS}(x)$ as $\mathbf B_\text{ref} \subseteq \mathbf B$ using the parameters in \cref{eq:conservativeness-parameters}. 
The integral $\int_\mathbb R \mathbf B(x) \mathbf B^\top(x) dx$ results in a matrix $\mathbf G$, known in the literature as the Gramian of the B-spline basis, and hereafter referred to simply as the Gramian (see \citet{Chu2022} for a more detailed description).
The Gramian matrix is an important mathematical object in the literature with one of its defining properties being that it is always positive semi-definite \citep{horn2013positive}. However, with our choice of basis functions with restrictions on knot multiplicality, the Gramians considered in this work will always be symmetric and strictly positive definite, i.e., all eigenvalues real and positive. Knowing the Gramian of the B-spline basis, the squared norms of the corresponding B-spline can be easily computed. Herafter, the B-spline norm $\|\mathbf e(x)\|_\mathbf H$ will refer to computing the bilinear form
\begin{equation}\label{eq:b-spline-norm}
    \|\mathbf e(x)\|_\mathbf H^2 = \sum_i \mathbf c_{\mathbf e,i}^\top \mathbf H \mathbf c_{\mathbf e,i},
\end{equation}
where $\mathbf H$ is a suitible symmetric positive definite matrix. 

Using the spectral (eigenvalue) decomposition of the Gramian $\mathbf G = \mathbf U \mathbf \Lambda \mathbf U^\top$ \citep{horn2013positive}, the cost function can be expressed differently. Here, $\mathbf U$ is a matrix whose columns are the unit-normalized eigenvectors of $\mathbf G$, and $\mathbf \Lambda$ is a diagonal matrix with the eigenvalues of $\mathbf G$ on the diagonal. Thus, the cost function can be rewritten as
\begin{equation}\label{eq:cost-function-spectral-decomposition}
    J_\text{ref} = \sum_i \mathbf c_{\mathbf e,i}^\top \mathbf G \mathbf c_{\mathbf e,i} = \sum_i \mathbf c_{\mathbf e,i}^\top \mathbf U \mathbf \Lambda \mathbf U^\top \mathbf c_{\mathbf e,i} = \sum_i \sum_{j=1}^{N} \lambda_j (\mathbf u_j^\top \mathbf c_{\mathbf e,i})^2,
\end{equation}
where $\lambda_j$ is the $j$-th eigenvalue of $\mathbf G$ and $\mathbf u_j$ is the $j$-th eigenvector of $\mathbf G$. With the cost function in this form, it is clear that the cost is a weighted sum of the squared projections of the coefficients $\mathbf c_{\mathbf e,i}$ onto the eigenvectors $\mathbf u_j$ of the Gramian $\mathbf G$. The eigenvalues $\lambda_j$ determine the weight of each projection, and thus the cost function penalizes the coefficients more heavily if they have a matching projection onto an eigenvector with a large eigenvalue.

As an example, consider the Gramian $\mathbf G_{2,\mathbf u(2,4)}$ for a uniform B-spline basis of degree $p=2$ with $N=4$ can be computed using the relevant formulas in \cref{sec:b-spline-theory} to yield
\begin{equation}
    \mathbf G_{2,\mathbf u(2,4)} = \int_\mathbb R \mathbf B_{2,\mathbf u(2,4)}(x) \mathbf B_{2,\mathbf u(2,4)}^\top(x) dx = 
    \frac{1}{120}\begin{bmatrix}
        12 & 7 & 1 & 0 \\
        7 & 20 & 12 & 1 \\
        1 & 12 & 20 & 7 \\
        0 & 1 & 7 & 12
    \end{bmatrix}.
\end{equation}

This matrix has the eigenvectors 
\begin{equation}
    \begin{aligned}
        \mathbf u_1 &\approx \begin{bmatrix}  
            0.23 \\  0.67 \\  0.67 \\  0.23 
        \end{bmatrix}, &
        \mathbf u_2 &\approx \begin{bmatrix}         
            0.57 \\  0.41 \\ -0.41 \\  -0.57 
        \end{bmatrix}, &
        \mathbf u_3 &\approx \begin{bmatrix}         
            0.67 \\  -0.23 \\  -0.23 \\  0.67 
        \end{bmatrix}, &
        \mathbf u_4 &\approx \begin{bmatrix}         
            0.41 \\  -0.57 \\  0.57 \\  -0.41 
        \end{bmatrix},
    \end{aligned}
\end{equation}
with corresponding eigenvalues
\begin{equation}
    \begin{aligned}
        \lambda_1 &\approx 0.29, &
        \lambda_2 &\approx 0.14, &
        \lambda_3 &\approx 0.07. &
        \lambda_4 &\approx 0.03.
    \end{aligned}
\end{equation}
The $u_1$ direction can be interpreted as a direction where all the coefficients have the same sign, while the $u_4$ direction can be interpreted as a direction where subsequent coefficients alternate in sign. 
As the cost in the $u_1$ direction compared to the $u_4$ direction is $\frac{\lambda_1}{\lambda_4} \approx 10$ times smaller, the cost function will amplify high frequency oscillations in the coefficients, which explains the behaviour in \cref{fig:conservativeness-traj-integral}. In \cref{fig:conservativeness-eigenvectors}, each eigenvector is shown as a weight on each B-spline coefficient where \cref{fig:conservativeness-eigenvectors-integral} shows the situation for the above example. Here, the relative eigenvalues $\hat\lambda_i = \lambda_i/\sum_{i=1}^{N} \lambda_i$ are shown, which will be used to better compare the costs between different cost functions. That the high frequency components of the eigenvectors are penalized less is appearent when increasing the number of B-spline basis functions $N$. This is shown in \cref{fig:integral-basis-eigenvectors-9} in \cref{app:reference-cost-plots} for $N=9$.
\Cref{fig:conservativeness-eigenvectors-coeffs} show the eigenvectors for when the cost has the form $\|\mathbf e(x)\|_\mathbf I^2$, where $\mathbf I$ is the identity matrix. 
So, in the pursuit of finding a more suitible cost function, these Hessians and how they interact with B-spline functions are now the subject of analysis.


\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/eigenvectors_integral_degree_2_N_4.svg}
        \caption{$\mathbf H = \mathbf G$}
        \label{fig:conservativeness-eigenvectors-integral}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/eigenvectors_coeffs_degree_2_N_4.svg}
        \caption{$\mathbf H = \mathbf I$}
        \label{fig:conservativeness-eigenvectors-coeffs}
    \end{subfigure}
    \caption{Eigenvectors of the Hessian $\mathbf H$ of the cost function $\|\mathbf e(x)\|_\mathbf H^2$ for $N=4$ and $p=2$. Note that $\hat\lambda_i$ is a scaled version of the eigenvalue $\lambda_i$ such that $\hat\lambda_i = \lambda_i/\sum_{i=1}^{N} \lambda_i$. Marks on the vertical axis are ommited as it is the relative size between the eigenvector components that are of interest.}
\label{fig:conservativeness-eigenvectors}
\end{figure}



Setting $\mathbf H = \mathbf I$ results in a cost function that penalizes all coefficients in the relevant basis equally. This is analogous to treating the coefficients as time-discretized points in the time-grid formulation in \cref{eq:total-track-error-time-grid}, where the cost is simply the sum of the squared TTE at each time step. \cref{fig:conservativeness-traj-coeffs} shows the resulting trajectory when using this cost function in \cref{eq:conservativeness-optimization} with the parameters in \cref{eq:conservativeness-parameters}. 

\begin{figure}
    \centering
    \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/conservativeness_traj_coeffs_degree_2.svg}
    \caption{Optimal trajectory in \cref{eq:conservativeness-optimization} with parameters \cref{eq:conservativeness-parameters} for $p=2$, $N\in\{3,\ldots,20\}$ and where the cost is changed to $J_\text{ref} = \|\mathbf e(x)\|_{\mathbf I}^2$.}
    \label{fig:conservativeness-traj-coeffs}
\end{figure}


It may be desirable to weigh the cost $J_\text{ref}$ against other objectives, such as the COLREGS objectives in the next section.
Using $J_\text{ref}=\| w_\text{ref}\;\mathbf e(x)\|_{\mathbf I}^2$ as the cost term works well when $w_\text{ref}$ is a scalar. If however, $w_\text{ref}(x)$ is a spline function itself, the oscillatory behavior of the cost function may return. This was discovered during the work for this thesis. The reason for wanting to use a spline function for $w_\text{ref}(x)$ is to allow for the cost to be weighted differently at different times, e.g., to allow for a higher cost on the TTE during transit and a lower cost during an encounter with a \acrshort{TS}. Again, this will be discussed in more detail in \cref{sec:colregs-objectives}.

The intuition for this comes from that while the cost yields independent directions for the eigenvectors in the Gramian $\mathbf{\tilde G}$ for the $\mathbf{\tilde B}$ basis, the Hessian of the cost function is ultimately expressed in the basis $\mathbf B$ of the original coefficients $\mathbf c_\mathbf e$. In vector notation, the cost function is expressed as
\begin{equation}\label{eq:cost-function-w-ref}
    J_\text{ref} = \|w_\text{ref}(x)\;\mathbf e(x)\|_{\mathbf I}^2 = \sum_i \mathbf{\tilde c}_{w \mathbf{e},i}^\top \mathbf I \mathbf{\tilde c}_{w \mathbf{e},i},
\end{equation}
where $\mathbf{\tilde c}_{w \mathbf{e}}$ are the coefficients of the spline function $w_\text{ref}(x)\;\mathbf e(x)$ in the $\mathbf{\tilde B}$ basis.  Using the transform $\mathbf T$ from $\mathbf B$ to $\mathbf{\tilde B}$ (see \cref{alg:basis-transformation}), the shape of the cost in the original basis can be approximated. Starting from \cref{eq:cost-function-w-ref}, the Hessian for the cost can be translated to the original basis as follows:
\begin{equation}\label{eq:cost-function-w-ref-derivation}
        J_\text{ref} = \sum_i \mathbf{\tilde c}_{w \mathbf{e},i}^\top \mathbf I \mathbf{\tilde c}_{w \mathbf{e},i} 
        = \sum_i \mathbf c_{w,\mathbf e,i}^\top \underbrace{\mathbf T^\top \mathbf I \mathbf T}_{\mathbf H} \mathbf c_{w,\mathbf e,i} 
        = \sum_i \mathbf c_{w,\mathbf e,i}^\top \mathbf H \mathbf c_{w,\mathbf e,i},
\end{equation}
where $\mathbf c_{w,\mathbf e}$ represents the approximated coefficients of the product $w_\text{ref}(x)\;\mathbf e(x)$ when expressed within the original, unrefined basis $\mathbf B$. This highlights a critical insight: modifying the basis in which the cost function is formulated inherently alters the cost function's characteristics and behavior.

As a simple example, consider the case where $w_\text{ref}(x)$ is a piecewise constant spline with its basis $\mathbf B_w$ sharing the same knot values as $\mathbf B$. I.e. for $N=4$ and $p=2$, the knot vector for $\mathbf B$ and $\mathbf B_w$ is $\mathbf u(2,4) = \left\{0, 0, 0, \frac{1}{2}, 1, 1, 1\right\}$ and $\left\{0, \frac{1}{2}, 1\right\}$ respectively. Multiplying $w_\text{ref}(x)$ with $\mathbf e(x)$ results in a new spline function which is minimally represented in $\mathbf{\tilde B}$ with $p=2$ and knot vector $\left\{0, 0, 0, \frac{1}{2}, \frac{1}{2}, \frac{1}{2}, 1, 1, 1\right\}$.  The transformation matrix $\mathbf T$ from $\mathbf B$ to $\mathbf{\tilde B}$ and the corresponding Hessian $\mathbf H$ is then:
\begin{equation*}
    \mathbf T = \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & \frac{1}{2} & \frac{1}{2} & 0 \\
        0 & \frac{1}{2} & \frac{1}{2} & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
    \end{bmatrix},
    \quad
    \mathbf H = \mathbf T^\top \mathbf T =
    \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & \frac{3}{2} & \frac{1}{2} & 0 \\
        0 & \frac{1}{2} & \frac{3}{2} & 0 \\
        0 & 0 & 0 & 1 \\
    \end{bmatrix}.
\end{equation*}
This Hessian reveals that the first and last basis functions remain unchanged, while the middle basis functions are now linear combinations. This is reflected in \Cref{fig:conservativeness-eigenvectors-full}, which shows the Hessian before and after multiplying the error with the weight function. The figure shows that the cost for consecutive coefficients being aligned is once again higher than for alternating coefficients. For a visualization with a higher number of basis functions the reader is referred to \cref{fig:refined-basis-eigenvectors-9} in \cref{app:reference-cost-plots}.

    
\begin{figure}
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/eigenvectors_coeffs_degree_2_N_4.svg}
        \caption{$J_\text{ref} = \|\mathbf e(x)\|_\mathbf I^2$}
        \label{fig:conservativeness-eigenvectors-coeffs-unrefined}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/eigenvectors_refined_basis_degree_2_N_4.svg}
        \caption{$J_\text{ref} = \|w_\text{ref}(x)\;\mathbf e(x)\|_\mathbf I^2$}
        \label{fig:conservativeness-eigenvectors-weight-refined}
    \end{subfigure}
    \caption{Eigenvectors of the Hessian $\mathbf H = \mathbf I$ of the cost function for $N=4$ and $p=2$ before (top) and after (bottom) weighing $\mathbf e(x)$ with $w_\text{ref}(x)$. The eigenvectors are shown in the original basis $\mathbf B$.}
    \label{fig:conservativeness-eigenvectors-full}
\end{figure}

With this analysis in mind, it is clear that the Hessian of the cost function should be chosen such that 1), high frequency oscillations in the coefficients are penalized more heavily than low frequency oscillations and 2), that the cost function should not be too sensitive to basis transformations. Choosing $\mathbf H = \mathbf G^{-1}$, the inverse Gramian, has been found during the course of this work to acheive these criteria. The inverse Gramian has the same eigenvectors as the Gramian, but the eigenvalues are now inversely proportional to the original eigenvalues \citep{horn2013positive}, so the eigen decomposition of the inverse Gramian is given by
\begin{equation}
    \mathbf G^{-1} = \left(\mathbf U \mathbf \Lambda \mathbf U^\top\right)^{-1} = \mathbf U \mathbf \Lambda^{-1} \mathbf U^\top.
\end{equation}

The eigenvectors are shown in \cref{fig:conservativeness-eigenvectors-inv-gramian-full} for $N=4$ and $p=2$ and in \cref{fig:refined-basis-inverse-gramian-eigenvectors-9} for $N=9$ and $p=2$. These figures show that the change in cost after the basis change induced by the weight function $w_\text{ref}(x)$ is now much less pronounced, and high frequency oscillations are damped by the high eigenvalues. 

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/eigenvectors_inv_gramian_degree_2_N_4.svg}
        \caption{$J_\text{ref} = \|\mathbf e(x)\|_{\mathbf G^{-1}}^2$}
        \label{fig:conservativeness-eigenvectors-inv-gramian}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/eigenvectors_refined_inv_gramian_degree_2_N_4.svg}
        \caption{$J_\text{ref} = \|w_\text{ref}(x)\;\mathbf e(x)\|_{\mathbf G^{-1}}^2$}
        \label{fig:conservativeness-eigenvectors-weight-inv-gramian}
    \end{subfigure}
    \caption{Eigenvectors of the Hessian $\mathbf H = \mathbf G^{-1}$ of the cost function for $N=4$ and $p=2$ before (top) and after (bottom) weighing $\mathbf e(x)$ with $w_\text{ref}(x)$. The eigenvectors are shown in the original basis $\mathbf B$.}
    \label{fig:conservativeness-eigenvectors-inv-gramian-full}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/conservativeness_traj_inv_gramian_degree_2.svg}
    \caption{Optimal trajectory in \cref{eq:conservativeness-optimization} with parameters \cref{eq:conservativeness-parameters} for $N\in\{3,\ldots,20\}$ and where the cost is changed to $J_\text{ref} = \|\mathbf e(x)\|_{\mathbf G^{-1}}^2$.}
    \label{fig:conservativeness-traj-inv-gramian}
\end{figure}

The trajectories resulting from using the inverse Gramian as the Hessian in \cref{eq:conservativeness-optimization} are shown in \cref{fig:conservativeness-traj-inv-gramian}. The trajectories are much smoother than those in \cref{fig:conservativeness-traj-integral}, but converge slower to the reference trajectory for low $N$. This is also the case for using the cost function with $\mathbf H = \mathbf I$ in \cref{fig:conservativeness-traj-coeffs}. For higher $N$, all cost functions converge to the same trajectory, as the B-spline basis functions become more localized and the oscillatory effects from the Gramian are damped. This convergence can be seen in \cref{fig:conservativeness}, where $\|\mathbf e_\mathbf c\|_\mathbf G^2$ is plotted against the number $N$ of B-spline basis functions for $p=2$ and for each of the cost functions $\|\mathbf e(x)\|_\mathbf I^2$, $\|\mathbf e(x)\|_\mathbf G^2$ and $\|\mathbf e(x)\|_{\mathbf G^{-1}}^2$.


The final cost function for the reference trajectory following is thus chosen as
\begin{equation}\label{eq:cost-reference-following}
    J_\text{ref} = \|w_\text{ref}(x)\;\mathbf e(x)\|_{\mathbf G^{-1}}^2 \frac{1}{\text{trace}(\mathbf G^{-1})},
\end{equation}
where $w_\text{ref}(x)$ is a B-spline function that can be used to weight the cost function at different times, and $\mathbf G^{-1}$ is the inverse Gramian in the given expressions' basis. The trace term is included to ensure that the total cost of all the directions is normalized to 1, as the trace of a matrix is the sum of its eigenvalues.

To the authors knowledge, this is the first time that the inverse Gramian has been used as a Hessian in a B-spline-based optimization problem, or that a similar analysis of the eigenvectors and eigenvalues of the Gramian has been performed in the context of B-spline trajectory following. Other cost functions have also been considered, but with poor results in terms of feasibility for the optimization problem in question. A brief overview of these costs is given in \cref{app:failed-reference-following}.

\begin{figure}
    \centering
    \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/compare_conservativeness_degree_2.svg}
    \caption{Convergence of the different cost functions. The integral of the square error $\|\mathbf e_\mathbf c\|_\mathbf G^2$ is plotted against the number $N$ of B-spline basis functions for $p=2$.}
    \label{fig:conservativeness}
\end{figure}


\FloatBarrier
\subsection{COLREGS Objectives}\label{sec:colregs-objectives}
Rule 8b) of the COLREGS states that any alteration of course should be large enough to be readily apparent to another vessel. 
This can be interpreted as a cost function that penalizes small alterations of course, but not large ones. 
Implementing this directly in an optimization context, by penalizing low turn rates without penalizing straight line motion, results in a cost function that is not convex. 
See \cref{fig:turn-rate-cost} for an illustration of the problem.

\begin{figure}
    \centering
    \includesvg[width=0.6\textwidth]{fig/illustrations/Turn rate cost.svg}
    \caption{Imagined Turn rate cost for a vessel following Rule 8b) of the COLREGS.}
    \label{fig:turn-rate-cost}
\end{figure}

\cite{Thyri2022-MPC} propose a solution to improve the compliance of rule 8 by introducing a time window where the cost for altering course is lowered. The goal of this is to control when the OS should alter course, and with manipulation of the length and position of the time window, the cost function can facilitate a more COLREGS compliant trajectory. This idea is adapted to the B-spline MINMPC framework as follows:
An encounter window $W_\text e$ is defined, during which the OS should ensure safe passage. Within $W_\text e$, the cost for reference trajectory following is reduced, while the cost for altering course remains high, except at the start and end of $W_\text e$, denoted as maneuver windows $W_\text{mv}$. 
The reason for the second maneuver window as opposed to the singular one in \cite{Thyri2022-MPC} is to facilitate a clear maneuver back to the reference trajectory after the encounter window.

\begin{figure}
    \centering
    \includesvg[width=0.8\textwidth]{fig/illustrations/Maneuver window.svg}
    \caption{Weights for turn rate and TTE plotted against time.}
    \label{fig:maneuver-window}
\end{figure}

The encounter and maneuver windows are incorporated into the cost function as weights $w_\text{ref}(x)$ and $w_\text{mv}(x)$, which are applied to the reference trajectory following cost and the cost for altering course, respectively. The resulting cost function is:
\begin{subequations}\label{eq:cost-maneuver-window}
    \begin{align}
        J_\text{ref} &= \|w_\text{ref}(x)\;\mathbf e(x)\|_{\mathbf G^{-1}}^2 \frac{1}{\text{trace}(\mathbf G^{-1})}, 
        \label{eq:cost-maneuver-window-ref}\\
        J_\text{mv} &= \|w_\text{mv}(x)\;\mathbf e'(x)\|_{\mathbf G^{-1}}^2 \frac{1}{\text{trace}(\mathbf G^{-1})},
        \label{eq:cost-maneuver-window-mv}\\
        J_\mathrm{acc} &= \|w_\text{acc}(x)\;\mathbf p''_\text{os}(x)\|_{\mathbf G^{-1}}^2 \frac{1}{\text{trace}(\mathbf G^{-1})}, \label{eq:cost-maneuver-window-acc}\\
        J_\mathrm{end} &= \|w_\text{end}(x)\;(\mathbf p_\text{OS}(x)-\mathbf p_\text{ref}(1))\|_{\mathbf G^{-1}}^2 \frac{1}{\text{trace}(\mathbf G^{-1})}, \label{eq:cost-maneuver-window-end}\\
        J &= J_\text{ref} + J_\text{mv} + J_\mathrm{acc} + J_\mathrm{end},
        \label{eq:cost-maneuver-window-total}
    \end{align}
\end{subequations}
where $J_\text{mv}$ is the cost for altering course, $w_\text e(x)$ is the weight for the reference trajectory following cost, and $w_\text{mv}(x)$ is the weight for the maneuver window cost as shown in \cref{fig:maneuver-window}. An additional cost $J_\mathrm{acc}$ is included to penalize course and speed alterations where desired. To provide a driving force in the direction of the reference trajectory, a cost $J_\mathrm{end}$ is included as the PXTE alone does not penalize moving in the opposing direction of the reference course. 

The advantage of the B-spline framework in this case is that the weights can be defined as B-spline functions themselves. This allows for great flexibility in shaping the desired encounter trajectory. The function shape shown in \cref{fig:maneuver-window}, can be represented using a zero-degree B-spline. If smoother transitions are desired, higher degree B-splines can be used.
If the weights are fixed as a function of time before the optimization problem is solved, then the costs are constant with respect to the optimization variables and the optimization problem is convex as opposed to that of \cref{fig:turn-rate-cost}.

Estimating when the encounter window should start and end is a non-trivial problem. As is determining how long the maneuver windows should be. \cite{Thyri2022-MPC} proposes to use the time of closest point of approach (TCPA) to the TS along the desired trajectory. The desired trajectory can for example be estimated using Line-of-Sight (LOS) guidance \citep{Fossen2011-Handbook} or by considering the optimzation problem without the TS. Estimation of the encounter window time is implemented according to \cite{Thyri2022-MPC}, and won't be discussed further here.


\section{Implementation}\label{sec:implementation}

\subsection{Python Library}\label{sec:python-implementation}

The B-spline MINMPC is implemented in Python using CasADi \citep{casadi} and numpy \citep{numpy}. The code is organized into two components: a B-spline library and a CasADi wrapper. The B-spline library follows the structure of the OMGTools library by \citet{mercy2016spline} with enhancements for type safety, vector-valued spline support, small performance optimizations, and added compatibility with CasADi’s MIP solvers. All B-spline transformations and operations are hidden behind a clean interface, so that spline variables and constraints can be declared on symbolic variables just like native CasADi types.

A single optimization-problem class provides a public interface mirroring CasADi’s Opti while extending it with B-spline variables, constraints, and integer/binary decision variables. 
A key feature of this class is its modular design, enabling the representation of distinct entities such as the OS, TS, and reference trajectories as individual objects. These objects can then be seamlessly integrated into a unified optimization problem.  The subsequent example illustrates the construction of a complete optimization problem using this class, presenting a simplified version of the problem addressed in this paper:

\begin{example}{Optimization Problem Setup}
\begin{python}
from cs_mpc import OptiObject, OptiCollection, BSplineBasis

# Define a spline basis for the own ship
os_basis = BSplineBasis.Uniform(degree=2, n_control_points=12)

# Create own-ship object
own_ship     = OptiObject('own_ship')
os_pos       = own_ship.declare_spline_variable('pos', n_dim=2, basis=os_basis)
os_pos_init  = own_ship.declare_parameter('pos_init',  n_dim=2)
os_pos_final = own_ship.declare_parameter('pos_final',  n_dim=2)

# Boundary constraints
own_ship.declare_constraint(os_pos(0) - os_pos_init, lower_bound=0, upper_bound=0)
own_ship.declare_constraint(os_pos(1) - os_pos_final, lower_bound=0, upper_bound=0)

# Objective: minimize end-point error
end_err = os_pos - os_pos_final
own_ship.declare_objective(end_err.dot(end_err).definite_integral(0,1))

# Create a target-ship object
target_ship = OptiObject('target_ship')
ts_basis    = BSplineBasis.Uniform(degree=1, n_control_points=2)
ts_pos      = target_ship.declare_spline_parameter('pos', n_dim=2, basis=ts_basis)
min_dist    = target_ship.declare_parameter('min_dist', n_dim=1)

# Collision-avoidance constraint
ts_err = os_pos - ts_pos
target_ship.declare_constraint(ts_err.dot(ts_err) - min_dist**2, lower_bound=0)

# Set paramter values and solve
opti = OptiCollection([own_ship, target_ship])
own_ship.set_value('pos_init', [0,0])
own_ship.set_value('pos_final', [1000,0])
target_ship.set_value('pos', [[500,200],[500,-200]])
target_ship.set_value('min_dist', 50)
solution = opti.solve()
\end{python}
\end{example}


\subsection{Operations on B-splines}\label{sec:operations-on-b-splines}

An algoritmic overview of how operations on B-splines are performed in the library is given in this section. 


From the recursive relation in \cref{eq:b-spline-recurrence} it follows that the B-spline basis functions are piecewise polynomial functions in $x$ of degree $p$. This property makes it clear that all polynomial operations on B-splines will result in a new B-spline. This property is exploited in the following sections to derive the B-spline derivative and integral, as well as to transform between different B-spline bases.

In all following sections, it is assumed that for a given B-spline basis of degree $p$, the corresponding knot vector $\mathbf t$ is $p+1$ regular, i.e. all knots have multiplicity of most $p+1$ as well as the first and last knot having multiplicity $p+1$. The knot vector $\mathbf t$ can then be written with the notation 
\begin{equation}\label{eq:regular-knot-vector}
    \mathbf t = \{\underbrace{\eta_0, \dots, \eta_0}_{m_0}, \underbrace{\eta_1, \dots, \eta_1}_{m_1}, \dots, \underbrace{\eta_{r}, \dots, \eta_{r}}_{m_r}\} = \{\eta_0^{(m_0)}, \eta_1^{(m_1)}, \dots, \eta_r^{(m_r)}\}
\end{equation}
where $\{\eta_i\}$ is the set of unique knots in increasing order and $m_i$ denotes the multiplicity of the corresponding unique knot. For the knot vector to be regular, $m_0 = m_r = p+1$ and $m_i \leq p+1$ for $i \in \{1,2,\ldots,n-1\}$.

\subsection{Differentiation}\label{sec:derivative}
Given a B-spline $f(x) = \sum_{j=0}^{n-1} c_j B_{j, p, \mathbf{t}}(x)$,
the derivative of the B-spline can be computed by simply differencing its coefficients. 
\begin{equation}\label{eq:b-spline-derivative}
    \frac{d}{dx} f(x) = (p-1) \sum_{j=1}^{n-1} \frac{c_j-c_{j-1}}{t_{j+p-1}-t_j} B_{j, p-1, \boldsymbol{\tau}}(x)
\end{equation}
where $\boldsymbol{\tau} = [t_j]_{j=1}^{n+p-1}$ is the same knot vector as $\mathbf{t}$, but with the first and last knot removed in order to maintain regularity of the knot vector for the derivative B-spline basis.

The new coefficients can be expressed using a transformation matrix $\mathbf T_D$ such that $\mathbf{c}_D = \mathbf T_D \mathbf{c}$, where $\mathbf{c}_D$ is the vector of coefficients of the derivative B-spline. The transformation matrix is then an $(n-1) \times n$ matrix with elements:

\begin{equation}
    \mathbf T_D = (p-1) \begin{bmatrix}
        -q_{1,p-1,\mathbf{t}}^{-1} & q_{1,p-1,\mathbf{t}}^{-1} & 0 & \cdots & 0 \\
        0 & -q_{2,p-1,\mathbf{t}}^{-1} & q_{2,p-1,\mathbf{t}}^{-1} & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & \cdots & 0 & -q_{n-1,p-1,\mathbf{t}}^{-1} & q_{n-1,p-1,\mathbf{t}}^{-1} 
    \end{bmatrix},
\end{equation}
where $q_{j,p,\mathbf{t}} = (t_{j+p}-t_j)$.


The denominator $t_{j+p}-t_j$ in \cref{eq:b-spline-derivative} illustrates the continuity of the B-spline with respect to knot multiplicity. If a knot is repeated more than $p$ times, the denominator will be zero, and the derivative is not defined at that knot.

\subsection{Integration}
Given a degree $p$ and initial knot sequence $\mathbf t$, \cref{eq:b-spline-derivative}, gives
\begin{equation}\label{eq:b-spline-integral-pre}
    p \sum_{j=1}^{n-1} \gamma_j B_{j, p, \boldsymbol{t}}(x) 
    = \frac{d}{dx} \sum_{j=0}^{n-1} c_j B_{j, p+1, \boldsymbol{\tau}}(x),
\end{equation}
given that
\begin{equation}
    \gamma_j = \frac{c_j-c_{j-1}}{t_{j+p}-t_j},
\end{equation}
holds. Here, $\boldsymbol{\tau}$ is now the knot vector $\mathbf{t}$ with 1 additional knot at the beginning and end. Now
the new coefficients on the right hand side of \cref{eq:b-spline-integral-pre} can be expressed as 
\begin{equation}\label{eq:b-spline-integral-coefficients}
    c_j = c_{j-1} + \gamma_j \frac{t_{j+p}-t_j}{p}.
\end{equation}
This then gives the general anti-derivative of a B-spline as
\begin{equation}\label{eq:b-spline-integral}
    \int f(x) dx = \sum_{j=0}^{n-1} c_j B_{j, p+1, \boldsymbol{\tau}}(x)
\end{equation}
where $[c_j]_{j=1}^{n-1}$ are given by \cref{eq:b-spline-integral-coefficients}, $c_0$ is the constant of integration and $[\gamma_j]_1^{n-1}$ are the original coefficients of $f(x)$. In matrix form, the coefficients of the integral B-spline can be expressed as
\begin{equation}\label{eq:b-spline-integral-matrix}
    \mathbf{c}_I = \mathbf T_I \mathbf{c} + \mathbf{c}_0,
\end{equation}
where $\mathbf T_I$ is an $n \times (n-1)$ matrix with elements
\begin{equation}
    \mathbf T_I = \frac{1}{p}\begin{bmatrix}
        0 & 0 & \cdots & 0 \\
        q_{1,p,\mathbf{t}} & 0 & \cdots & 0 \\
        q_{1,p,\mathbf{t}} & q_{2,p,\mathbf{t}} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        q_{1,p,\mathbf{t}} & q_{2,p,\mathbf{t}} & \cdots & q_{n-1,p,\mathbf{t}}
    \end{bmatrix},\quad
    \mathbf{c}_0 = \begin{bmatrix}
        c_0 \\
        c_0 \\
        \vdots \\
        c_0
    \end{bmatrix}
\end{equation}

and $q_{j,p,\mathbf{t}} = (t_{j+p}-t_j)$ still.
As integration and differentiation are inverse operations, it should come as no surprise that the transformation matrices $\mathbf T_I$ and $\mathbf T_D$ are similirly related. This is explored further in \cref{app:homogeneous-integration-matrix}.


\subsection{Transforming Between Bases}\label{sec:basis-transformation}
In order to perform binary operations on spline functions such as multiplication and addition, it is necessary to be able to represent the same spline function in different bases. A powerful result from theory is that any spline function $\mathbf S$ expressed in a given basis $\mathbf B_{p,\mathbf t}$ can be represented in any other basis $\mathbf B_{q,\boldsymbol \tau}$ as long as $q \geq p$ and $\mathbf t \subseteq \boldsymbol \tau$\todo[inlinepar]{ikke helt sant} \citep{Grimstad2016}. Moreover, the coefficients in the new basis can be found by solving a linear system of equations. The following basis transformation method is employed in many algorithms in literature, and forms the foundation for the rest of the methods in this chapter, which is why a thorough explanation of this method is given attention here.

The goal is to find the transformation matrix $\mathbf T$ such that $\mathbf c_{q, \boldsymbol \tau} = \mathbf T \mathbf c_{p, \mathbf t}$, where $\mathbf c_{q, \boldsymbol \tau}$ and $\mathbf c_{p, \mathbf t}$ are the coefficients of the spline function $\mathbf S$ in the bases $\mathbf B_{q,\boldsymbol \tau}$ and $\mathbf B_{p,\mathbf t}$, respectively.

%The strategy to find $\mathbf T$ involves expressing the coefficients of the original basis $\mathbf c_{p, \mathbf t}$ as a linear combination of the refined basis $\mathbf c_{q,\boldsymbol \tau}$.
Noting that the two spline functions $\mathbf B_{q,\boldsymbol \tau}^\top \mathbf c_{q, \boldsymbol \tau}$, $\mathbf B_{p,\mathbf t}^\top \mathbf c_{p, \mathbf t}$ have to be equal,
the transformation matrix $\mathbf T$ can be found by sampling the B-spline basis functions at strategic points in the interval $[\tau_0, \tau_{n+q})$ to obtain a system of $m$ linear equations with $m$ unknowns.

The Greville abscissae $\boldsymbol \xi_{q,\boldsymbol \tau}=[\xi_{q,\boldsymbol \tau}]_{i=0}^{n-1}$ are a natural choice for the sampling points, as they represent the points at which the $i$-th basis function is most influential. 
They are given by the mean of the knots in each basis functions support $[\tau_i, \tau_{i+q+1})$ as
\begin{equation}\label{eq:greville-abscissae}
    \xi_{q,\boldsymbol \tau}(i) = \frac{1}{q+1} \sum_{j=i}^{i+q} \tau_j.
\end{equation}
The transformation matrix $\mathbf T$ can then be found by evaluating the B-spline basis functions at the Greville abscissae and solving the linear system of equations
\begin{equation}\label{eq:transformation-matrix-equation}
    \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(i))^\top \mathbf c_{q, \boldsymbol \tau} = \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(i))^\top \mathbf c_{p, \mathbf t} \quad \forall i\in\{0,1,\ldots,m-1\}
\end{equation}
for $\mathbf c_{q, \boldsymbol \tau}$. This can be written in matrix form as
\begin{equation}\label{eq:transformation-matrix}
    \underbrace{\begin{bmatrix}
        \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(0))^\top \\
        \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(1))^\top \\
        \vdots                                                             \\
        \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(m-1))^\top
    \end{bmatrix}}_{\mathbf A:\;m \times m}
    \underbrace{\begin{bmatrix}
        c_{q, \boldsymbol \tau, 0}   \\
        c_{q, \boldsymbol \tau, 1}   \\
        \vdots                       \\
        c_{q, \boldsymbol \tau, m-1} \\
    \end{bmatrix}}_{\mathbf c_{q,\boldsymbol\tau}:\;m \times 1}
    = 
    \underbrace{\begin{bmatrix}
        \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(0))^\top  \\
        \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(1))^\top  \\
        \vdots                                                       \\
        \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(m-1))^\top
    \end{bmatrix}}_{\mathbf D:\;m \times n}
    \underbrace{\begin{bmatrix}
        c_{p, \mathbf t, 0}   \\
        c_{p, \mathbf t, 1}   \\
        \vdots                \\
        c_{p, \mathbf t, n-1}
    \end{bmatrix}}_{\mathbf c_{p,\mathbf t}:\;n \times 1}
\end{equation}
or more compactly as
\begin{equation}
    [\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top] \mathbf c_{q, \boldsymbol \tau} = [\mathbf B_{p,\mathbf t}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top] \mathbf c_{p, \mathbf t}.
\end{equation}

The matrix $\mathbf T$ is then simply
\begin{equation}\label{eq:transformation-matrix-solution}
    \mathbf T = [\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]^{-1} [\mathbf B_{p,\mathbf t}(\boldsymbol \xi_{q,\boldsymbol \tau})^{\top}].
\end{equation}
As the B-spline basis functions are linearly independent, and the Greville abscissae are chosen such that each basis function is sampled at a point near the ``center'' of its support, the matrix $[\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]$ is always invertible. This method follows from the Schoenberg-Whitney Theorem \citep{schoenberg1988polya} for which a simple proof can be found in \cite{schoenberg2022proof}. The matrix $\mathbf A$ in \cref{eq:transformation-matrix} is known as a \emph{Collocation matrix} in the literature \todo[inlinepar]{Finn kilde} and can be used to interpolate an arbitrary function with a B-spline by replacing the right hand side of \cref{eq:transformation-matrix} with the function values sampled at the Greville abscissae.

\begin{algorithm}
    \caption{B-spline Basis Transformation}\label{alg:basis-transformation}
    \begin{algorithmic}[1]
        \State \textbf{Input:} B-spline basis $\mathbf{B}_{p,\mathbf{t}}$
        \State \textbf{Input:} Refined B-spline basis $\mathbf B_{q,\boldsymbol \tau}$
        \Ensure $q \geq p$
        \Ensure $\mathbf t \subseteq \boldsymbol \tau$
        \State $\boldsymbol \xi_{q, \boldsymbol \tau} \gets$ result from \cref{eq:greville-abscissae}
        \State $\mathbf A \gets [\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]$
        \State $\mathbf B \gets [\mathbf B_{p,\mathbf t}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]$
        \State $\mathbf T \gets \mathbf A^{-1} \mathbf B$
        \State \textbf{Output:} $\mathbf T$
    \end{algorithmic}
\end{algorithm}

\subsection{Knot Refinement and Degree Elevation}\label{sec:knot-refinement-degree-elevation}
The method in \cref{sec:basis-transformation} can be utilized for the two most commonly used techniques for manipulating B-splines: knot refinement and degree elevation. Knot refinement is simply the process of adding new knots to the knot vector, keeping the degree of the B-spline constant. 

Degree elevation, on the other hand, is the process of increasing the degree of the B-spline. There is a subtle caveat here, as the continuity of the B-spline at a knot is equal to the degree of the B-spline minus the multiplicity of the knot. This means that if a spline is to be represented in a higher degree basis, the knot multiplicity must be increased to maintain the same continuity at the knots. So, if a spline function in the basis $\mathbf B_{p,\mathbf t}$ is to be represented in the basis $\mathbf B_{p+1,\boldsymbol \tau}$, the knot vector $\boldsymbol \tau$ must contain the original knots $\mathbf t$ with multiplicity $p+1$.

In both knot refinement and degree elevation, the transformation matrix $\mathbf T$ from \cref{eq:transformation-matrix-solution} can be used to find the coefficients in the new basis. The following examples illustrate how to perform knot refinement and degree elevation on a B-spline.

$t = \{\underbrace{a, \dots, a}_{p+1}, \underbrace{\eta_1, \dots, \eta_1}_{m_1}, \dots, \underbrace{\eta_{n}, \dots, \eta_{n}}_{m_n}, \underbrace{b, \dots, b}_{p+1}\}$
$\mathbf u = \unique(x) = \{a, \eta_1, \dots, \eta_n, b\}$

\begin{algorithm}
    \caption{Degree Elevation}\label{alg:degree-elevation}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Knot vector $\mathbf t = \{\eta_0^{(m_0)}, \eta_1^{(m_1)}, \dots, \eta_r^{(m_r)}\}$, degree $p$
        \State \textbf{Input:} Desired degree $q$
        \Ensure $q > p$
        \Ensure $m_0 = m_r = p+1$
        \Ensure $m_i \leq p+1$ for $i \in \{0,1,\ldots,r\}$
        \State $\boldsymbol\tau \gets \{\eta_0^{(m_0+p-q)}, \eta_1^{(m_1+p-q)}, \dots, \eta_r^{(m_r+p-q)}\}$
        \State $\mathbf T \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p,\mathbf t}$ to $\mathbf B_{q,\boldsymbol \tau}$
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol \tau}, \mathbf T$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Knot Refinement}\label{alg:knot-refinement}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Knot vector $\mathbf t = \{\eta_0^{(m_0)}, \eta_1^{(m_1)}, \dots, \eta_r^{(m_r)}\}$, degree $p$
        \State \textbf{Input:} New knots to insert $\boldsymbol \xi = \{\xi_1, \xi_2, \dots, \xi_k\}$
        \Ensure $\xi_i \in (\eta_0, \eta_r)$
        \State $\boldsymbol \tau = \{\hat\eta_0^{(\hat m_0)}, \hat\eta_1^{(\hat m_1)}, \dots, \hat\eta_{\hat r}^{(\hat m_{\hat r})}\} \gets \mathbf t \cup_< \boldsymbol \xi$ \Comment{$\cup_<$ denotes the sorted union}
        \Ensure $\hat m_i \leq p+1$ for $i \in \{0,1,\ldots,\hat r\}$
        \State $\mathbf T \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p,\mathbf t}$ to $\mathbf B_{p,\boldsymbol \tau}$
        \State \textbf{Output:} $\mathbf B_{p, \boldsymbol \tau}, \mathbf T$
    \end{algorithmic}
\end{algorithm}

\todo[inline]{legg til plot av konstant spline og lineær spline}
\begin{indentedexample}[Degree Elevation]
    We want to express the constant spline $f(x) =\mathbf B_{0, \mathbf t}^\top\mathbf c$ with knot vector $\mathbf t = \left[0, 1\right]$ as a linear spline $g(x) = \mathbf B_{1, \boldsymbol \tau}^\top \mathbf d$ with knot vector $\boldsymbol \tau = \left[0, 0, 1, 1\right]$. 

    The Greville abscissae for the linear spline are 
    \begin{equation*}
        \boldsymbol \xi_{1,\boldsymbol \tau} = 
        \begin{bmatrix}
            \frac{\tau_0 + \tau_1 + \tau_2}{3} &
            \frac{\tau_1 + \tau_2 + \tau_3}{3} 
        \end{bmatrix}
        =
        \begin{bmatrix}
            \frac{0 + 0 + 1}{3} & \frac{0 + 1 + 1}{3}
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{3} & \frac{2}{3}
        \end{bmatrix}.
    \end{equation*}
    Evaluating $\mathbf B_{0, \mathbf t}$ and $\mathbf B_{1, \boldsymbol \tau}$  at the Greville abscissae gives
    \begin{equation*}
        \begin{aligned}
            \mathbf B_{0, \mathbf t}\begin{pmatrix}\frac{1}{3}\end{pmatrix} &= \begin{bmatrix} 1 \end{bmatrix}, 
            &\mathbf B_{0, \mathbf t}\begin{pmatrix}\frac{2}{3}\end{pmatrix} &= \begin{bmatrix} 1 \end{bmatrix} 
            \\
            \mathbf B_{1, \boldsymbol \tau}\begin{pmatrix}\frac{1}{3}\end{pmatrix} &= \begin{bmatrix} \frac{2}{3} & \frac{1}{3} \end{bmatrix}^\top, 
            &\mathbf B_{1, \boldsymbol \tau}\begin{pmatrix}\frac{2}{3}\end{pmatrix} &= \begin{bmatrix} \frac{1}{3} & \frac{2}{3} \end{bmatrix}^\top
        \end{aligned}
    \end{equation*}
    Solving
    \begin{equation*}
        \begin{bmatrix}
            \frac{2}{3} & \frac{1}{3} \\
            \frac{1}{3} & \frac{2}{3}
        \end{bmatrix}
        \begin{bmatrix} d_0 \\ d_1 \end{bmatrix}
        =
        \begin{bmatrix} 1 \\ 1 \end{bmatrix}
        \begin{bmatrix} c_0 \end{bmatrix}
    \end{equation*}
    gives the solution
    \begin{equation*}
        \begin{bmatrix} d_0 \\ d_1 \end{bmatrix}
        =
        \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}
        \begin{bmatrix} 1 \\ 1 \end{bmatrix}
        \begin{bmatrix} c_0 \end{bmatrix}
        =
        \begin{bmatrix} 1 \\ 1 \end{bmatrix}
        \begin{bmatrix} c_0 \end{bmatrix}
    \end{equation*}
\end{indentedexample}

\begin{indentedexample}[Knot Refinement]
    Given a linear spline function $f(x) = \mathbf B_{1, \mathbf t}^\top \mathbf c$ with knot vector $\mathbf t = \left\{ 0, 0, \frac{1}{2}, 1, 1\right\}$, the goal is to express it with in the basis $\mathbf B_{1, \boldsymbol \tau}$ with the refined knot vector $\boldsymbol \tau = \left\{ 0, 0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1, 1\right\}$.

        The Greville abscissae for the refined basis are
        \begin{equation*}
            \boldsymbol \xi_{1,\boldsymbol \tau} = 
            \left\{
                \frac{1}{12}, \frac{3}{12}, \frac{6}{12}, \frac{9}{12}, \frac{11}{12}
            \right\}.
        \end{equation*}
        Using \cref{tab:linear-spline-evaluation} to evaluate $\mathbf B_{1,\boldsymbol\tau}(x)$ at the Greville abscissae gives
        \begin{equation*}
            \begin{aligned}
                \mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{1}{12}\end{pmatrix} &= \begin{bmatrix} \frac{3}{4} & \frac{1}{4} & 0 & 0 & 0 \end{bmatrix}^\top,
                &\mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{3}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 1 & 0 & 0 & 0 \end{bmatrix}^\top, \\
                \mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{6}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 0 & 1 & 0 & 0 \end{bmatrix}^\top,
                &\mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{9}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 0 & 0 & 1 & 0 \end{bmatrix}^\top, \\
                \mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{11}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 0 & 0 & \frac{1}{4} & \frac{3}{4} \end{bmatrix}^\top,
            \end{aligned}
        \end{equation*}
        while evaluating $\mathbf B_{1,\mathbf t}(x)$ at the Greville abscissae gives
        \begin{equation*}
            \begin{aligned}
                \mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{1}{12}\end{pmatrix} &= \begin{bmatrix} \frac{5}{6} & \frac{1}{6} & 0 \end{bmatrix}^\top,
                &\mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{3}{12}\end{pmatrix} &= \begin{bmatrix} \frac{1}{2} & \frac{1}{2} & 0 \end{bmatrix}^\top, \\
                \mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{6}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^\top,
                &\mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{9}{12}\end{pmatrix} &= \begin{bmatrix} 0 & \frac{1}{2} & \frac{1}{2} \end{bmatrix}^\top, \\
                \mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{11}{12}\end{pmatrix} &= \begin{bmatrix} 0 & \frac{1}{6} & \frac{5}{6} \end{bmatrix}^\top.
            \end{aligned}
        \end{equation*}
        Inserting the calculated values into \cref{eq:transformation-matrix} gives
        \begin{equation*}
            \begin{bmatrix}
                \frac{3}{4} & \frac{1}{4} & 0 & 0 & 0 \\
                0 & 1 & 0 & 0 & 0 \\
                0 & 0 & 1 & 0 & 0 \\
                0 & 0 & 0 & 1 & 0 \\
                0 & 0 & 0 & \frac{1}{4} & \frac{3}{4}
            \end{bmatrix}
            \mathbf{d}
            =
            \begin{bmatrix}
                \frac{5}{6} & \frac{1}{6} & 0 \\
                \frac{1}{2} & \frac{1}{2} & 0 \\
                0 & 1 & 0 \\
                0 & \frac{1}{2} & \frac{1}{2} \\
                0 & \frac{1}{6} & \frac{5}{6}
            \end{bmatrix}
            \mathbf c,
        \end{equation*}
        which has the solution
        \begin{equation*}
            \mathbf d = \begin{bmatrix}
                \frac{8}{9} & \frac{1}{9} & 0 \\
                \frac{1}{2} & \frac{1}{2} & 0 \\
                0 & 1 & 0 \\
                0 & \frac{1}{2} & \frac{1}{2} \\
                0 & \frac{1}{9} & \frac{8}{9}
            \end{bmatrix}
            \mathbf c.
        \end{equation*}

            \centering
            \begin{tabular}{c|c|c|c|c|c}
                Knot Interval & $\mathbf B_{1, \boldsymbol\tau, 0}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 1}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 2}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 3}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 4}(x)$ \\[5pt]
                \hline
                \rule{0pt}{2.5ex} $\left[0,\frac{1}{4}\right]$ & $1-4x$ & $4x$ & 0 & 0 & 0 \\[5pt]
                \hline
                \rule{0pt}{2.5ex} $\left[\frac{1}{4},\frac{1}{2}\right]$ & 0 & $2-4x$ & $-1+4x$ & 0 & 0 \\[5pt]
                \hline
                \rule{0pt}{2.5ex} $\left[\frac{1}{2},\frac{3}{4}\right]$ & 0 & 0 & $3-4x$ & $-2+4x$ & 0 \\[5pt]
                \hline
                \rule{0pt}{2.5ex} $\left[\frac{3}{4},1\right]$ & 0 & 0 & 0 & $4-4x$ & $-3+4x$
            \end{tabular}
            \captionof{table}{The polynomials for each basis function in $\mathbf B_{1, \left[0, 0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1, 1\right]}$ in their respecive knot intervals}
            \label{tab:linear-spline-evaluation}

\end{indentedexample}

\subsection{Addition and Subtraction}
Finding the sum of two B-splines that share the same basis is a simple task as their coefficients can just be added together:
\begin{equation}
    f(x) + g(x) =  \mathbf{B}_{p, \mathbf{t}}^{\top}(x) \mathbf{c}_1 + \mathbf{B}_{p, \mathbf{t}}^{\top}(x) \mathbf{c}_2
    = \mathbf{B}_{p, \mathbf{t}}^{\top}(x)(\mathbf{c}_1 + \mathbf{c}_2)
\end{equation}
If the spline functions are in different bases, however, they must be transformed to a common basis before the addition can be performed. So, for spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$, with degrees $p_1$ and $p_2$ and knot vectors $\mathbf{t}_1$ and $\mathbf{t}_2$, the goal is to find a common basis $\mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x)$ with degree $q$ and knot vector $\boldsymbol{\tau}$ such that $h(x) = \mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x) \mathbf{d} = f(x) + g(x)$.

To do this, the basis of the lowest degree is first elevated to the highest degree, before the knot vectors are refined to a common knot vector. The transformation matrices $\mathbf T_1$ and $\mathbf T_2$ from the original bases to the common basis are then found using \cref{eq:transformation-matrix-solution}. The new coefficients $\mathbf d$ are then given by
\begin{equation}
    \mathbf d = \mathbf T_1 \mathbf c_1 + \mathbf T_2 \mathbf c_2.
\end{equation}

\begin{algorithm}
    \caption{Common Basis}\label{alg:common-basis}
    \begin{algorithmic}[1]
        \State \textbf{Input:} B-Spline basis functions $\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x)$ and $\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)$
        \State \textbf{Input:} Desired degree $q$
        \Ensure $q \geq \max(p_1, p_2)$
        \State $\mathbf B_{q, \boldsymbol{\tau_1}} \gets$ result from \cref{alg:degree-elevation} with basis $\mathbf{B}_{p_1, \mathbf{t}_1}$ and degree $q$
        \State $\mathbf B_{q, \boldsymbol{\tau_2}} \gets$ result from \cref{alg:degree-elevation} with basis $\mathbf{B}_{p_2, \mathbf{t}_2}$ and degree $q$
        \State $\boldsymbol{\tau} \gets$ union of $\boldsymbol{\tau_1}$ and $\boldsymbol{\tau_2}$ with multiplicity for each knot set to the maximum of the two knot vectors
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol{\tau}}^{\top}(x)$
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \caption{Addition}\label{alg:addition}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$
        \State $q \gets \max(p_1, p_2)$
        \State $\mathbf B_{q, \boldsymbol{\tau}} \gets $ result from \cref{alg:common-basis} with bases $\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x)$ and $\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)$ and desired degree $q$
        \State $\mathbf T_1 \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_1, \mathbf{t}_1}$ to $\mathbf B_{q, \boldsymbol{\tau}}$
        \State $\mathbf T_2 \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_2, \mathbf{t}_2}$ to $\mathbf B_{q, \boldsymbol{\tau}}$
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol{\tau}}(x), \quad\mathbf T_1 \mathbf c_1 + \mathbf T_2 \mathbf c_2$
    \end{algorithmic}
\end{algorithm}

Scalar addition in the form of $f(x) + a$ can be performed by simply adding the scalar to all the coefficients of the B-spline. This is clear from the fact thats a scalar can be represented as a B-spline with degree $0$ (constant spline), and then performing the addition as described above. 

\subsection{Multiplication and Inner Product}\label{sec:multiplication}

For addition, the common basis had to have degree $q \geq \max(p_1, p_2)$, but for multiplication, the degree of the common basis must be $q = p_1 + p_2$\todo[inlinepar]{forklar hvorfor}. Intuitively, this is because the product of two polynomials of degree $p_1$ and $p_2$ will have degree of at most $p_1 + p_2$ while the sum of two polynomials of degree $p_1$ and $p_2$ will have degree of at most $\max(p_1, p_2)$.

In order to find the product of two B-splines, an approach similar to \cref{sec:basis-transformation} can be used. Given two spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$, the goal is to find a common basis $\mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x)$ with degree $q$ and knot vector $\boldsymbol{\tau}$ such that $h(x) = \mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x) \mathbf{d} = f(x)  g(x)$.

Writing the equation for the product of the two B-splines gives
\begin{subequations}\label{eq:b-spline-product}
    \begin{align}
        h(x) &= f(x) g(x) \label{eq:b-spline-product-1} \\
        &= \left(\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1\right) 
        \left(\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2\right) \label{eq:b-spline-product-2} \\
        &= \left(\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \otimes \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)\right) 
        \left(\mathbf{c}_1 \otimes \mathbf{c}_2\right) \label{eq:b-spline-product-3} \\
        \mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x) \mathbf{d} 
        &= \left(\mathbf{B}_{p_1, \mathbf{t}_1}(x) \otimes \mathbf{B}_{p_2, \mathbf{t}_2}(x)\right)^{\top} \left(\mathbf{c}_1 \otimes \mathbf{c}_2\right), \label{eq:b-spline-product-4}
    \end{align}
\end{subequations}

where $\otimes$ denotes the Kronecker product. In \cref{eq:b-spline-product-3}, the mixed product property of the Kronecker product has been used, while \cref{eq:b-spline-product-4} is obtained using the fact that the transpose is distributive over the Kronecker product \citep{LOAN200085}. \Cref{eq:b-spline-product-4} resembles the structure in \cref{eq:transformation-matrix-equation}, and the transformation matrix $\mathbf T$ from $\mathbf c_1 \otimes \mathbf c_2$ to $\mathbf d$ can similarily be found by sampling \cref{eq:b-spline-product-4} at the Greville abscissae of $\mathbf B_{q, \boldsymbol{\tau}}^{\top}(x)$. This is summarized in \cref{alg:multiplication}.

This algorithm can be made more efficient by exploiting the local support property of B-splines. The product of two B-splines is zero outside the union of the supports of the two B-splines, and the transformation matrix $\mathbf T$ will have many zero entries. So, instead of computing the Kronecker products $\mathbf B_{p_1, \mathbf{t}_1}(x) \otimes \mathbf B_{p_2, \mathbf{t}_2}(x)$ and $\mathbf c_1 \otimes \mathbf c_2$, which are column vectors of $p_1\cdot p_2$ entries, one can instead create vectors where only the non-zero entries of $\mathbf B_{p_1, \mathbf{t}_1}(x) \otimes \mathbf B_{p_2, \mathbf{t}_2}(x)$ and corresponding entries of $\mathbf c_1 \otimes \mathbf c_2$ are stored. 

\begin{algorithm}
    \caption{Product}\label{alg:multiplication}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$
        \State $q \gets p_1 + p_2$
        \State $\mathbf B_{q, \boldsymbol{\tau}} \gets $ result from \cref{alg:common-basis} with bases $\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x)$ and $\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)$ and desired degree $q$
        \State $\mathbf T \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_1, \mathbf{t}_1}^{\top}(x) \otimes \mathbf B_{p_2, \mathbf{t}_2}^{\top}(x)$ to $\mathbf B_{q, \boldsymbol{\tau}}^{\top}(x)$
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol{\tau}}(x), \quad\mathbf T (\mathbf c_1 \otimes \mathbf c_2)$
    \end{algorithmic}
\end{algorithm}


\begin{indentedexample}[B-Spline Product]
    \label{ex:linear-spline-multiplication}
    Consider the two B-splines
    \begin{equation*}
        f(x) = \mathbf{B}_{p_1, \mathbf t_1}^\top(x) \mathbf{c}_1
        \quad g(x) = \mathbf{B}_{p_2, \mathbf t_2}^\top(x) \mathbf{c}_2
    \end{equation*}
    with knot vectors $\mathbf t_1 = \left\{0, 0, \frac{1}{3}, \frac{2}{3}, 1, 1\right\}$,  $\mathbf t_2 = \left\{0, 0, 0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1, 1, 1\right\}$ and degrees  $p_1 = 1$, $p_2 = 2$. 
    The common basis $\mathbf{B}_{q,\boldsymbol{\tau}}$ is then found using \cref{alg:common-basis}, to get:
    \begin{equation*}
        q = p_1 + p_2 = 3,
        \quad
        \boldsymbol{\tau} = \left\{0, 0, 0, 0, \tfrac{1}{4}, \tfrac{1}{4}, \tfrac{1}{3}, \tfrac{1}{3}, \tfrac{1}{3}, \tfrac{1}{2}, \tfrac{1}{2}, \tfrac{2}{3}, \tfrac{2}{3}, \tfrac{2}{3}, \tfrac{3}{4}, \tfrac{3}{4}, 1, 1, 1, 1\right\},
    \end{equation*}
    where the internal knots of $\mathbf t_1$ and $\mathbf t_2$ have been duplicated twice and once respectively to account for the degree elevation. 
    The left hand side of \cref{eq:b-spline-product-4} is found as in \cref{eq:transformation-matrix}, sampling $\mathbf B_{q, \boldsymbol{\tau}}(x)$ at the Greville abscissae $\boldsymbol{\zeta_\tau}$.
    For the right hand side, we can write out the product of the B-splines in matrix form using the Kronecker product:
    \begin{equation*}
        \begin{aligned}
            (\mathbf B_{p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) \otimes \mathbf B_{p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}))^\top \mathbf{c}_1 \otimes \mathbf{c}_2 = \begin{bmatrix}
                B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{0, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
                B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{1, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
                B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{2, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
                \vdots \\
                B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{m, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
                B_{1, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{0, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
                \vdots \\
                B_{n, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{m, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
            \end{bmatrix}^\top 
            \begin{bmatrix}
                c_{0, 1} c_{0, 2} \\
                c_{0, 1} c_{1, 2} \\
                c_{0, 1} c_{2, 2} \\
                \vdots \\
                c_{0, 1} c_{m, 2} \\
                c_{1, 1} c_{0, 2} \\
                \vdots \\
                c_{n, 1} c_{m, 2}
            \end{bmatrix}                
        \end{aligned}
    \end{equation*}
    This is a matrix with $p_1 \cdot p_2$ rows and column number equal to the number of Greville abscissae in $\boldsymbol{\tau}$, which depend on the degrees of $\mathbf B_{p_1, \mathbf{t}_1}$ and $\mathbf B_{p_2, \mathbf{t}_2}$ and how many knots they share.
    In order to save computational resources, the products with non-overlapping supports should be ignored. In this example the supports of the two B-splines are given in \cref{tab:ex-spline-supports}. Here we only get 6 zero-entries, but as the knot vectors get denser, this number will increase.


    \centering
    \small
    \begin{tabular}{c|c||c|c|c|c|c|c}
        \textbf{Basis} &  & $B_{0, p_2, \mathbf{t}_2}$ & $B_{1, p_2, \mathbf{t}_2}$ & $B_{2, p_2, \mathbf{t}_2}$ & $B_{3, p_2, \mathbf{t}_2}$ & $B_{4, p_2, \mathbf{t}_2}$ & $B_{5, p_2, \mathbf{t}_2}$ \\[5pt]
        \hline
        \rule{0pt}{2.5ex} & \textbf{Support} & $\left[0, \frac{1}{4}\right]$ & $\left[0, \frac{1}{2}\right]$ & $\left[0, \frac{3}{4}\right]$ & $\left[\frac{1}{4}, \frac{3}{4}\right]$ & $\left[\frac{1}{2}, 1\right]$ & $\left[\frac{3}{4}, 1\right]$ \\[5pt]
        \hline\hline
        \rule{0pt}{2.5ex}$B_{0, p_1, \mathbf{t}_1}$ & $\left[0, \frac{1}{3}\right]$ & 
        1 & 1 & 1 & 1 & 0 & 0 \\[5pt]\hline\rule{0pt}{2.5ex}
        $B_{0, p_1, \mathbf{t}_1}$ & $\left[0, \frac{2}{3}\right]$ &
        1 & 1 & 1 & 1 & 1 & 0 \\[5pt]\hline\rule{0pt}{2.5ex}
        $B_{2, p_1, \mathbf{t}_1}$ & $\left[\frac{1}{3}, 1\right]$ &
        0 & 1 & 1 & 1 & 1 & 1 \\[5pt]\hline\rule{0pt}{2.5ex}
        $B_{2, p_1, \mathbf{t}_1}$ & $\left[\frac{2}{3}, 1\right]$ & 
        0 & 0 & 1 & 1 & 1 & 1 \\[5pt]
    \end{tabular}
    \captionof{table}{Overlapping supports of $B_{p_1, \mathbf{t}_1}$ and $B_{p_2, \mathbf{t}_2}$ in \cref{ex:linear-spline-multiplication}.}
    \label{tab:ex-spline-supports}

\end{indentedexample}

The inner product of two (vector-valued) functions is a scalar that measures the similarity between the functions. combines the components of the two vector-valued functions. This operation is useful in applications such as calculating projections, or defining cost functions in optimization problems. The $\mathcal L_2$ inner product (henceforth referred to as the inner product)for vector-valued functions is defined as:
\begin{equation}
    \label{eq:dot-product}
    \langle\mathbf f(x), \mathbf g(x)\rangle = \int_{-\infty}^\infty\sum_{i=1}^m f_i(x) g_i(x)
\end{equation}
where $\mathbf f, \mathbf g: \mathbb{R} \to \mathbb{R}^m$, $\mathbf f(x) = [f_1(x), \ldots, f_m(x)]^\top$ and $\mathbf g(x) = [g_1(x), \ldots, g_m(x)]^\top$ are vector-valued functions with $m$ components. The expression inside the integral also carries useful information, and one may refer to it as the pointwise (PW) inner product which is defined as
\begin{equation}
    \label{eq:dot-product-pointwise}
    \mathbf f(x) \cdot \mathbf g(x) = \sum_{i=1}^m f_i(x) g_i(x).
\end{equation}

When $\mathbf f$ and $\mathbf g$ are spline functions, the PW inner product can be expressed in terms of their B-spline representations as
\begin{equation}
    \label{eq:dot-product-spline}
    \begin{aligned}
        \langle\mathbf f(x), \mathbf g(x)\rangle &= \langle\mathbf B_{p_1, \mathbf t_1}^\top(x) \mathbf c_1, \mathbf B_{p_2, \mathbf t_2}^\top(x) \mathbf c_2\rangle \\
        &= \sum_{i=1}^m \left(\mathbf B_{p_1, \mathbf t_1}^\top(x) \mathbf c_{1,i}\right) \left(\mathbf B_{p_2, \mathbf t_2}^\top(x) \mathbf c_{2,i}\right) \\
        &= \sum_{i=1}^m \left(\mathbf B_{p_1, \mathbf t_1}^\top(x) \otimes \mathbf B_{p_2, \mathbf t_2}^\top(x)\right) \left(\mathbf c_{1,i} \otimes \mathbf c_{2,i}\right) \\
        &= \left(\mathbf B_{p_1, \mathbf t_1}(x) \otimes \mathbf B_{p_2, \mathbf t_2}(x)\right)^{\top} \sum_{i=1}^m \mathbf c_{1,i} \otimes \mathbf c_{2,i},
    \end{aligned}
\end{equation}
where $\mathbf c_{1,i}$ and $\mathbf c_{2,i}$ are the coefficients of the $i$-th component of $\mathbf f$ and $\mathbf g$, respectively. Notice that the expression $\left(\mathbf B_{p_1, \mathbf t_1}(x) \otimes \mathbf B_{p_2, \mathbf t_2}(x)\right)^{\top}$ is found in both \cref{eq:b-spline-product-4} and \cref{eq:dot-product-spline}. This means that the transformation matrix $\mathbf T$ for the inner product can be computed in the same way as for the multiplication, using \cref{alg:multiplication}.
This is due to the fact that per \cref{eq:transformation-matrix-solution}, the transformation matrix $\mathbf T$ is independent of the coefficients $\mathbf c_{1}$ and $\mathbf c_{2}$.

So, to find the PW inner product $h(x) = \mathbf f(x) \cdot \mathbf g(x)$, \cref{alg:multiplication} is used on the bases of $\mathbf f$ and $\mathbf g$, and the coefficients $\mathbf d$ of $h(x) = \mathbf B_{q, \boldsymbol{\tau}}^\top(x) \mathbf d$ are given by
\begin{equation}
    \mathbf d = \mathbf T \sum_{i=1}^m (\mathbf c_{1,i} \otimes \mathbf c_{2,i}).
\end{equation}

As for the regular product, the PW inner product can be made more efficient by not computing the terms in the Kronecker product that correspond to non-overlapping supports of the basis functions. The most common case for applying the PW inner product is between two vector-valued B-splines of the same basis, in which case the number of non-zero entires is $n(p+1)$, where $n$ is the number of basis functions and $p$ is the degree of the B-spline. This is linear in the number of basis functions, instead of quadratic as for when this optimization is not used. See explanation in \cref{fig:overlapping-bases}. Finding a general formula arbitrary bases is not as easily acheivable, as the number of non-zero entries depends on the placement of the knots in the knot vectors.

\begin{figure}
    \centering
    \includesvg[width=\textwidth,pretex=\footnotesize]{fig/b-spline/overlapping-bases.svg}
    \caption{The overlapping supports of the B-splines. Each rectangle represents the support of a basis function, so counting the number of basis functions withing each knot interval gives the number of multiplications for that interval. The total number of multiplications then the area of all the rectangles, which is $n(p+1)$, where $n$ is the number of basis functions and $p$ is the degree of the B-spline.}
    \label{fig:overlapping-bases}
\end{figure}


\section{Rational B-spline expression to NURBS conversion}


To convert a general rational B-spline
$$
    f(x) = \frac{N(x)}{D(x)},
$$
where $N(x)=\mathbf B_{p_N,\mathbf t_N}^\top\mathbf n$ and $D(x)=\mathbf B_{p_D,\mathbf t_D}^\top\mathbf d$ are each in separate B-spline bases, into the standard NURBS form, one applies degree elevation and knot insertion \citep{Piegl1997} to obtain a common basis ${B_{q,i}(x)}$:
\begin{enumerate}
    \item Elevate both $N$ and $D$ to degree $q\ge\max{p_N,p_D}$ using \cref{alg:degree-elevation}.
    \item Refine knot vectors so that both share the same sequence using \cref{alg:knot-refinement}.
\end{enumerate}
This yields
$$
    \widetilde N(x) = \sum_{i=0}^M \tilde n_i\,N_{q,i}(x),\quad \widetilde D(x) = \sum_{i=0}^M \tilde d_i\,N_{q,i}(x),
$$
from which the NURBS weights and control values follow:
$$
    w_i = \tilde d_i, \quad
    c_i = \frac{\tilde n_i}{\tilde d_i}.
$$
The resulting representation,
$$
    f(x) = \frac{\sum_i w_i\,c_i\,N_{q,i}(x)}{\sum_i w_i\,N_{q,i}(x)},
$$
is a degree-$q$ NURBS with weights $w_i$, knot vector $\boldsymbol{\tau}$ and control points $c_i$. This is summarized in \cref{alg:nurbs-conversion}.

\begin{algorithm}
    \caption{Convert rational B-spline to NURBS}\label{alg:nurbs-conversion}
    \begin{algorithmic}[1]
        \State \textbf{Input:} B-splines $N(x)$ and $D(x)$ with correspondingknot vectors $\mathbf t_N$ and $\mathbf t_D$, and degrees $p_N$ and $p_D$
        \State $q \gets \max(p_N, p_D)$
        \State $\mathbf B_{q,\boldsymbol{\tau}}(x) \gets $ result from \cref{alg:common-basis} with bases $\mathbf B_{p_N,\mathbf t_N}(x)$ and $\mathbf B_{p_D,\mathbf t_D}(x)$ and desired degree $q$
        \State $\mathbf T_N \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_N,\mathbf t_N}(x)$ to $\mathbf B_{q,\boldsymbol{\tau}}(x)$
        \State $\mathbf T_D \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_D,\mathbf t_D}(x)$ to $\mathbf B_{q,\boldsymbol{\tau}}(x)$
        \State $\mathbf{\tilde n} \gets \mathbf T_N \mathbf n$
        \State $\mathbf{\tilde d} \gets \mathbf T_D \mathbf d$
        \State $\mathbf w \gets \mathbf{\tilde d}$
        \State $\mathbf c \gets \mathbf{\tilde n} \oslash \mathbf{\tilde d}$ \Comment{element-wise division}
        \State \textbf{Output:} Degree $q$ NURBS curve $\displaystyle r(x) = \frac{N(x)}{D(x)}$ with weights $w_i$, knot vector $\boldsymbol{\tau}$ and control points $c_i$
    \end{algorithmic}
\end{algorithm}








\section{Summary}

The complete B-spline MINMPC problem can be written compactly as
\begin{equation}
\label{eq:minmpc-compact}
    \begin{aligned}
        \min
        \quad & J(\mathbf p_\text{OS}) + w_\text{time}\, \mathbf t(1)
        \\
        \text{subject to}\quad
        & \mathcal{D}_{DI}(\mathbf{p}_\text{OS})
        \text{ or }
        \mathcal{D}_{PH}(\mathbf{p}_\text{OS}),
        \\
        & \mathcal{O}_j\bigl(
            \mathbf{n}_{p,j}, \mathbf{n}_{s,j}, b_{p,j}, b_{s,j}, z_j
        \bigr),
        \quad \forall j\in \{1,\ldots,N_\text{TS}\},
    \end{aligned}
\end{equation}

where:
\begin{itemize}
    \item $J$ includes maneuver window costs, reference trajectory following costs, and acceleration costs as defined in \cref{eq:cost-maneuver-window-total}.
    \item $\mathcal{D}_{DI}$ and $\mathcal{D}_{PH}$ denote the double‐integrator and PH‐spline Dubins model constraints, respectively.
    \item $\mathcal{O}_j$ is the Big-M collision‐avoidance constraint set from \eqref{eq:colregs-bigM} for each target ship $j$ in the encounter.
    \item $w_\text{time}$ is a weight on the final time of the variable $\mathbf t(x)$, which is a B-spline variable representing the time of the OS.
\end{itemize}

The optimization variables are the B-spline coefficients of the spline functions
$\mathbf{p}_\text{OS}(t)$, $\mathbf t(x)$, $\mathbf{n}_{p,j}(x)$, $\mathbf{n}_{s,j}(x)$, $b_{p,j}(x)$, $b_{s,j}(x)$ defined over a suitable knot vector $\mathbf{t}\in[0,1]$, and the binary variables $z_j$.


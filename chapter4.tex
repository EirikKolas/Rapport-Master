% !TeX root = main.tex
%===================================== CHAP 4 =================================

\chapter{B-spline MINMPC}\label{chap:b-spline-minmpc}

This chapter presents a unified framework for \acrfull{MINMPC} based on B‐spline trajectory representations. Ship motion, timing and collision‐avoidance constraints are expressed as polynomial inequalities in the B‐spline coefficients, yielding a smooth, continuous formulation capable of minimum‐time planning under dynamic and COLREGS requirements. Two vessel models are introduced: a spline‐relaxed double‐integrator and a Pythagorean‐hodograph (PH) Dubins model, each producing convex or polynomial constraints on velocity and acceleration. Collision avoidance against multiple targets is enforced via hyperplane separation and a Big–M mixed‐integer approach, allowing port‐ or starboard‐side passage decisions. A structured cost combines reference tracking, maneuver windows, and minimum‐time objectives, leveraging inverse‐Gramian norms to suppress high‐frequency oscillations. The complete MINMPC formulation is implemented in a Python/CasADi library with modular components for own‐ship, target‐ships and reference trajectories. 

The chapter is organized into four parts: (i) dynamic models, (ii) collision constraints, (iii) objective design, and (iv) software implementation.


\section{Dynamic Model}

Two continuous‐time vessel models are relaxed into B‐spline form for MINMPC. First, a double‐integrator model enforces continuous position, velocity and acceleration with speed and thrust limits. Second, a Pythagorean‐hodograph Dubins model incorporates heading‐dependent motion and turn‐rate constraints. Each formulation yields convex or polynomial inequalities in the spline coefficients, enabling smooth, minimum‐time trajectory optimization under dynamic and COLREGS requirements.


\subsection{Double Integrator}\label{sec:double-integrator}
The double integrator model is a simple model that ensures a continuous position, velocity, and acceleration for the ship. The model is described by the following equations:
\begin{subequations}\label{eq:double-integrator}
    \begin{align}
        \dot{\mathbf{p}}(t) &= \mathbf{v}(t), \label{eq:double-integrator-x} \\
        \dot{\mathbf{v}}(t) &= \mathbf{a}(t), \label{eq:double-integrator-v} \\
        \|\mathbf{v}(t)\|_2 &\leq v_{\max}, \label{eq:double-integrator-vmax} \\
        \|\mathbf{a}(t)\|_2 &\leq a_{\max}, \label{eq:double-integrator-a}
    \end{align}
\end{subequations}
where $\mathbf{p} = [p_N, p_E]^\top$ denotes the position of the ship, $\mathbf{v} = [v_N, v_E]^\top$ is the velocity, and $\mathbf{a} = [a_N, a_E]^\top$ is the acceleration in North-East coordinates. The $\|\cdot\|_2$ notation denotes the Euclidean norm, and $v_{\max}$ and $a_{\max}$ are the maximum speed and acceleration of the ship, respectively.

This continuous model can be relaxed to a B-spline model by letting $\mathbf{p}(x)$, $\mathbf{v}(x)$, and $\mathbf{a}(x)$ be spline functions on a chosen B-spline basis. It is desirable to let time be a variable in the optimization problem so that minimum time trajectories can be found. It is impractical to let the knot values of the B-spline representation of $\mathbf{p}(x)$ be optimization variables as an analysis of the Cox-de Boor recursion formula in \cref{eq:b-spline-recurrence} shows that the spline function is non-linear in these knot values. More specifically a degree $p$ B-spline with the parameter $x$ fixed, will be a polynomial of degree $p$ in the knot values. Instead, the time variable $t$ is introduced as a function of the parameter $x$, which is a common approach in these types of problems \citep{mercy2017spline,ShortestPathsConvexSets}. 

With this approach, the velocity and acceleration can now be expressed as
\begin{subequations}\label{eq:double-integrator-spline}
    \begin{align}
        \mathbf{v}(t) &= \frac{\partial \mathbf{p}}{\partial x} \frac{\partial x}{\partial t} = \frac{\mathbf{p}'}{t'},
        \label{eq:double-integrator-v-spline} \\
        \mathbf{a}(t) &= \frac{\partial \mathbf{v}}{\partial t} = \frac{\partial}{\partial t} \left(\frac{\mathbf{p}'}{t'}\right) = 
        \frac{
            t'\frac{\partial \mathbf{p}'}{\partial t} - \mathbf{p}'\frac{\partial t'}{\partial t}
            }{(t')^2} = 
        \frac{\mathbf{p}'' - \mathbf{p}'\frac{t''}{t'}}{(t')^2}.
        \label{eq:double-integrator-a-spline}
    \end{align}
\end{subequations}
Here $(\cdot)'$ and $(\cdot)''$ denotes the first and second derivative with respect to the spline parameter $x$, respectively. In \cref{eq:double-integrator-spline}, the inverse function theorem was used to flip the relation between $t$ and $x$. This is only possible if $t$ is a strictly increasing function of $x$, which is the case if the constraint 
\begin{equation}\label{eq:constraint-t}
    t'(x) > 0
\end{equation}
holds.  
If we in addition require the time to be positive, the velocity constraint in \cref{eq:double-integrator-vmax} becomes
\begin{equation}\label{eq:double-integrator-vmax-spline}
    \begin{aligned}
        &&\|\mathbf{v}(t)\|_2 = \left\|\frac{\mathbf{p}'}{t'}\right\|_2 &\leq v_{\max} \\
        &\implies& \|\mathbf{p}'(x)\|_2 &\leq t'(x)v_{\max} \\
        &\implies& \mathbf{p}'(x)^\top\mathbf{p}'(x) &\leq (t'(x))^2v_{\max}^2.
    \end{aligned}
\end{equation}
To simplify the model, yet another restriction is put on the time function $t(x)$ by requiring it to be a 1st degree B-spline function. This means that the time variable is a piecewise linear function of the parameter $x$, and the second derivative of $t$ is zero, giving the simplified acceleration constraint
\begin{equation}\label{eq:double-integrator-a-spline-simplified}
    \begin{aligned}
        &&\|\mathbf{a}(t)\|_2 = \left\|\frac{\mathbf{p}''}{(t')^2}\right\|_2 &\leq a_{\max} \\
        &\implies& \|\mathbf{p}''(x)\|_2 &\leq t'(x)^2a_{\max} \\
        &\implies& \mathbf{p}''(x)^\top\mathbf{p}''(x) &\leq t'(x)^4a_{\max}^2.
    \end{aligned}
\end{equation}

Now the B-Spline relaxation is fully given by the following equations:
\begin{subequations}\label{eq:double-integrator-spline-complete}
    \begin{align}
        \mathbf{p}(x) &= \sum_{i=0}^{n} \mathbf{p}_i B_{i,p,\mathbf{t}}(x), \label{eq:double-integrator-spline-complete-p} \\
        t(x) &= \sum_{i=0}^{m} t_i B_{i,1,\boldsymbol{\tau}}(x), \label{eq:double-integrator-spline-complete-t} \\
        \mathbf{p}'(x) ^\top \mathbf{p}'(x) &\leq t'(x)^2v_{\max}^2, \label{eq:double-integrator-spline-complete-vmax} \\
        \mathbf{p}''(x) ^\top \mathbf{p}''(x) &\leq t'(x)^4a_{\max}^2, \label{eq:double-integrator-spline-complete-a} \\
        t(x) &\geq 0, \label{eq:double-integrator-spline-complete-t-constraint} \\
        t'(x) &> 0. \label{eq:double-integrator-spline-complete-t-derivative} \end{align}
\end{subequations}
The only optimization variables needed are the B-Spline coefficients $\mathbf{p}_i\in\mathbb{R}^2$ and $t_i\in\mathbb{R}^+$ from \cref{eq:double-integrator-spline-complete-p,eq:double-integrator-spline-complete-t}, which are subject to the constraints \cref{eq:double-integrator-spline-complete-vmax,eq:double-integrator-spline-complete-a,eq:double-integrator-spline-complete-t-constraint,eq:double-integrator-spline-complete-t-derivative}.


\subsection{Dubins Model}\label{sec:dubins-model}
The double integrator model is a simple model that ensures a continuous position, velocity, and acceleration for the ship. However, it does not take into account the ship's heading and turning radius. Dubins model (also referred to as the unicycle model) is a more realistic model for ship dynamics, as it describes a vehicle moving in a plane where the velocity points in the direction of the heading. In the motion control literature, there exists a variety of parameterizations for this kind of model. Notably \citet{mercy2017spline}, which uses the tangent half-angle substitution to represent the heading angle in a B-spline framework and \citet{Wang2020}, which uses a convex relaxation technique with sequential convex programming to exchange nonlinear equality constraints with convex inequalities. 

The model in \cite{Wang2020} was discarded as the assumtions for the relaxation technique are not valid under a reference following scenario. \cite{mercy2017spline} used the tangent half-angle to parameterize the bicycle model, which also includes the steering angle as a control input. In this work, the simplified Dubins model was tested, but ultimately discarded as the Pythogerean-hodograph (PH) B-spline model has the same properties as the tangent half-angle model, but has in addition all the nice properties described in \cref{sec:pythogerean-hodograph}. See \cref{app:tangent-half-angle} for a more detailed description of the Dubins model and the tangent half-angle substitution.

Dubins model can be described by the following equations:
\begin{subequations}\label{eq:dubins-model}
    \begin{align}
        \dot p_N(t) &= V \cos(\chi(t)),       \label{eq:dubins-x} \\
        \dot p_E(t) &= V \sin(\chi(t)),       \label{eq:dubins-y} \\
        \dot \chi(t) &= \omega(t),          \label{eq:dubins-chi} \\
        |\omega(t)| &\leq \omega_{\max}, \label{eq:dubins-omega} \\
        0 \leq V &\leq V_{\max},      \label{eq:dubins-V} 
    \end{align}
\end{subequations}
where $p_N$ and $p_E$ are the North and East position coordinates of the ship, and $\chi$ is the heading angle. The control input is the speed $V$ and turn rate $\omega$. Using the PH B-spline model from \cref{sec:pythogerean-hodograph}, the Dubins model can be expressed as
\begin{subequations}\label{eq:dubins-model-ph}
    \begin{align}
        \dot p_N(x) &= u(x)^2 - v(x)^2, \label{eq:dubins-ph-x} \\
        \dot p_E(x) &= 2u(x)v(x),       \label{eq:dubins-ph-y} \\
        \dot \chi(x) &= \omega(x) 
        = \frac{2}{t'(x)}\frac{\big(u(x)v'(x) - u'(x)v(x)\big)}{u(x)^2 + v(x)^2},  \label{eq:dubins-ph-chi} \\
        |\omega(x)| &\leq \omega_{\max},  \\
        V(x) &= u(x)^2 + v(x)^2 \\
        0 \leq V(x) &\leq V_{\max}, 
    \end{align}
\end{subequations}
where \cref{eq:dubins-ph-x,eq:dubins-ph-y} correspond to the PH parameterization in \cref{eq:ph-b-spline} and the turn rate \cref{eq:dubins-ph-chi} follows from \cref{eq:turn-rate} which is given in \cref{app:ph-spline-derivation}. The time variable $t(x)$ is introduced as a function of the parameter $x$, similar to the double integrator model in \cref{sec:double-integrator}. The speed $V(x)$ is given by a polynomial expression in the B-spline variables $u(x)$ and $v(x)$, and is therefore a B-spline expression as well. 

To implement this model in an optimization framework, $u(x)$ and $v(x)$ are represented as B-spline functions, and the turn-rate constraints are derived as follows
\begin{equation}
    \begin{aligned}
        |\omega(x)| &\le \omega_{\max} \\
        \implies \left|\frac{2}{t'(x)}\frac{\big(u(x)v'(x) - u'(x)v(x)\big)}{u(x)^2 + v(x)^2}\right| &\le \omega_{\max} \\
        \implies \left|u(x)v'(x) - u'(x)v(x)\right| &\le \frac{\omega_{\max}}{2}t'(x)V(x),
    \end{aligned}
\end{equation}
yeilding the polynomial constraints
\begin{subequations}\label{eq:dubins-ph-turn-rate}
    \begin{align}
        u(x)v'(x) - u'(x)v(x) &\le \frac{\omega_{\max}}{2}t'(x)V(x), \\
        u(x)v'(x) - u'(x)v(x) &\ge -\frac{\omega_{\max}}{2}t'(x)V(x).
    \end{align}
\end{subequations}
To retrieve the position $\mathbf{p}(x)$, the velocity is multiplied by the time derivative and integrated:
\begin{equation}\label{eq:dubins-ph-position}
    \mathbf{p}(x) = \mathbf p(t_0) 
    + \int_0^x \begin{bmatrix} 
        u(s)^2-v(s)^2 \\ 2 u(s)v(s)
    \end{bmatrix} t'(s) ds,
\end{equation}
where $\mathbf p(0)$ is the initial position of the ship at $t=0$.

\section{Target Ships}
Target ships (TS) are treated as dynamic obstacles whose trajectories are approximated by simple spline‐based kinematic models. TS motion is typically represented by a constant‐velocity B‐spline, enabling analytic position evaluation and integration. Collision‐avoidance constraints against each TS are then formulated via hyperplane separation within the same spline framework, ensuring continuous minimum‐separation guarantees. Mixed integer programming (MIP) is employed to enforce port or starboard passage decisions, allowing flexible COLREGS‐compliant maneuvers.

\subsection{Constant Velocity Model}\label{sec:constant-velocity-model}
The \acrshort{TS} is modeled as a constant velocity model:
\begin{equation}
    \dot{\mathbf{p}}_{\text{TS}}(t) = \mathbf{v}_{\text{TS}}(t)
\end{equation}
To fit this model into the B-spline framework with time as a variable, the velocity of the \acrshort{TS} is expressed as a B-spline function of the parameter $x$, and the position is retrieved by integrating the velocity multiplied by the time derivative:
\begin{equation}\label{eq:ts-constant-velocity}
    \mathbf{p}_{\text{TS}}(x) = \mathbf p_\text{TS}(0) + \int_0^x \mathbf{v}_{\text{TS}}(s) t'(s) ds,
\end{equation}
where $\mathbf p_\text{TS}(t_0)$ is the initial position of the \acrshort{TS} at $t=0$. 

Unfortunately, using time as a variable $t(s)$ in the optimization problem, does not allow for anything other than constant velocity TSs, as the velocity as a function of the parameter $x$ is not a polynomial function of the position parameterized by the actual time passed $t$. In most cases, a constant velocity model is sufficient, as the \acrshort{TS} is usually not maneuvering. However, if a more complex model is needed, the optimization problem can be formulated with the time as a fixed variable, or by parameterizing on $t$ directly instead of $x$. To keep notation consistent, the former is used in this work.

\subsection{Hyperplane Separation Theorem}
A simple approach to enforce collision constraints between the OS and TS is to apply a minumum distance constraint between the two ships as
\begin{equation}\label{eq:minimum-distance}
    (\mathbf p_{\text{OS}} - \mathbf p_{\text{TS}})^\top (\mathbf p_{\text{OS}} - \mathbf p_{\text{TS}}) \geq d_{\text{min}}^2,
\end{equation}
where $\mathbf p_{\text{OS}}$ and $\mathbf p_{\text{TS}}$ are the positions of the OS and TS, respectively, and $d_{\text{min}}$ is the minimum distance between the two ships. This constraint is non-convex, as the feasible region for the OS lies outside the circle with radius $d_{\text{min}}$ centered at the TS.

Another method of enforcing collision avoidance is to use the hyperplane separation theorem. Although still non-convex, this method allows for a more flexible formulation of the constraint, as it is not only restricted to circular shapes. 
The hyperplane separation theorem states that for two disjoint convex sets $\mathcal A$ and $\mathcal B$, there exists $\mathbf n\in \mathbb R^n\backslash\{\mathbf0\}$ and $b\in\mathbb R$ such that the hyperplane $H=\{\mathbf x\in\mathbb R^n \mid \mathbf n^\top \mathbf x = b\}$ separates $\mathcal A$ and $\mathcal B$ \citep{Boyd2004-ih}. In other words, there exists a function $a^\top x - b$ that is non-negative for all $x \in \mathcal A$ and non-positive for all $x \in \mathcal B$. 

Using this theorem, the collision avoidance task can be transformed to a classification problem. The objective is to find a line that separates the OS and TS, such that the distance between the line and the ships is greater than a given threshold. This is done in the B-spline framework by introducing a hyperplane $\mathcal H$ with normal $\mathbf n(t)\in\mathcal{S}^2_{p,\mathbf t}$ and offset $b(t)\in\mathcal{S}_{p,\mathbf t}$, which separates the OS and TS. This hyperplane is implemented using the following constraints:
\begin{subnumcases}{\label{eq:minimum-distance-hyperplane}\mathcal{H}:}
    \mathbf p_\text{OS}(t)^\top{\mathbf n}(t) & $\ge b(t) + d_\text{OS}$,
    \label{eq:hyperplane-os} \\
    \mathbf p_\text{TS}(t)^\top{\mathbf n}(t) & $\le b(t) - d_\text{TS}$,
    \label{eq:hyperplane-ts} \\
    \|{\mathbf n}(t)\|_\infty & $\le 1$.
    \label{eq:hyperplane-norm}
\end{subnumcases}
The constraints in \zcref{eq:hyperplane-os,eq:hyperplane-ts} are enforced by
letting $\mathbf n(t)$ and $b(t)$ be optimization variables. Equation \zcref{eq:hyperplane-norm} is a box constraint on $\mathbf n(t)$, ensuring that the hyperplane normal doesn't become too large. The minimum distance constraint in \cref{eq:minimum-distance} can be equivalently expressed using the hyperplane separation theorem by letting $d_\text{OS} = 0$ and $d_\text{TS} = d_\text{min}$ in \zcref{eq:hyperplane-os,eq:hyperplane-ts}. As the following section shows,

\subsection{Collision Constraints}\label{sec:collision-constraints}

Collision avoidance is non-convex: with fixed start/end points one cannot deform a trajectory from one side of the obstacle to the other without intersection. A convex solver will therefore remain on the same side (see \cref{fig:non-convex-obstacle}), potentially missing the globally optimal path.
\begin{figure}
    \centering
    \includesvg[width=0.8\textwidth]{fig/b-spline/non-convex-obstacle.svg}
    \caption{Black dots: fixed start/end points. Dotted line: current trajectory. Solid line: reference. Red circle: obstacle. A convex solver cannot switch sides without violating the obstacle constraint.}
    \label{fig:non-convex-obstacle}
\end{figure}

For designing a COLREGS-compliant trajectory, being able to decide which side of a given target ship to pass on is essential. So to address the aforementioned issue, which side to pass the obstacle on is made a decision variable in a mixed integer programming (MIP) problem as follows:

The idea is that each target ship is represented by two obstacles, each obstacle having only one feasible side to be passed on. The passing side of the target ship is enforced by adding additional points to each obstacle on the opposide side of the passing side using the normal $\mathbf{\hat n}_\text{ref}$ of the reference trajectory $\mathbf p_\text{ref}$. The two obstacles are then moved into and out of the feasible region by introducing a binary variable $z_j\in\{0,1\}$ for each target ship $j$ which controls an offset $M \mathbf{\hat n}_\text{ref}$, where $M$ is a large positive number. More formally, the constraints are given by

\begin{subnumcases}{\label{eq:colregs-bigM}\mathcal{O}_j:}
    \mathbf{n}_{p,j}(t)^\top\mathbf{p}_{\mathrm{OS}}(t)
      & \label{eq:colregs-bigM-os-p}$\ge b_{p,j}(t) + d_{\text{OS}}$\\
    \mathbf{n}_{p,j}(t)^\top\mathbf{p}_{\mathrm{TS},j}(t)
      & \label{eq:colregs-bigM-ts-p}$\le b_{p,j}(t) - d_{\text{TS},j} + M\,z_j$\\
    \mathbf{n}_{p,j}(t)^\top\bigl(\mathbf{p}_{\mathrm{TS},j}(t)+2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)\bigr)
      & \label{eq:colregs-bigM-ts-o-p}$\le b_{p,j}(t) - M$\\[1.5ex]
      %
    \mathbf{n}_{s,j}(t)^\top\mathbf{p}_{\mathrm{OS}}(t)
      & \label{eq:colregs-bigM-os-s}$\ge b_{s,j}(t) + d_{\text{OS}}$\\
    \mathbf{n}_{s,j}(t)^\top\mathbf{p}_{\mathrm{TS},j}(t)
      & \label{eq:colregs-bigM-ts-s}$\le b_{s,j}(t) - d_{\text{TS},j} + M\,(1-z_j)$\\
    \mathbf{n}_{s,j}(t)^\top\bigl(\mathbf{p}_{\mathrm{TS},j}(t)-2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)\bigr)
      & \label{eq:colregs-bigM-ts-o-s}$\le b_{s,j}(t) - M$\\[1.5ex]
    z_j \in \{0,1\} & \label{eq:colregs-bigM-z} \\
    \|\mathbf{n}_{p,j}(t)\|_\infty\le 1 & \label{eq:colregs-bigM-np} \\
    \|\mathbf{n}_{s,j}(t)\|_\infty\le 1 & \label{eq:colregs-bigM-ns}
\end{subnumcases}
    
Here,
\begin{itemize}
    \item $\mathbf{n}_{p,j}(t)$ and $b_{p,j}(t)$ are the port‐side hyperplane normal and offset,
    \item $\mathbf{n}_{s,j}(t)$ and $b_{s,j}(t)$ are the starboard‐side normal and offset,
    \item $d_{\text{OS}}$ and $d_{\text{TS},j}$ are the required minimum separations for the OS and TS from the hyperplane, 
    \item $z_j\in\{0,1\}$ selects port‐side ($z_j=0$) or starboard‐side ($z_j=1$) passage,
    \item $\mathbf{p}_{\mathrm{TS},j}(t)+2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)$ and $\mathbf{p}_{\mathrm{TS},j}(t)-2M\,\mathbf{\hat n}_{\mathrm{ref}}(t)$ are the two obstacles on the opposite side of the passing side,
    \item $M$ is a large constant that renders inactive constraints nonbinding.
    \item and $\mathbf{\hat n}_{\mathrm{ref}}(t)$ is the unit normal of the reference trajectory $\mathbf p_\text{ref}(t)$, defined as
    \begin{equation}\label{eq:reference-normal}
        \mathbf{\hat n}_{\mathrm{ref}}(t) = \mathbf R\frac{\mathbf p_\text{ref}'(t)}{\|\mathbf p_\text{ref}'(t)\|_2}.
    \end{equation}
    where $\mathbf R$ is a rotation matrix that rotates 90 degrees counterclockwise around the down axis in a North-East-Down coordinate system.
\end{itemize}
The first three inequalities
(\zcref{eq:colregs-bigM-os-p,eq:colregs-bigM-ts-p,eq:colregs-bigM-ts-o-p})
apply when passing on the port side, while the last three
(\zcref{eq:colregs-bigM-os-s,eq:colregs-bigM-ts-s,eq:colregs-bigM-ts-o-s})
enforce the starboard‐side conditions.


Including all ships’ constraints $\mathcal{O}_j$ guarantees that a passing side is chosen for each TS and that the required separation is maintained.
    
In the NLP relaxation of the MINLP (letting $z_j$ be continuous), there is now a way to continuously move the trajectory from one side of the obstacle to the other without violating any constraints. To be clear, all of the obstacles and points are active at the same time, but with a large enough chosen constant $M$, the inactive obstacles are so far away from the trajectory that they do not affect the optimization problem. This strategy is commonly referred to as the big-M method in Mixed Integer Programming (MIP) literature \citep{gan2012adaptive,Cococcioni2020}.

\begin{figure}
    \centering
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\scriptsize]{fig/b-spline/non-convex-obstacle-mi-sb.svg}
        \caption{Starboard maneuver ($z_j=1$)}
        \label{fig:non-convex-obstacle-mi-sb}
    \end{subfigure}
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includesvg[width=\textwidth,pretex=\scriptsize]{fig/b-spline/non-convex-obstacle-mi-port.svg}
        \caption{Port maneuver ($z_j=0$)}
        \label{fig:non-convex-obstacle-mi-port}
    \end{subfigure}
    \caption{Visualization of the Big–$M$ collision-avoidance constraints in \cref{eq:colregs-bigM}: the top panel shows a starboard maneuver and the bottom panel a port maneuver. The red/orange polygon represents the target ship (TS) and the blue polygon the own ship (OS). Grey areas indicate infeasible regions (not to scale). The solid line denotes the reference trajectory, while the dashed line depicts the planned trajectory under the current optimization variables.}
    \label{fig:non-convex-obstacle-mi}
\end{figure}



\section{Objective Function}
The goals of the objective function should be 1) to find a safe, COLREGS-compliant trajectory, and 2) to follow a reference trajectory. These objectives are conflicting as rule 8b) of the COLREGS (see \cref{sec:colregs}) states that any alteration of course should be large enough to be readily apparent to another vessel. Following a reference trajectory on the other hand often involves the squared error between the vessel and the reference trajectory, which inherently penalizes sudden deviations in course. 
Thus a careful choice of the objective function is important to ensure robust and predictable behaviour. 
The following section will examine the behavior of the B-spline method when applied to a basic reference-following task. The subsequent section will then address COLREGS considerations.


\subsection{Reference Following Metric}
\label{sec:reference-following-metric}

\begin{figure}
    \centering
    \includesvg[width=\textwidth]{fig/illustrations/cross-track- and along-track- error.svg}
\caption{Cross-track error (XTE), total track error (TTE) and psuedo cross-track error (PXTE) between an OS $\mathbf p_\text{OS}(t)$ and a reference trajectory $\mathbf p_\text{ref}(t)$. Each identically colored arrow represents the position of $\mathbf p_\text{OS}(t_i)$ and $\mathbf p_\text{ref}(t_i)$ at a given time $t_i$.}
    \label{fig:cross-track-along-track-error}
\end{figure}

Literature distinguishes between path following (geometric relations) and trajectory following (geometric and temporal relations). Path following often uses \acrfull{XTE} metric, the closest distance between the OS and the reference path \citep{Fossen2011-Handbook}. However, XTE is in general non-convex and requires numerical methods for computation in a B-spline framework \citep{johnson2005distance,hu2005second,chen2009computing}.

\acrfull{TTE} is the distance between a time-parameterized OS and a reference trajectory:
\begin{equation}\label{eq:total-track-error}
    \mathbf e(t) = \mathbf p_\text{OS}(t) - \mathbf p_\text{ref}(t).
\end{equation}
TTE can cause the OS to converge towards a point ahead of the geometrically closest point on the reference trajectory when the OS lags the reference trajectory.


A reference‐following metric that balances geometric and temporal alignment is the \acrfull{PXTE}, defined by
\begin{equation}\label{eq:pxte}
  \bar e(t)
  = \mathbf e(t)^\top \mathbf{\hat n}_{\mathrm{ref}}(t),
\end{equation}
where $\mathbf{\hat n}_{\mathrm{ref}}(t)$ is the unit normal to the reference trajectory defined as 
\begin{equation}\label{eq:reference-normal-2}
    \mathbf{\hat n}_{\mathrm{ref}}(t) = \mathbf R\frac{\mathbf p_\text{ref}'(t)}{\|\mathbf p_\text{ref}'(t)\|_2},
\end{equation}
where $\mathbf R$ is a 90-degree rotation. 

In time‐grid formulations this error is often penalized as a sum of squared values at discrete instants. In the B‐spline framework, one may instead formulate a continuous cost over the spline parameter, for example
\begin{equation}\label{eq:total-track-error-b-spline}
J_\text{ref} = \int_0^1 \bar e(x)^2 \, dx,
\end{equation}
This integral does not yield a closed form function as the unit normal is a rational expression. Instead, it can be approximated by dividing each coefficient of $\mathbf p'_\text{ref}(x)$ by the average speed over each basis function's support, resulting in an approximate unit normal vector $\bf\tilde n$. This approximation simplifies the integral in \cref{eq:total-track-error-b-spline} to a B-spline function, suitable for use as the error to minimize in a cost in the optimization problem.






% filepath: c:\Users\eirik\NTNU lokal\Rapport-Master\chapter4.tex
\subsection{Reference Following Cost Function}\label{sec:oscillations}
To ensure smoothness and suppress high-frequency oscillations in the B-spline trajectory, a novel method of constructing the cost function is proposed.
When the PXTE is penalized with the function in \cref{eq:total-track-error-b-spline}, 
the Hessian of this cost in its B‐spline coefficients is the Gramian matrix $G_e$, whose eigenvalues decrease with an increasing spatial frequency of the corresponding modes.  Consequently, high‐frequency oscillations are under‐penalized and tend to dominate the solution. Using the inverse of this Hessian as weighting on the coefficients of $\bar e$, the high frequency modes are penalized instead. This is explained in great detail in \cref{sec:norms,sec:b-spline-norms-as-cost-functions}. Still, a brief overview is given here:

The cost \cref{eq:total-track-error-b-spline} can be written in the bilinear form:
\begin{equation}\label{eq:cost-reference-following-bilinear}
  J_{\rm ref} = c_e^\top\,\left(\int_0^1 \mathbf B_e(x) \mathbf B_e(x)^\top \, dx\right)\,c_e =
  \mathbf c_e^\top\,\mathbf G_e\, \mathbf c_e,
\end{equation}
where \(G_e\) is the Gramian of the basis $\mathbf B_e$ for \(\bar e(x)\). Its eigenvalues decay with an increasing spatial frequency of the corresponding modes. Consequently, high‐frequency oscillations are under‐penalized and tend to dominate the solution.

To allow spatially varying emphasis of the PXTE cost along the spline parameter $x\in[0,1]$, a nonnegative weight function $w_{\rm ref}(x)$ (e.g.\ piecewise constant or any B-spline) is introduced. This weight is multiplied by $\bar e$ resulting in an expression in a new basis $\mathbf B_{we}$, with new coefficients $\mathbf c_{we}$. The Gramian is then computed for $\mathbf B_{we}$ and the final cost is written as
\begin{equation}\label{eq:cost-reference-following-gramian}
    J_\text{ref} = \|w_\text{ref}(x)\;\mathbf e(x)\|_{\mathbf{\hat G}_{we}^{-1}}^2 
    = \mathbf{c}_{we}^\top \mathbf{\hat G}_{we}^{-1} \mathbf{c}_{we}
\end{equation}
where
\begin{equation}\label{eq:gramian-density}
    \mathbf{\hat G}_{we} = \frac{\mathbf G_{we}}{\tr(\mathbf G_{we})}, 
\end{equation}
is the Gramian matrix of the new basis, normalized by its trace for easier weighting of other cost terms. 
Because $\mathbf{\hat G}_{we}^{-1}$ has the reciprocal eigenvalues of $\mathbf{\hat G}_{we}$, its largest weights now lie on the high‐frequency eigenvectors, thereby suppressing oscillatory modes. \Cref{fig:inverse-gramian-eigenvectors} nine eigenvectors of $\mathbf{\hat G}_e$ (quadratic, uniform basis) and show how applying the inverse‐Gramian cost flips the spectrum, effectively damping high‐frequency components.


As far as the author is aware, this is the first time the inverse Gramian matrix is used as a cost function in the context of B-spline optimization.
Other cost weighting schemes have also been considered for this work, using more classical methods sucha as penalizing the integral of the squared acceleration, or the squared velocity, but with unsatisfactory results. These and other methods are briefly discussed in \cref{app:failed-reference-following}.

\begin{figure}
    \centering
    \includesvg[width=\textwidth]{fig/conservativeness/eigenvectors_refined_inv_gramian_degree_2_N_9.svg}
    \caption{Eigenvectors of the inverse Gramian matrix $\mathbf{\hat G}_e^{-1}$ for the PXTE cost function in \cref{eq:cost-reference-following-gramian}. The eigenvectors are ordered by their eigenvalues, which decay with increasing spatial frequency. The inverse Gramian cost function flips the spectrum, effectively damping high‐frequency components.}    
    \label{fig:inverse-gramian-eigenvectors}
\end{figure}


\FloatBarrier
\subsection{COLREGS Objectives}\label{sec:colregs-objectives}
Rule 8b) of the COLREGS states that any alteration of course should be large enough to be readily apparent to another vessel. 
This can be interpreted as a cost function that penalizes small alterations of course, but not large ones. 
Implementing this directly in an optimization context, by penalizing low turn rates without penalizing straight line motion, results in a cost function that is not convex. 
See \cref{fig:turn-rate-cost} for an illustration of the problem.

\begin{figure}
    \centering
    \includesvg[width=0.6\textwidth]{fig/illustrations/Turn rate cost.svg}
    \caption{Imagined Turn rate cost for a vessel following Rule 8b) of the COLREGS.}
    \label{fig:turn-rate-cost}
\end{figure}

\cite{Thyri2022-MPC} propose a solution to improve the compliance of rule 8 by introducing a time window where the cost for altering course is lowered. The goal of this is to control when the OS should alter course, and with manipulation of the length and position of the time window, the cost function can facilitate a more COLREGS compliant trajectory. This idea is adapted to the B-spline MINMPC framework as follows:
An encounter window $W_\text e$ is defined, during which the OS should ensure safe passage. Within $W_\text e$, the cost for reference trajectory following is reduced, while the cost for altering course remains high, except at the start and end of $W_\text e$, denoted as maneuver windows $W_\text{mv}$. 
The reason for the second maneuver window as opposed to the singular one in \cite{Thyri2022-MPC} is to facilitate a clear maneuver back to the reference trajectory after the encounter window.

\begin{figure}
    \centering
    \includesvg[width=0.8\textwidth]{fig/illustrations/Maneuver window.svg}
    \caption{Weights for turn rate and TTE plotted against time.}
    \label{fig:maneuver-window}
\end{figure}

The encounter and maneuver windows are incorporated into the cost function as weights $w_\text{ref}(x)$ and $w_\text{mv}(x)$, which are applied to the reference trajectory following cost and the cost for altering course, respectively. The resulting cost function is:
\begin{equation}\label{eq:cost-maneuver-window-total}
    J = J_\text{ref} + J_\text{mv} + J_\mathrm{acc} + J_\mathrm{end},
\end{equation}
where the individual costs are defined as follows:
\begin{subequations}\label{eq:cost-maneuver-window}
    \begin{align}
        J_\text{ref} &= \|w_\text{ref}(x)\;\mathbf e(x)\|_{\mathbf{\hat G}^{-1}}^2 , 
        \label{eq:cost-maneuver-window-ref}\\
        J_\text{mv} &= \|w_\text{mv}(x)\;\mathbf e'(x)\|_{\mathbf{\hat G}^{-1}}^2 ,
        \label{eq:cost-maneuver-window-mv}\\
        J_\mathrm{acc} &= \|w_\text{acc}(x)\;\mathbf p''_\text{os}(x)\|_{\mathbf{\hat G}^{-1}}^2 , \label{eq:cost-maneuver-window-acc}\\
        J_\mathrm{end} &= \|w_\text{end}(x)\;(\mathbf p_\text{OS}(x)-\mathbf p_\text{ref}(1))\|_{\mathbf{\hat G}^{-1}}^2 , \label{eq:cost-maneuver-window-end}\\
    \end{align}
\end{subequations}
where $J_\text{mv}$ is the cost for deviationg from the reference course. $w_\text e(x)$ is the weight for the reference trajectory following cost, and $w_\text{mv}(x)$ is the weight for the maneuver window cost as shown in \cref{fig:maneuver-window}. An additional cost $J_\mathrm{acc}$ is included to penalize course and speed alterations where desired. To provide a driving force in the direction of the reference trajectory, a cost $J_\mathrm{end}$ is included as the PXTE alone does not penalize moving in the opposing direction of the reference course. 

The advantage of the B-spline framework in this case is that the weights can be defined as B-spline functions themselves. This allows for great flexibility in shaping the desired encounter trajectory. The function shape shown in \cref{fig:maneuver-window}, can be represented using a zero-degree B-spline. If smoother transitions are desired, higher degree B-splines can be used.
If the weights are fixed as a function of time before the optimization problem is solved, then the costs are constant with respect to the optimization variables and the optimization problem is convex as opposed to that of \cref{fig:turn-rate-cost}.

Estimating when the encounter window should start and end is a non-trivial problem. As is determining how long the maneuver windows should be. \cite{Thyri2022-MPC} proposes to use the time of closest point of approach (TCPA) to the TS along the desired trajectory. The desired trajectory can for example be estimated using Line-of-Sight (LOS) guidance \citep{Fossen2011-Handbook} or by considering the optimzation problem without the TS. Estimation of the encounter window time is implemented according to \cite{Thyri2022-MPC}, and won't be discussed further here.


\section{Implementation}\label{sec:implementation}


The B-spline MINMPC is realized as a Python/CasADi library with two layers:

\begin{itemize}
  \item \textbf{B-spline core:}  
    Implements symbolic spline operations—differentiation, integration, basis transformations, addition, multiplication, inner products (Gramians), and NURBS conversion—via CasADi SX/MX variables and sparse transform matrices.
  \item \textbf{CasADi wrapper:}  
    Extends CasADi’s Opti interface with methods to declare B-spline variables, polynomial and Big-M collision constraints, binary passage variables, and structured cost terms.
\end{itemize}

In the subsections that follow the following topics are discussed in detail:
\begin{enumerate}
  \item \emph{Operations on B-splines}: differentiation, integration, basis transformations and more (\cref{sec:operations-on-b-splines} to \cref{sec:inner-product}).
  \item \emph{Cost implementation}: Gramian‐based norms, inverse‐Gramian weighting, maneuver-window costs (\cref{sec:norms,sec:b-spline-norms-as-cost-functions}).
  \item \emph{Python API}: OptiObject/OptiCollection usage for problem setup and solving B-spline based optimization problems (\cref{sec:python-implementation}).
\end{enumerate}

\subsection{Operations on B-splines}\label{sec:operations-on-b-splines}

The B-spline library implements various unary and binary operations, as outlined in this section. First, the differentiation and integration of B-splines are presented. Then, the transformation between different B-spline bases is explained, along with its use in B-spline addition, multiplication, and inner product calculations. Finally, the implementation of cost functions and constraints within the optimization problem is discussed.


From the recursive relation in \cref{eq:b-spline-recurrence} it follows that the B-spline basis functions are piecewise polynomial functions in $x$ of degree $p$. This property makes it clear that all polynomial operations on B-splines will result in a new B-spline. This property is exploited in the following sections to derive the B-spline derivative and integral, as well as to transform between different B-spline bases.

In all following sections, it is assumed that for a given B-spline basis of degree $p$, the corresponding knot vector $\mathbf t$ is $p+1$ regular, which means that all knots have multiplicity of at most $p+1$, as well as the first and last knot having a multiplicity of exactly $p+1$. This ensures that the B-spline basis is well-defined and constrains the endpoints of the B-spline to the first and last coefficients of the B-spline.


It is convenient to work with the vector-valued B-spline in the form:
\begin{equation}\label{eq:b-spline-vector-valued}
    \mathbf f(x) = \begin{bmatrix}
        \mathbf{B}_{p, \mathbf{t}}^\top(x) \mathbf c_1 \\
        \mathbf{B}_{p, \mathbf{t}}^\top(x) \mathbf c_2 \\
        \vdots \\
        \mathbf{B}_{p, \mathbf{t}}^\top(x) \mathbf c_d
    \end{bmatrix}
    = 
    (\mathbf{I}_d \otimes \mathbf{B}_{p, \mathbf{t}}^\top(x)) 
    \begin{bmatrix}
        \mathbf c_1 \\
        \mathbf c_2 \\
        \vdots \\
        \mathbf c_d
    \end{bmatrix} 
    = (\mathbf{I}_d \otimes \mathbf{B}_{p, \mathbf{t}}^\top(x)) \mathbf{\vec c}
\end{equation}
where $\mathbf{I}_d$ is the identity matrix of size $d\times d$, $\otimes$ denotes the Kronecker product, and $\mathbf{\vec c} = [\mathbf c_1^\top, \mathbf c_2^\top, \ldots, \mathbf c_d^\top]^\top\in\mathbb R^{nd}$ is the stacked coefficient vector, each $\mathbf c_i \in \mathbb R^n$. Another practiacl form is with the coefficients in matrix form $\mathbf C = [\mathbf c_1, \ldots \mathbf c_d]\in \mathbb R^{n \times d}$.  The vector-valued B-spline can then be compactly expressed as:
\begin{equation}\label{eq:b-spline-vector-valued-matrix}
    \mathbf f(x) = \mathbf C^T \mathbf B_{p, \mathbf{t}}(x) 
\end{equation}


\subsection{Differentiation}\label{sec:derivative}
Given a B-spline $f(x) = \sum_{j=0}^{n-1} c_j B_{j, p, \mathbf{t}}(x)$,
the derivative of the B-spline can be computed by simply differencing its coefficients. 
\begin{equation}\label{eq:b-spline-derivative}
    \frac{\mathrm d}{\mathrm dx} f(x) = (p-1) \sum_{j=1}^{n-1} \frac{c_j-c_{j-1}}{t_{j+p-1}-t_j} B_{j, p-1, \boldsymbol{\tau}}(x)
\end{equation}
where $\boldsymbol{\tau} = [t_j]_{j=1}^{n+p-1}$ is the same knot vector as $\mathbf{t}$, but with the first and last knot removed in order to maintain regularity of the knot vector for the derivative B-spline basis.

The new coefficients can be expressed using a transformation matrix $\mathbf T_D$ such that $\mathbf{c}_D = \mathbf T_D \mathbf{c}$, where $\mathbf{c}_D$ is the vector of coefficients of the derivative B-spline. The transformation matrix is then an $(n-1) \times n$ matrix with elements:

\begin{equation}
    \mathbf T_D = (p-1) \begin{bmatrix}
        -q_{1,p-1,\mathbf{t}}^{-1} & q_{1,p-1,\mathbf{t}}^{-1} & 0 & \cdots & 0 \\
        0 & -q_{2,p-1,\mathbf{t}}^{-1} & q_{2,p-1,\mathbf{t}}^{-1} & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & \cdots & 0 & -q_{n-1,p-1,\mathbf{t}}^{-1} & q_{n-1,p-1,\mathbf{t}}^{-1} 
    \end{bmatrix},
\end{equation}
where $q_{j,p,\mathbf{t}} = (t_{j+p}-t_j)$.


The denominator $t_{j+p}-t_j$ in \cref{eq:b-spline-derivative} illustrates the continuity of the B-spline with respect to knot multiplicity. If a knot is repeated more than $p$ times, the denominator will be zero, and the derivative is not defined at that knot.

\subsection{Integration}
Given a degree $p$ and initial knot sequence $\mathbf t$, \cref{eq:b-spline-derivative}, gives
\begin{equation}\label{eq:b-spline-integral-pre}
    p \sum_{j=1}^{n-1} \gamma_j B_{j, p, \boldsymbol{t}}(x) 
    = \frac{\rm d}{\mathrm dx} \sum_{j=0}^{n-1} c_j B_{j, p+1, \boldsymbol{\tau}}(x),
\end{equation}
given that
\begin{equation}
    \gamma_j = \frac{c_j-c_{j-1}}{t_{j+p}-t_j},
\end{equation}
holds. Here, $\boldsymbol{\tau}$ is now the knot vector $\mathbf{t}$ with 1 additional knot at the beginning and end. Now
the new coefficients on the right hand side of \cref{eq:b-spline-integral-pre} can be expressed as 
\begin{equation}\label{eq:b-spline-integral-coefficients}
    c_j = c_{j-1} + \gamma_j \frac{t_{j+p}-t_j}{p}.
\end{equation}
This then gives the general anti-derivative of a B-spline as
\begin{equation}\label{eq:b-spline-integral}
    \int f(x) \mathrm dx = \sum_{j=0}^{n-1} c_j B_{j, p+1, \boldsymbol{\tau}}(x)
\end{equation}
where $[c_j]_{j=1}^{n-1}$ are given by \cref{eq:b-spline-integral-coefficients}, $c_0$ is the constant of integration and $[\gamma_j]_1^{n-1}$ are the original coefficients of $f(x)$. In matrix form, the coefficients of the integral B-spline can be expressed as
\begin{equation}\label{eq:b-spline-integral-matrix}
    \mathbf{c}_I = \mathbf T_I \mathbf{c} + \mathbf{c}_0,
\end{equation}
where $\mathbf T_I$ is an $n \times (n-1)$ matrix with elements
\begin{equation}
    \mathbf T_I = \frac{1}{p}\begin{bmatrix}
        0 & 0 & \cdots & 0 \\
        q_{1,p,\mathbf{t}} & 0 & \cdots & 0 \\
        q_{1,p,\mathbf{t}} & q_{2,p,\mathbf{t}} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        q_{1,p,\mathbf{t}} & q_{2,p,\mathbf{t}} & \cdots & q_{n-1,p,\mathbf{t}}
    \end{bmatrix},\quad
    \mathbf{c}_0 = \begin{bmatrix}
        c_0 \\
        c_0 \\
        \vdots \\
        c_0
    \end{bmatrix}
\end{equation}

and $q_{j,p,\mathbf{t}} = (t_{j+p}-t_j)$ still.


\subsection{Transforming Between Bases}\label{sec:basis-transformation}
In order to perform binary operations on spline functions such as multiplication and addition, it is necessary to be able to represent the same spline function in different bases. A powerful result from theory is that any spline function $\mathbf S$ expressed in a given basis $\mathbf B_{p,\mathbf t}$ can be represented in any other basis $\mathbf B_{q,\boldsymbol \tau}$ as long as $q \geq p$ and $\mathbf t \subseteq \boldsymbol \tau$ \citep{Grimstad2016}. Moreover, the coefficients in the new basis can be found by solving a linear system of equations. The following basis transformation method is employed in many algorithms in literature, and forms the foundation for the rest of the implementations in this chapter, which is why a thorough explanation of this method is given attention here.

The goal is to find the transformation matrix $\mathbf T$ such that $\mathbf c_{q, \boldsymbol \tau} = \mathbf T \mathbf c_{p, \mathbf t}$, where $\mathbf c_{q, \boldsymbol \tau}$ and $\mathbf c_{p, \mathbf t}$ are the coefficients of the spline function $f(x)$ in the bases $\mathbf B_{q,\boldsymbol \tau}$ and $\mathbf B_{p,\mathbf t}$, respectively.

%The strategy to find $\mathbf T$ involves expressing the coefficients of the original basis $\mathbf c_{p, \mathbf t}$ as a linear combination of the refined basis $\mathbf c_{q,\boldsymbol \tau}$.
Noting that the two spline functions $\mathbf B_{q,\boldsymbol \tau}^\top \mathbf c_{q, \boldsymbol \tau}$, $\mathbf B_{p,\mathbf t}^\top \mathbf c_{p, \mathbf t}$ have to be equal,
the transformation matrix $\mathbf T$ can be found by sampling the B-spline basis functions at strategic points in the interval $[\tau_0, \tau_{n+q})$ to obtain a system of $m$ linear equations with $m$ unknowns.

The Greville abscissae $\boldsymbol \xi_{q,\boldsymbol \tau}=[\xi_{q,\boldsymbol \tau}]_{i=0}^{n-1}$ are a natural choice for the sampling points, as they represent the points at which the $i$-th basis function is most influential. 
They are given by the mean of the knots in each basis functions support $[\tau_i, \tau_{i+q+1})$ as
\begin{equation}\label{eq:greville-abscissae}
    \xi_{q,\boldsymbol \tau}(i) = \frac{1}{q+1} \sum_{j=i}^{i+q} \tau_j.
\end{equation}
The transformation matrix $\mathbf T$ can then be found by evaluating the B-spline basis functions at the Greville abscissae and solving the linear system of equations
\begin{equation}\label{eq:transformation-matrix-equation}
    \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(i))^\top \mathbf c_{q, \boldsymbol \tau} = \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(i))^\top \mathbf c_{p, \mathbf t} \quad \forall i\in\{0,1,\ldots,m-1\}
\end{equation}
for $\mathbf c_{q, \boldsymbol \tau}$. This can be written in matrix form as
\begin{equation}\label{eq:transformation-matrix}
    \underbrace{\begin{bmatrix}
        \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(0))^\top \\
        \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(1))^\top \\
        \vdots                                                             \\
        \mathbf B_{q,\boldsymbol \tau}(\xi_{q,\boldsymbol \tau}(m-1))^\top
    \end{bmatrix}}_{\mathbf A:\;m \times m}
    \underbrace{\begin{bmatrix}
        c_{q, \boldsymbol \tau, 0}   \\
        c_{q, \boldsymbol \tau, 1}   \\
        \vdots                       \\
        c_{q, \boldsymbol \tau, m-1} \\
    \end{bmatrix}}_{\mathbf c_{q,\boldsymbol\tau}:\;m \times 1}
    = 
    \underbrace{\begin{bmatrix}
        \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(0))^\top  \\
        \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(1))^\top  \\
        \vdots                                                       \\
        \mathbf B_{p,\mathbf t}(\xi_{q,\boldsymbol \tau}(m-1))^\top
    \end{bmatrix}}_{\mathbf D:\;m \times n}
    \underbrace{\begin{bmatrix}
        c_{p, \mathbf t, 0}   \\
        c_{p, \mathbf t, 1}   \\
        \vdots                \\
        c_{p, \mathbf t, n-1}
    \end{bmatrix}}_{\mathbf c_{p,\mathbf t}:\;n \times 1}
\end{equation}
or more compactly as
\begin{equation}
    [\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top] \mathbf c_{q, \boldsymbol \tau} = [\mathbf B_{p,\mathbf t}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top] \mathbf c_{p, \mathbf t}.
\end{equation}

The matrix $\mathbf T$ is then simply
\begin{equation}\label{eq:transformation-matrix-solution}
    \mathbf T = [\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]^{-1} [\mathbf B_{p,\mathbf t}(\boldsymbol \xi_{q,\boldsymbol \tau})^{\top}].
\end{equation}
As the B-spline basis functions are linearly independent, and the Greville abscissae are chosen such that each basis function is sampled at a point near the center of its support, the matrix $[\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]$ is always invertible. This method follows from the Schoenberg-Whitney Theorem \citep{schoenberg1988polya} and a simple proof of this entire method can be found in \cite{schoenberg2022proof}. 

The matrix $\mathbf A$ in \cref{eq:transformation-matrix} is known as a \emph{Collocation matrix} and can be used to approximate an arbitrary function with a B-spline by replacing the right hand side of \cref{eq:transformation-matrix} with the function values sampled at the Greville abscissae. 
If the basis of the target spline is chosen such that it is the minimal basis (lowest degree and fewest knots) that can represent the b-spline expression on the right hand side, then the transformation matrix $\mathbf T$ always gives the \textit{exact} coefficients of the target spline in the new basis. This is a powerful result, as it allows for the transformation of B-spline coefficients between different bases without the need for numerical integration or interpolation.
And while this is seldom the most efficient way to compute this transformatiobn, it is one of the most general and robust methods.

\begin{algorithm}
    \caption{B-spline Basis Transformation}\label{alg:basis-transformation}
    \begin{algorithmic}[1]
        \State \textbf{Input:} B-spline basis $\mathbf{B}_{p,\mathbf{t}}$
        \State \textbf{Input:} Refined B-spline basis $\mathbf B_{q,\boldsymbol \tau}$
        \Ensure $q \geq p$
        \Ensure $\mathbf t \subseteq \boldsymbol \tau$
        \State $\boldsymbol \xi_{q, \boldsymbol \tau} \gets$ result from \cref{eq:greville-abscissae}
        \State $\mathbf A \gets [\mathbf B_{q,\boldsymbol \tau}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]$
        \State $\mathbf B \gets [\mathbf B_{p,\mathbf t}(\boldsymbol \xi_{q,\boldsymbol \tau})^\top]$
        \State $\mathbf T \gets \mathbf A^{-1} \mathbf B$
        \State \textbf{Output:} $\mathbf T$
    \end{algorithmic}
\end{algorithm}

\subsection{Knot Refinement and Degree Elevation}\label{sec:knot-refinement-degree-elevation}
The method in \cref{sec:basis-transformation} can be utilized for knot refinement (\cref{alg:knot-refinement}) and degree elevation (\cref{alg:degree-elevation}). Knot refinement adds new knots to the knot vector, keeping the degree constant, while degree elevation increases the degree. To maintain continuity at the knots during degree elevation, the knot multiplicity must be increased. In both cases, the transformation matrix $\mathbf T$ from \cref{eq:transformation-matrix-solution}, computed using \cref{alg:basis-transformation}, finds the coefficients in the new basis.

\begin{algorithm}
    \caption{Degree Elevation}\label{alg:degree-elevation}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Knot vector $\mathbf t = \{\eta_0^{(m_0)}, \eta_1^{(m_1)}, \dots, \eta_r^{(m_r)}\}$, degree $p$
        \State \textbf{Input:} Desired degree $q$
        \Ensure $q > p$
        \Ensure $m_0 = m_r = p+1$
        \Ensure $m_i \leq p+1$ for $i \in \{0,1,\ldots,r\}$
        \State $\boldsymbol\tau \gets \{\eta_0^{(m_0+p-q)}, \eta_1^{(m_1+p-q)}, \dots, \eta_r^{(m_r+p-q)}\}$
        \State $\mathbf T \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p,\mathbf t}$ to $\mathbf B_{q,\boldsymbol \tau}$
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol \tau}, \mathbf T$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Knot Refinement}\label{alg:knot-refinement}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Knot vector $\mathbf t = \{\eta_0^{(m_0)}, \eta_1^{(m_1)}, \dots, \eta_r^{(m_r)}\}$, degree $p$
        \State \textbf{Input:} New knots to insert $\boldsymbol \xi = \{\xi_1, \xi_2, \dots, \xi_k\}$
        \Ensure $\xi_i \in (\eta_0, \eta_r)$
        \State $\boldsymbol \tau = \{\hat\eta_0^{(\hat m_0)}, \hat\eta_1^{(\hat m_1)}, \dots, \hat\eta_{\hat r}^{(\hat m_{\hat r})}\} \gets \mathbf t \cup_< \boldsymbol \xi$ \Comment{$\cup_<$ denotes the sorted union}
        \Ensure $\hat m_i \leq p+1$ for $i \in \{0,1,\ldots,\hat r\}$
        \State $\mathbf T \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p,\mathbf t}$ to $\mathbf B_{p,\boldsymbol \tau}$
        \State \textbf{Output:} $\mathbf B_{p, \boldsymbol \tau}, \mathbf T$
    \end{algorithmic}
\end{algorithm}


% \begin{indentedexample}[Degree Elevation]
%     We want to express the constant spline $f(x) =\mathbf B_{0, \mathbf t}^\top\mathbf c$ with knot vector $\mathbf t = \left[0, 1\right]$ as a linear spline $g(x) = \mathbf B_{1, \boldsymbol \tau}^\top \mathbf d$ with knot vector $\boldsymbol \tau = \left[0, 0, 1, 1\right]$. 

%     The Greville abscissae for the linear spline are 
%     \begin{equation}
%         \boldsymbol \xi_{1,\boldsymbol \tau} = 
%         \begin{bmatrix}
%             \frac{\tau_0 + \tau_1 + \tau_2}{3} &
%             \frac{\tau_1 + \tau_2 + \tau_3}{3} 
%         \end{bmatrix}
%          = \begin{bmatrix}
%             \frac{1}{3} & \frac{2}{3}
%         \end{bmatrix}.
%     \end{equation}
%     Evaluating $\mathbf B_{0, \mathbf t}$ and $\mathbf B_{1, \boldsymbol \tau}$  at the Greville abscissae gives
%     \begin{equation}
%         \begin{aligned}
%             \mathbf B_{0, \mathbf t}\begin{pmatrix}\frac{1}{3}\end{pmatrix} &= \begin{bmatrix} 1 \end{bmatrix}, 
%             &\mathbf B_{0, \mathbf t}\begin{pmatrix}\frac{2}{3}\end{pmatrix} &= \begin{bmatrix} 1 \end{bmatrix} 
%             \\
%             \mathbf B_{1, \boldsymbol \tau}\begin{pmatrix}\frac{1}{3}\end{pmatrix} &= \begin{bmatrix} \frac{2}{3} & \frac{1}{3} \end{bmatrix}^\top, 
%             &\mathbf B_{1, \boldsymbol \tau}\begin{pmatrix}\frac{2}{3}\end{pmatrix} &= \begin{bmatrix} \frac{1}{3} & \frac{2}{3} \end{bmatrix}^\top
%         \end{aligned}
%     \end{equation}
%     Solving
%     \begin{equation}
%         \begin{bmatrix}
%             \frac{2}{3} & \frac{1}{3} \\
%             \frac{1}{3} & \frac{2}{3}
%         \end{bmatrix}
%         \begin{bmatrix} d_0 \\ d_1 \end{bmatrix}
%         =
%         \begin{bmatrix} 1 \\ 1 \end{bmatrix}
%         \begin{bmatrix} c_0 \end{bmatrix}
%     \end{equation}
%     gives the solution
%     \begin{equation}
%         \begin{bmatrix} d_0 \\ d_1 \end{bmatrix}
%         =
%         \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}
%         \begin{bmatrix} 1 \\ 1 \end{bmatrix}
%         \begin{bmatrix} c_0 \end{bmatrix}
%         =
%         \begin{bmatrix} 1 \\ 1 \end{bmatrix}
%         \begin{bmatrix} c_0 \end{bmatrix}.
%     \end{equation}
%     This makes intuitive sense, as representing a constant in a linear basis simply means that the two coefficients of the linear spline are equal to the constant value.
% \end{indentedexample}

% \begin{indentedexample}[Knot Refinement]
%     Given a linear spline function $f(x) = \mathbf B_{1, \mathbf t}^\top \mathbf c$ with knot vector $\mathbf t = \left\{ 0, 0, \frac{1}{2}, 1, 1\right\}$, the goal is to express it with in the basis $\mathbf B_{1, \boldsymbol \tau}$ with the refined knot vector $\boldsymbol \tau = \left\{ 0, 0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1, 1\right\}$.

%         The Greville abscissae for the refined basis are
%         \begin{equation}
%             \boldsymbol \xi_{1,\boldsymbol \tau} = 
%             \left\{
%                 \frac{1}{12}, \frac{3}{12}, \frac{6}{12}, \frac{9}{12}, \frac{11}{12}
%             \right\}.
%         \end{equation}
%         Using \cref{tab:linear-spline-evaluation} to evaluate $\mathbf B_{1,\boldsymbol\tau}(x)$ at the Greville abscissae gives
%         \begin{equation}
%             \begin{aligned}
%                 \mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{1}{12}\end{pmatrix} &= \begin{bmatrix} \frac{3}{4} & \frac{1}{4} & 0 & 0 & 0 \end{bmatrix}^\top,
%                 &\mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{3}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 1 & 0 & 0 & 0 \end{bmatrix}^\top, \\
%                 \mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{6}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 0 & 1 & 0 & 0 \end{bmatrix}^\top,
%                 &\mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{9}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 0 & 0 & 1 & 0 \end{bmatrix}^\top, \\
%                 \mathbf B_{1, \boldsymbol\tau}\begin{pmatrix}\frac{11}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 0 & 0 & \frac{1}{4} & \frac{3}{4} \end{bmatrix}^\top,
%             \end{aligned}
%         \end{equation}
%         while evaluating $\mathbf B_{1,\mathbf t}(x)$ at the Greville abscissae gives
%         \begin{equation}
%             \begin{aligned}
%                 \mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{1}{12}\end{pmatrix} &= \begin{bmatrix} \frac{5}{6} & \frac{1}{6} & 0 \end{bmatrix}^\top,
%                 &\mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{3}{12}\end{pmatrix} &= \begin{bmatrix} \frac{1}{2} & \frac{1}{2} & 0 \end{bmatrix}^\top, \\
%                 \mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{6}{12}\end{pmatrix} &= \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^\top,
%                 &\mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{9}{12}\end{pmatrix} &= \begin{bmatrix} 0 & \frac{1}{2} & \frac{1}{2} \end{bmatrix}^\top, \\
%                 \mathbf B_{1, \mathbf t}\begin{pmatrix}\frac{11}{12}\end{pmatrix} &= \begin{bmatrix} 0 & \frac{1}{6} & \frac{5}{6} \end{bmatrix}^\top.
%             \end{aligned}
%         \end{equation}
%         Inserting the calculated values into \cref{eq:transformation-matrix} gives
%         \begin{equation}
%             \begin{bmatrix}
%                 \frac{3}{4} & \frac{1}{4} & 0 & 0 & 0 \\
%                 0 & 1 & 0 & 0 & 0 \\
%                 0 & 0 & 1 & 0 & 0 \\
%                 0 & 0 & 0 & 1 & 0 \\
%                 0 & 0 & 0 & \frac{1}{4} & \frac{3}{4}
%             \end{bmatrix}
%             \mathbf{d}
%             =
%             \begin{bmatrix}
%                 \frac{5}{6} & \frac{1}{6} & 0 \\
%                 \frac{1}{2} & \frac{1}{2} & 0 \\
%                 0 & 1 & 0 \\
%                 0 & \frac{1}{2} & \frac{1}{2} \\
%                 0 & \frac{1}{6} & \frac{5}{6}
%             \end{bmatrix}
%             \mathbf c,
%         \end{equation}
%         which has the solution
%         \begin{equation}
%             \mathbf d = \begin{bmatrix}
%                 \frac{8}{9} & \frac{1}{9} & 0 \\
%                 \frac{1}{2} & \frac{1}{2} & 0 \\
%                 0 & 1 & 0 \\
%                 0 & \frac{1}{2} & \frac{1}{2} \\
%                 0 & \frac{1}{9} & \frac{8}{9}
%             \end{bmatrix}
%             \mathbf c.
%         \end{equation}

%             \centering
%             \begin{tabular}{c|c|c|c|c|c}
%                 Knot Interval & $\mathbf B_{1, \boldsymbol\tau, 0}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 1}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 2}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 3}(x)$ & $\mathbf B_{1, \boldsymbol\tau, 4}(x)$ \\[5pt]
%                 \hline
%                 \rule{0pt}{2.5ex} $\left[0,\tfrac{1}{4}\right]$ & $1-4x$ & $4x$ & 0 & 0 & 0 \\[5pt]
%                 \hline
%                 \rule{0pt}{2.5ex} $\left[\tfrac{1}{4},\tfrac{1}{2}\right]$ & 0 & $2-4x$ & $-1+4x$ & 0 & 0 \\[5pt]
%                 \hline
%                 \rule{0pt}{2.5ex} $\left[\tfrac{1}{2},\tfrac{3}{4}\right]$ & 0 & 0 & $3-4x$ & $-2+4x$ & 0 \\[5pt]
%                 \hline
%                 \rule{0pt}{2.5ex} $\left[\tfrac{3}{4},1\right]$ & 0 & 0 & 0 & $4-4x$ & $-3+4x$
%             \end{tabular}
%             \captionof{table}{The polynomials for each basis function in $\mathbf B_{1, \left[0, 0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1, 1\right]}$ in their respecive knot intervals}
%             \label{tab:linear-spline-evaluation}
% \end{indentedexample}

\subsection{Addition and Subtraction}
Finding the sum of two B-splines that share the same basis is a simple task as their coefficients can just be added together:
\begin{equation}
    f(x) + g(x) =  \mathbf{B}_{p, \mathbf{t}}^{\top}(x) \mathbf{c}_1 + \mathbf{B}_{p, \mathbf{t}}^{\top}(x) \mathbf{c}_2
    = \mathbf{B}_{p, \mathbf{t}}^{\top}(x)(\mathbf{c}_1 + \mathbf{c}_2)
\end{equation}
If the spline functions are in different bases, however, they must be transformed to a common basis before the addition can be performed. So, for spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$, with degrees $p_1$ and $p_2$ and knot vectors $\mathbf{t}_1$ and $\mathbf{t}_2$, the goal is to find a common basis $\mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x)$ with degree $q$ and knot vector $\boldsymbol{\tau}$ such that $h(x) = \mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x) \mathbf{d} = f(x) + g(x)$.

To do this, the basis of the lowest degree is first elevated to the highest degree, before the knot vectors are refined to a common knot vector. The transformation matrices $\mathbf T_1$ and $\mathbf T_2$ from the original bases to the common basis are then found using \cref{eq:transformation-matrix-solution}. The new coefficients $\mathbf d$ are then given by
\begin{equation}
    \mathbf d = \mathbf T_1 \mathbf c_1 + \mathbf T_2 \mathbf c_2.
\end{equation}

\begin{algorithm}
    \caption{Common Basis}\label{alg:common-basis}
    \begin{algorithmic}[1]
        \State \textbf{Input:} B-Spline basis functions $\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x)$ and $\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)$
        \State \textbf{Input:} Desired degree $q$
        \Ensure $q \geq \max(p_1, p_2)$
        \State $\mathbf B_{q, \boldsymbol{\tau_1}} \gets$ result from \cref{alg:degree-elevation} with basis $\mathbf{B}_{p_1, \mathbf{t}_1}$ and degree $q$
        \State $\mathbf B_{q, \boldsymbol{\tau_2}} \gets$ result from \cref{alg:degree-elevation} with basis $\mathbf{B}_{p_2, \mathbf{t}_2}$ and degree $q$
        \State $\boldsymbol{\tau} \gets$ union of $\boldsymbol{\tau_1}$ and $\boldsymbol{\tau_2}$ with multiplicity for each knot set to the maximum of the two knot vectors
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol{\tau}}^{\top}(x)$
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \caption{Addition}\label{alg:addition}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$
        \State $q \gets \max(p_1, p_2)$
        \State $\mathbf B_{q, \boldsymbol{\tau}} \gets $ result from \cref{alg:common-basis} with bases $\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x)$ and $\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)$ and desired degree $q$
        \State $\mathbf T_1 \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_1, \mathbf{t}_1}$ to $\mathbf B_{q, \boldsymbol{\tau}}$
        \State $\mathbf T_2 \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_2, \mathbf{t}_2}$ to $\mathbf B_{q, \boldsymbol{\tau}}$
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol{\tau}}(x), \quad\mathbf T_1 \mathbf c_1 + \mathbf T_2 \mathbf c_2$
    \end{algorithmic}
\end{algorithm}

Scalar addition in the form of $f(x) + a$ can be performed by simply adding the scalar to all the coefficients of the B-spline. This is clear from the fact thats a scalar can be represented as a B-spline with degree $0$ (constant spline), and then performing the addition as described above. 

\subsection{Multiplication}\label{sec:multiplication}

For addition, the common basis had to have degree $q \geq \max(p_1, p_2)$, but for multiplication, the degree of the common basis must be $q = p_1 + p_2$\todo[inlinepar]{forklar hvorfor}. Intuitively, this is because the product of two polynomials of degree $p_1$ and $p_2$ will have degree of at most $p_1 + p_2$ while the sum of two polynomials of degree $p_1$ and $p_2$ will have degree of at most $\max(p_1, p_2)$.

In order to find the product of two B-splines, an approach similar to \cref{sec:basis-transformation} can be used. Given two spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$, the goal is to find a common basis $\mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x)$ with degree $q$ and knot vector $\boldsymbol{\tau}$ such that $h(x) = \mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x) \mathbf{d} = f(x)  g(x)$.

Writing the equation for the product of the two B-splines gives
\begin{subequations}\label{eq:b-spline-product}
    \begin{align}
        h(x) &= f(x) g(x) \label{eq:b-spline-product-1} \\
        &= \left(\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1\right) 
        \left(\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2\right) \label{eq:b-spline-product-2} \\
        &= \left(\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \otimes \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)\right) 
        \left(\mathbf{c}_1 \otimes \mathbf{c}_2\right) \label{eq:b-spline-product-3} \\
        \mathbf{B}_{q, \boldsymbol{\tau}}^{\top}(x) \mathbf{d} 
        &= \left(\mathbf{B}_{p_1, \mathbf{t}_1}(x) \otimes \mathbf{B}_{p_2, \mathbf{t}_2}(x)\right)^{\top} \left(\mathbf{c}_1 \otimes \mathbf{c}_2\right), \label{eq:b-spline-product-4}
    \end{align}
\end{subequations}

where $\otimes$ denotes the Kronecker product. In \cref{eq:b-spline-product-3}, the mixed product property of the Kronecker product has been used, while \cref{eq:b-spline-product-4} is obtained using the fact that the transpose is distributive over the Kronecker product. \Cref{eq:b-spline-product-4} resembles the structure in \cref{eq:transformation-matrix-equation}, and the transformation matrix $\mathbf T$ required for computing the inner product can be derived analogously to the multiplication operation, as described in \cref{alg:multiplication}.

So, to find the product $h(x) = f(x) g(x)$, one first finds a common basis for $f(x)$ and $g(x)$ using \cref{alg:common-basis}, before finding the transformation matrix $\mathbf T$ from $\mathbf c_1 \otimes \mathbf c_2$ to $\mathbf d$ using \cref{eq:transformation-matrix-solution}. The product is then given by
\begin{equation}
    h(x) = \mathbf B_{q, \boldsymbol{\tau}}^\top(x) \mathbf d = \mathbf B_{q, \boldsymbol{\tau}}^\top(x) \mathbf T (\mathbf c_1 \otimes \mathbf c_2).
\end{equation}

\begin{algorithm}
    \caption{Product}\label{alg:multiplication}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Spline functions $f(x) = \mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x) \mathbf{c}_1$ and $g(x) = \mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x) \mathbf{c}_2$
        \State $q \gets p_1 + p_2$
        \State $\mathbf B_{q, \boldsymbol{\tau}} \gets $ result from \cref{alg:common-basis} with bases $\mathbf{B}_{p_1, \mathbf{t}_1}^{\top}(x)$ and $\mathbf{B}_{p_2, \mathbf{t}_2}^{\top}(x)$ and desired degree $q$
        \State $\mathbf T \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_1, \mathbf{t}_1}^{\top}(x) \otimes \mathbf B_{p_2, \mathbf{t}_2}^{\top}(x)$ to $\mathbf B_{q, \boldsymbol{\tau}}^{\top}(x)$
        \State \textbf{Output:} $\mathbf B_{q, \boldsymbol{\tau}}(x), \quad\mathbf T (\mathbf c_1 \otimes \mathbf c_2)$
    \end{algorithmic}
\end{algorithm}


% \begin{indentedexample}[B-Spline Product]
%     \label{ex:linear-spline-multiplication}
%     Consider the two B-splines
%     \begin{equation}
%         f(x) = \mathbf{B}_{p_1, \mathbf t_1}^\top(x) \mathbf{c}_1
%         \quad g(x) = \mathbf{B}_{p_2, \mathbf t_2}^\top(x) \mathbf{c}_2
%     \end{equation}
%     with knot vectors $\mathbf t_1 = \left\{0, 0, \frac{1}{3}, \frac{2}{3}, 1, 1\right\}$,  $\mathbf t_2 = \left\{0, 0, 0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1, 1, 1\right\}$ and degrees  $p_1 = 1$, $p_2 = 2$. 
%     The common basis $\mathbf{B}_{q,\boldsymbol{\tau}}$ is then found using \cref{alg:common-basis}, to get:
%     \begin{equation}
%         q = p_1 + p_2 = 3,
%         \quad
%         \boldsymbol{\tau} = \left\{0, 0, 0, 0, \tfrac{1}{4}, \tfrac{1}{4}, \tfrac{1}{3}, \tfrac{1}{3}, \tfrac{1}{3}, \tfrac{1}{2}, \tfrac{1}{2}, \tfrac{2}{3}, \tfrac{2}{3}, \tfrac{2}{3}, \tfrac{3}{4}, \tfrac{3}{4}, 1, 1, 1, 1\right\},
%     \end{equation}
%     where the internal knots of $\mathbf t_1$ and $\mathbf t_2$ have been duplicated twice and once respectively to account for the degree elevation. 
%     The left hand side of \cref{eq:b-spline-product-4} is found as in \cref{eq:transformation-matrix}, sampling $\mathbf B_{q, \boldsymbol{\tau}}(x)$ at the Greville abscissae $\boldsymbol{\zeta_\tau}$.
%     For the right hand side, we can write out the product of the B-splines in matrix form using the Kronecker product:
%     \begin{equation}
%         \begin{aligned}
%             (\mathbf B_{p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) \otimes \mathbf B_{p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}))^\top \mathbf{c}_1 \otimes \mathbf{c}_2 = \begin{bmatrix}
%                 B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{0, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
%                 B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{1, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
%                 B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{2, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
%                 \vdots \\
%                 B_{0, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{m, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
%                 B_{1, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{0, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
%                 \vdots \\
%                 B_{n, p_1, \mathbf{t}_1}(\boldsymbol{\zeta_\tau}) B_{m, p_2, \mathbf{t}_2}(\boldsymbol{\zeta_\tau}) \\
%             \end{bmatrix}^\top 
%             \begin{bmatrix}
%                 c_{0, 1} c_{0, 2} \\
%                 c_{0, 1} c_{1, 2} \\
%                 c_{0, 1} c_{2, 2} \\
%                 \vdots \\
%                 c_{0, 1} c_{m, 2} \\
%                 c_{1, 1} c_{0, 2} \\
%                 \vdots \\
%                 c_{n, 1} c_{m, 2}
%             \end{bmatrix}                
%         \end{aligned}
%     \end{equation}
%     This is a matrix with $nm$ rows and column number equal to the number of Greville abscissae in $\boldsymbol{\tau}$, which depend on the degrees of $\mathbf B_{p_1, \mathbf{t}_1}$ and $\mathbf B_{p_2, \mathbf{t}_2}$ and how many knots they share.
%     In order to save computational resources, the products with non-overlapping supports should be ignored. In this example the supports of the two B-splines are given in \cref{tab:ex-spline-supports}. Here we only get 6 zero-entries, but as the knot vectors get denser, this number will increase.


%     \centering
%     \small
%     \begin{tabular}{c|c||c|c|c|c|c|c}
%         \textbf{Basis} &  & $B_{0, p_2, \mathbf{t}_2}$ & $B_{1, p_2, \mathbf{t}_2}$ & $B_{2, p_2, \mathbf{t}_2}$ & $B_{3, p_2, \mathbf{t}_2}$ & $B_{4, p_2, \mathbf{t}_2}$ & $B_{5, p_2, \mathbf{t}_2}$ \\[5pt]
%         \hline
%         \rule{0pt}{2.5ex} & \textbf{Support} & $\left[0, \frac{1}{4}\right]$ & $\left[0, \frac{1}{2}\right]$ & $\left[0, \frac{3}{4}\right]$ & $\left[\frac{1}{4}, \frac{3}{4}\right]$ & $\left[\frac{1}{2}, 1\right]$ & $\left[\frac{3}{4}, 1\right]$ \\[5pt]
%         \hline\hline
%         \rule{0pt}{2.5ex}$B_{0, p_1, \mathbf{t}_1}$ & $\left[0, \frac{1}{3}\right]$ & 
%         1 & 1 & 1 & 1 & 0 & 0 \\[5pt]\hline\rule{0pt}{2.5ex}
%         $B_{0, p_1, \mathbf{t}_1}$ & $\left[0, \frac{2}{3}\right]$ &
%         1 & 1 & 1 & 1 & 1 & 0 \\[5pt]\hline\rule{0pt}{2.5ex}
%         $B_{2, p_1, \mathbf{t}_1}$ & $\left[\frac{1}{3}, 1\right]$ &
%         0 & 1 & 1 & 1 & 1 & 1 \\[5pt]\hline\rule{0pt}{2.5ex}
%         $B_{2, p_1, \mathbf{t}_1}$ & $\left[\frac{2}{3}, 1\right]$ & 
%         0 & 0 & 1 & 1 & 1 & 1 \\[5pt]
%     \end{tabular}
%     \captionof{table}{Overlapping supports of $B_{p_1, \mathbf{t}_1}$ and $B_{p_2, \mathbf{t}_2}$ in \cref{ex:linear-spline-multiplication}.}
%     \label{tab:ex-spline-supports}
% \end{indentedexample}

\subsection{Inner Product, Norms and the B-spline Gramian}\label{sec:inner-product}
\subsubsection{Inner Product}
The inner product of two (vector-valued) functions is a scalar that measures the similarity between the functions. combines the components of the two vector-valued functions. This operation is useful in applications such as calculating projections, or defining cost functions in optimization problems. The $\mathcal L_2$ inner product (henceforth referred to as the inner product) for vector-valued functions is defined as:
\begin{equation}
    \label{eq:dot-product}
    \langle\mathbf f(x), \mathbf g(x)\rangle = \int_{-\infty}^\infty \mathbf f^\top(x) \mathbf g(x)
\end{equation}
where $\mathbf f, \mathbf g: \mathbb{R} \to \mathbb{R}^d$, $\mathbf f(x) = [f_1(x), \ldots, f_d(x)]^\top$ and $\mathbf g(x) = [g_1(x), \ldots, g_d(x)]^\top$ are vector-valued functions with $d$ components. The expression inside the integral also carries useful information, and one may refer to it as the pointwise (PW) inner product, defined as
\begin{equation}
    \label{eq:dot-product-pointwise}
    \mathbf (f\cdot \mathbf g)(x) = \mathbf f^\top(x) \mathbf g(x).
\end{equation}

When $\mathbf f$ and $\mathbf g$ are spline functions, the PW inner product can be expressed in terms of their B-spline representations as
\begin{equation}
    \label{eq:dot-product-spline}
    \begin{aligned}
        \langle\mathbf f(x), \mathbf g(x)\rangle &= \langle\mathbf B_{p_1, \mathbf t_1}^\top(x) \mathbf c_1, \mathbf B_{p_2, \mathbf t_2}^\top(x) \mathbf c_2\rangle \\
        &= \sum_{i=1}^m \left(\mathbf B_{p_1, \mathbf t_1}^\top(x) \mathbf c_{1,i}\right) \left(\mathbf B_{p_2, \mathbf t_2}^\top(x) \mathbf c_{2,i}\right) \\
        &= \sum_{i=1}^m \left(\mathbf B_{p_1, \mathbf t_1}^\top(x) \otimes \mathbf B_{p_2, \mathbf t_2}^\top(x)\right) \left(\mathbf c_{1,i} \otimes \mathbf c_{2,i}\right) \\
        &= \left(\mathbf B_{p_1, \mathbf t_1}(x) \otimes \mathbf B_{p_2, \mathbf t_2}(x)\right)^{\top} \sum_{i=1}^m \mathbf c_{1,i} \otimes \mathbf c_{2,i},
    \end{aligned}
\end{equation}
where $\mathbf c_{1,i}$ and $\mathbf c_{2,i}$ represent the B-spline coefficients associated with the $i$-th component of the vector-valued functions $\mathbf f$ and $\mathbf g$, respectively. It is noteworthy that the term $\left(\mathbf B_{p_1, \mathbf t_1}(x) \otimes \mathbf B_{p_2, \mathbf t_2}(x)\right)^{\top}$ appears in both \cref{eq:b-spline-product-4} and \cref{eq:dot-product-spline}. Consequently, the transformation matrix $\mathbf T$ required for computing the inner product can be derived analogously to the multiplication operation, as described in \cref{alg:multiplication}.

So, to find the PW inner product $h(x) = \mathbf f(x) \cdot \mathbf g(x)$, \cref{alg:multiplication} is used on the bases of $\mathbf f$ and $\mathbf g$, and the coefficients $\mathbf d$ of $h(x) = \mathbf B_{q, \boldsymbol{\tau}}^\top(x) \mathbf d$ are given by
\begin{equation}
    \mathbf d = \mathbf T \sum_{i=1}^m (\mathbf c_{1,i} \otimes \mathbf c_{2,i}).
\end{equation}

% As for the regular product, the PW inner product can be made more efficient by not computing the terms in the Kronecker product that correspond to non-overlapping supports of the basis functions. The most common case for applying the PW inner product is between two vector-valued B-splines of the same basis, in which case the number of non-zero entires is $n(p+1)$, where $n$ is the number of basis functions and $p$ is the degree of the B-spline. This is linear in the number of basis functions, instead of quadratic as for when this optimization is not used. See explanation in \cref{fig:overlapping-bases}. Finding a general formula arbitrary bases is not as easily acheivable, as the number of non-zero entries depends on the placement of the knots in the knot vectors.

% \begin{figure}
%     \centering
%     \includesvg[width=\textwidth,pretex=\footnotesize]{fig/b-spline/overlapping-bases.svg}
%     \caption{The overlapping supports of the B-splines. Each rectangle represents the support of a basis function, so counting the number of basis functions withing each knot interval gives the number of multiplications for that interval. The total number of multiplications then the area of all the rectangles, which is $n(p+1)$, where $n$ is the number of basis functions and $p$ is the degree of the B-spline.}
%     \label{fig:overlapping-bases}
% \end{figure}


\subsubsection{Norms and the B-spline Gramian}\label{sec:norms}
The inner product defined in \cref{eq:dot-product} can be used to define a norm of a vector-valued function $\mathbf f(x)$ as
\begin{equation}
    \label{eq:norm}
    \|\mathbf f(x)\|_\mathbf G = \sqrt{\langle\mathbf f(x), \mathbf f(x)\rangle} = \sqrt{\int_{-\infty}^\infty \mathbf f^\top(x) \mathbf f(x)}.
\end{equation}

The norm is a measure of the size of the function, and its square is useful in applications such as optimization, where it can be used as an objective function or a constraint. If the function $f(x)$ is a scalar B-spline, the squared norm can be computed as
\begin{equation}
    \label{eq:norm-spline}
    \begin{aligned}
        \|f\|_\mathbf G^2 &= \langle\mathbf B_{p, \mathbf t}^\top(x) \mathbf c, \mathbf B_{p, \mathbf t}^\top(x) \mathbf c\rangle \\
        &= \int_{-\infty}^\infty \mathbf c^\top \mathbf B_{p, \mathbf t}\mathbf B_{p, \mathbf t}^\top(x) \mathbf c \\
        &= \mathbf c^\top \left(\int_{-\infty}^\infty \mathbf B_{p, \mathbf t}(x) \mathbf B_{p, \mathbf t}^\top(x) \, dx\right) \mathbf c \\
        &= \mathbf c^\top \mathbf G \mathbf c,
\end{aligned}
\end{equation}
where the matrix $\mathbf G$ is the Gramian matrix of the B-spline basis $\mathbf B_{p, \mathbf t}(x)$. The Gramian matrix encodes the autocorrelation  structure of the B-spline basis functions with one of its defining properties being that it is symmetric and positive semi-definite \citep{horn2013positive}. With a basis defined on a regular knot vector, the Gramian can be shown to be symmetric and positive definite \acrshort{SPD}, which means that the norm is well-defined and non-zero for all non-zero B-splines.  Knowing the Gramian of the B-spline basis, the squared norms of the corresponding B-spline can be easily computed. In this work, the $\bf H$-norm $\|\mathbf f\|_\mathbf H$ on a spline function $f: \mathbb R\to\mathbb R$ with $n$ basis functions, will refer to computing the bilinear form
\begin{equation}\label{eq:b-spline-norm}
    \|f\|_\mathbf H^2 = \mathbf{c}^\top \mathbf H \mathbf {c}
\end{equation}
where $\mathbf{c}\in\mathbb R^{n}$ is the coefficients of $\mathbf f$ as in and $\mathbf H\in\mathbb R^{n\times n}$ is a suitible SPD matrix. 



\subsubsection{The B-spline Norms as a Cost Function}\label{sec:b-spline-norms-as-cost-functions}
Considering $\|f\|_\mathbf G^2$ as a cost function in an optimization problem, analyzing its eigenvalues and eigenvectors provides insights into the behavior of the optimal solution.


Using the spectral (eigenvalue) decomposition of the Gramian $\mathbf G = \mathbf U \mathbf \Lambda \mathbf U^\top$ \citep{horn2013positive}, the cost function can be reduced into weighted sum over different frequency components. Here, $\mathbf U$ is a matrix whose columns are the unit-normalized eigenvectors of $\mathbf G$, and $\mathbf \Lambda$ is a diagonal matrix with the eigenvalues of $\mathbf G$ on the diagonal. Thus, the norm can be rewritten as
\begin{equation}\label{eq:cost-function-spectral-decomposition}
    \|f\|_\mathbf G^2 = \mathbf c^\top \mathbf G \mathbf c = \mathbf c^\top \mathbf U \mathbf \Lambda \mathbf U^\top \mathbf c = \sum_{i=1}^{n} \lambda_i (\mathbf u_i^\top \mathbf c)^2,
\end{equation}
where $\lambda_i$ and $\mathbf u_i$ are the $i$-th eigenvalue and eigenvector of $\mathbf G$, respectively. With the cost function in this form, it is clear that the cost is a weighted sum of the squared projections of the coefficients $\mathbf c$ onto the eigenvectors $\mathbf u_i$ of the Gramian $\mathbf G$. The eigenvalues $\lambda_i$ determine the weight of each projection, and thus a cost function based on this norm penalizes the coefficients more heavily if they have a matching projection onto an eigenvector with a large eigenvalue.


\begin{figure}
    \centering
        \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/eigenvectors_integral_degree_2_N_4.svg}
        \caption{Eigenvectors of the Gramian $\mathbf G$ of the cost function $\|f\|_\mathbf G^2$ for $n=4$ and $p=2$. Note that $\hat\lambda_i$ is a scaled version of the eigenvalue $\lambda_i$ such that $\hat\lambda_i = \lambda_i/\sum_{i=1}^{n} \lambda_i$. Values on the vertical axis are ommited as it is the relative size between the eigenvector components that are of interest.}
        \label{fig:conservativeness-eigenvectors-integral}
\end{figure}


As an example, consider the Gramian $\mathbf G$ for a uniform B-spline basis of degree $p=2$ with $n=4$. This can be computed using the relevant formulas in \cref{sec:operations-on-b-splines} to yield
\begin{equation}
    \mathbf G_{2,\mathbf u(2,4)} = \int_\mathbb R \mathbf B_{2,\mathbf u(2,4)}(x) \mathbf B_{2,\mathbf u(2,4)}^\top(x) dx = 
    \frac{1}{120}\begin{bmatrix}
        12 & 7 & 1 & 0 \\
        7 & 20 & 12 & 1 \\
        1 & 12 & 20 & 7 \\
        0 & 1 & 7 & 12
    \end{bmatrix}.
\end{equation}

This matrix has the eigenvectors 
\begin{equation}
    \begin{aligned}
        \mathbf u_1 &\approx \begin{bmatrix}  
            0.23 \\  0.67 \\  0.67 \\  0.23 
        \end{bmatrix}, &
        \mathbf u_2 &\approx \begin{bmatrix}         
            0.57 \\  0.41 \\ -0.41 \\  -0.57 
        \end{bmatrix}, &
        \mathbf u_3 &\approx \begin{bmatrix}         
            0.67 \\  -0.23 \\  -0.23 \\  0.67 
        \end{bmatrix}, &
        \mathbf u_4 &\approx \begin{bmatrix}         
            0.41 \\  -0.57 \\  0.57 \\  -0.41 
        \end{bmatrix},
    \end{aligned}
\end{equation}
with corresponding eigenvalues
\begin{equation}
    \begin{aligned}
        \lambda_1 &= 0.29, &
        \lambda_2 &= 0.14, &
        \lambda_3 &= 0.07. &
        \lambda_4 &= 0.03.
    \end{aligned}
\end{equation}
The $u_1$ direction can be interpreted as a direction where all the coefficients have the same sign, while the $u_4$ direction can be interpreted as a direction where subsequent coefficients alternate in sign. 
As the cost in the $u_1$ direction compared to the $u_4$ direction is $\frac{\lambda_1}{\lambda_4} \approx 10$ times smaller, the cost function will amplify high frequency oscillations in the coefficients. In \cref{fig:conservativeness-eigenvectors-integral}, each eigenvector is shown as a weight on each B-spline coefficient. Here, the relative eigenvalues $\hat\lambda_i = \lambda_i/\sum_{i=1}^{N} \lambda_i$ are shown to better compare the costs between different Hessians. That high frequency directions are amplified is more appearent when increasing the number of B-spline basis functions $n$. This is shown in \cref{fig:integral-basis-eigenvectors-20} for $n=20$.


\begin{figure}
    \centering
    \includesvg[width=\textwidth,pretex=\tiny]{fig/conservativeness/eigenvectors_integral_degree_2_N_20.svg}
    \caption{Eigenvectors of the Hessian $\mathbf H = \mathbf G$ where $\mathbf G$ is the Gramian matrix of a uniform quadratic B-spline with 20 basis functions. Each mark on the x-axis corresponds to a coefficient. The eigenvectors of $\mathbf G^{-1}$ are the same as for $\mathbf G$, but with inverted eigenvalues.}
    \label{fig:integral-basis-eigenvectors-20}
\end{figure}



To illustrate the oscillations using the Gramian, consider the following optimization problem, which is a simple reference-following problem where the OS is constrained to a maximum velocity $v_\text{max}$:
\begin{equation}\label{eq:conservativeness-optimization}
    \begin{aligned}
        \min_{\mathbf c} \quad & J_\text{ref} \\
        \text{subject to} \quad &\mathbf p_\text{OS}(0) = \mathbf p_0, \\
                    &\mathbf p_\text{OS} = \mathbf C^\top\mathbf B(x) , \\
                    &\mathbf p_\text{ref} = \mathbf C_\text{ref}^\top\mathbf B_\text{ref}(x) , \\
                    & \mathbf p_\text{OS}'^\top(x) \mathbf p_\text{OS}'(x) \le v_\text{max}^2.
    \end{aligned}
\end{equation}
Here, $\mathbf p_0$ is the initial position of the OS. The B-spline basis functions $\mathbf B$ and $\mathbf B_\text{ref}$ are defined as uniform B-splines of degree $p$ and 1, respectively, with the parameter domain $x\in[0,1]$. The control points $\mathbf C$ of the OS B-spline function are optimized to minimize the TTE to a reference trajectory $\mathbf p_\text{ref}$, which is defined by the control points $\mathbf C_\text{ref}$.
This optimization problem is then analyzied for different choices of $p$ and number $N$ of uniform B-spline basis functions $\mathbf B$. The resulting trajectories are shown in \cref{fig:conservativeness-traj-integral} with the parameters for the optimization problem set to
\begin{equation}\label{eq:conservativeness-parameters}
    \begin{aligned}
        \mathbf p_0 &= \begin{bmatrix} 1 & 0 \end{bmatrix} \\
        \mathbf B &= \mathbf B_{p, \mathbf u(p,n)} \\
        \mathbf B_\text{ref} &= \mathbf B_{1,\{0, 0, 1, 1\}} \\
        \mathbf c_\text{ref} &= \begin{bmatrix}
            [0 & 0] \\
            [0 & 3]
        \end{bmatrix} \\
        v_\text{max} &= 6, \\
    \end{aligned}
\end{equation}
where $\mathbf u(p,n) = \{\{0\}^{(p)}, \{i/N\}_{i=0}^{N}, \{1\}^{(p)}\}$ is a function that generates the knot vector for a uniform B-spline of degree $p$ with $N$ basis functions. The resulting trajectories are shown in \cref{fig:conservativeness-traj-integral}.

\begin{figure}
    \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/conservativeness_traj_integral_degree_2.svg}
    \caption{Optimal trajectory in \cref{eq:conservativeness-optimization} with parameters \cref{eq:conservativeness-parameters} for $p=2$,$n\in\{3,\ldots,20\}$.}
    \label{fig:conservativeness-traj-integral}
\end{figure}

Choosing $\mathbf H = \mathbf G^{-1}$, the inverse Gramian, suppresses the high frequency oscillations in the coefficients instead, as shown in \cref{fig:conservativeness-traj-inv-gramian}.  
The inverse Gramian has the same eigenvectors as the Gramian, but the eigenvalues are now the reciprocals of the original eigenvalues \citep{horn2013positive}, so the eigen-decomposition of the inverse Gramian is given by
\begin{equation}
    \mathbf G^{-1} = \left(\mathbf U \mathbf \Lambda \mathbf U^\top\right)^{-1} = \mathbf U \mathbf \Lambda^{-1} \mathbf U^\top.
\end{equation}

\begin{figure}
    \centering
    \includesvg[width=\textwidth,pretex=\small]{fig/conservativeness/conservativeness_traj_inv_gramian_degree_2.svg}
    \caption{Optimal trajectory in \cref{eq:conservativeness-optimization} with parameters \cref{eq:conservativeness-parameters} for $N\in\{3,\ldots,20\}$ and where the cost is changed to $J_\text{ref} = \|\mathbf e(x)\|_{\mathbf G^{-1}}^2$.}
    \label{fig:conservativeness-traj-inv-gramian}
\end{figure}


To the authors knowledge, this is the first time that the inverse Gramian has been used as a Hessian in a B-spline-based optimization problem, or that a similar analysis of the eigenvectors and eigenvalues of the Gramian has been performed in the context of B-spline trajectory following. Other cost functions have also been considered, but with poor results in terms of feasibility for the optimization problem in question. A brief overview of these costs is given in \cref{app:failed-reference-following}.



\FloatBarrier
\subsection{Rational B-spline expression to NURBS conversion}


To convert a general rational B-spline
$$
    f(x) = \frac{N(x)}{D(x)},
$$
where $N(x)=\mathbf B_{p_N,\mathbf t_N}^\top\mathbf n$ and $D(x)=\mathbf B_{p_D,\mathbf t_D}^\top\mathbf d$ are each in separate B-spline bases, into the standard NURBS form, one applies degree elevation and knot insertion \citep{Piegl1997} to obtain a common basis ${B_{q,i}(x)}$:
\begin{enumerate}
    \item Elevate both $N$ and $D$ to degree $q\ge\max{p_N,p_D}$ using \cref{alg:degree-elevation}.
    \item Refine knot vectors so that both share the same sequence using \cref{alg:knot-refinement}.
\end{enumerate}
This yields
$$
    \widetilde N(x) = \sum_{i=0}^M \tilde n_i\,N_{q,i}(x),\quad \widetilde D(x) = \sum_{i=0}^M \tilde d_i\,N_{q,i}(x),
$$
from which the NURBS weights and control values follow:
$$
    w_i = \tilde d_i, \quad
    c_i = \frac{\tilde n_i}{\tilde d_i}.
$$
The resulting representation,
$$
    f(x) = \frac{\sum_i w_i\,c_i\,N_{q,i}(x)}{\sum_i w_i\,N_{q,i}(x)},
$$
is a degree-$q$ NURBS with weights $w_i$, knot vector $\boldsymbol{\tau}$ and control points $c_i$. This is summarized in \cref{alg:nurbs-conversion}.

\begin{algorithm}
    \caption{Convert rational B-spline to NURBS}\label{alg:nurbs-conversion}
    \begin{algorithmic}[1]
        \State \textbf{Input:} B-splines $N(x)$ and $D(x)$ with correspondingknot vectors $\mathbf t_N$ and $\mathbf t_D$, and degrees $p_N$ and $p_D$
        \State $q \gets \max(p_N, p_D)$
        \State $\mathbf B_{q,\boldsymbol{\tau}}(x) \gets $ result from \cref{alg:common-basis} with bases $\mathbf B_{p_N,\mathbf t_N}(x)$ and $\mathbf B_{p_D,\mathbf t_D}(x)$ and desired degree $q$
        \State $\mathbf T_N \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_N,\mathbf t_N}(x)$ to $\mathbf B_{q,\boldsymbol{\tau}}(x)$
        \State $\mathbf T_D \gets $ result from \cref{alg:basis-transformation} with transformation from $\mathbf B_{p_D,\mathbf t_D}(x)$ to $\mathbf B_{q,\boldsymbol{\tau}}(x)$
        \State $\mathbf{\tilde n} \gets \mathbf T_N \mathbf n$
        \State $\mathbf{\tilde d} \gets \mathbf T_D \mathbf d$
        \State $\mathbf w \gets \mathbf{\tilde d}$
        \State $\mathbf c \gets \mathbf{\tilde n} \oslash \mathbf{\tilde d}$ \Comment{element-wise division}
        \State \textbf{Output:} Degree $q$ NURBS curve $\displaystyle r(x) = \frac{N(x)}{D(x)}$ with weights $w_i$, knot vector $\boldsymbol{\tau}$ and control points $c_i$
    \end{algorithmic}
\end{algorithm}


\subsection{Summary of B-spline operations}\label{sec:summary-b-spline-operations}
All spline operations are implemented via basis transformations: each input spline is mapped into a chosen target B-spline basis by solving the collocation system at Greville abscissae (Algorithm~\ref{alg:basis-transformation}), producing sparse coefficient mappings. Once in the common basis, differentiation, integration, addition, multiplication, inner products, and rational-to-NURBS conversion reduce to simple linear or bilinear operations on the control coefficients. Although this general collocation-based approach is robust and uniform, specialized closed-form or recursive algorithms exist for each operation (e.g.\ explicit derivative matrices, direct knot insertion, Cox–de Boor recursions) that offer greater computational efficiency.


\Cref{tab:operations} lists the core B-spline operations, their input/output spline spaces, the transformation type applied to the coefficient vectors, and the implementation reference. All operations are realized via sparse linear or bilinear mappings on the control coefficients; rational operations are handled through NURBS conversion.

\renewcommand{\arraystretch}{1.2}
\begin{table}
    \centering
    \small
    \begin{tabular}{|l|l|l|l|c|}
    \hline
    \textbf{Operation} 
      & \textbf{Expression} 
        & \textbf{Space} 
          & \textbf{Coefficients} 
            & \textbf{Implementation} \\
    \hline
    \hline
    Evaluation   
      & $\mathbf f(x)$ 
        & $\mathcal S^n_{p}\to\mathbb{R}^n$ 
          & Linear 
            & \cref{eq:b-spline-recurrence} \\
    \hline
    Derivative   
      & $\mathbf f'$  
        & $\mathcal S^n_{p}\to\mathcal S^n_{p-1}$ 
          & Linear 
            & \cref{eq:b-spline-derivative} \\
    \hline
    Integral     
      & \rule{0pt}{4ex}$\displaystyle\int \mathbf f(x)\,dx$ 
        & $\mathcal S^n_{p}\to\mathcal S^n_{p+1}$ 
          & Linear 
            & \cref{eq:b-spline-integral} \\[1.5ex]
    \hline
    Degree elevation  
      & $\mathbf f$  
        & $\mathcal S^n_{p}\to\mathcal S^n_{p+1}$ 
          & Linear 
            & \Cref{alg:degree-elevation} \\      
    \hline
    Knot insertion    
      & $\mathbf f$  
        & $\mathcal S^n_{p}\to\mathcal S^n_{p}$ 
          & Linear 
            & \Cref{alg:knot-refinement} \\
    \hline
    \hline
    Vector Addition  
      & $\mathbf a + \mathbf f$  
        & $\mathbb R^n \times\mathcal S^n_{p}\to\mathcal S^n_{p}$ 
          & Linear 
            & \Cref{alg:addition} \\
    \hline
    Scalar multiplication  
      & $a\,\mathbf f$  
        & $\mathbb R\times\mathcal S^n_{p}\to\mathcal S^n_{p}$ 
          & Linear 
            & $-$ \\
    \hline
    Addition     
      & $\mathbf f +\mathbf g$  
        & $\mathcal S^n_{p_1}\times\mathcal S^n_{p_2}\to\mathcal S^n_{\max(p_1,p_2)}$ 
          & Linear 
            & \Cref{alg:addition} \\[.5ex]
    \hline
    Multiplication   
      & $\mathbf f\,g$  
        & $\mathcal S^n_{p_1}\times\mathcal S_{p_2}\to\mathcal S^n_{p_1+p_2}$ 
          & Bilinear 
            & \Cref{alg:multiplication} \\
    \hline
    PW inner product  
      & $\mathbf f \cdot \mathbf g$  
        & $\mathcal S^n_{p_1}\times\mathcal S^n_{p_2}\to\mathcal S_{p_1+p_2}$
          & Bilinear 
            & \cref{eq:dot-product-pointwise} \\
    \hline
    Inner product    
      & $\langle \mathbf f, \mathbf g \rangle$  
        & $\mathcal S^n_{p_1}\times\mathcal S^n_{p_2}\to\mathbb R$ 
          & Bilinear 
            & \cref{eq:dot-product} \\
    \hline
    Division  
      & \rule{0pt}{4ex}$\displaystyle\frac{\mathbf f}{g}$  
        & $\mathcal S^n_{p_1}\times\mathcal S_{p_2}\to\mathcal B^n_{\max(p_1,p_2)}$
          & Rational 
            & \Cref{alg:nurbs-conversion} \\[1.5ex]
    \hline
    \end{tabular}
    \caption{Summary of B-spline operations and their properties. The symbols $f$ and $g$ denote B-spline functions, while $a$ denotes a scalar. Their corresponding bold-faced symbols denote vector-valued functions and vectors in $\mathbf R^n$ space respectively. The ``Coefficients'' column indicates the transformation type of the operation on the coefficients. The ``Space'' column indicates the space of the resulting B-spline. The knot vectors for these spaces are ommited for brevity and can be found in the references in the ``Implementation'' column.}
    \label{tab:operations}
\end{table}
\renewcommand{\arraystretch}{1}
    


\FloatBarrier
\subsection{Python Library}\label{sec:python-implementation}

The B-spline MINMPC is implemented in Python using CasADi \citep{casadi} and numpy \citep{numpy}. The code is organized into two components: a B-spline library and a CasADi wrapper. The B-spline library follows the structure of the OMGTools library by \citet{mercy2016spline} with enhancements for type safety, vector-valued spline support, small performance optimizations, and added compatibility with CasADi’s MIP solvers. All B-spline transformations and operations are hidden behind a clean interface, so that spline variables and constraints can be declared on symbolic variables just like native CasADi types.

A single optimization-problem class provides a public interface mirroring CasADi’s Opti while extending it with B-spline variables, constraints, and integer/binary decision variables. 
A key feature of this class is its modular design, enabling the representation of distinct entities such as the OS, TS, and reference trajectories as individual objects. These objects can then be seamlessly integrated into a unified optimization problem.  The subsequent example illustrates the construction of a complete optimization problem using this class, presenting a simplified version of the problem addressed in this paper:

\begin{example}{Optimization Problem Setup}
\begin{python}
from cs_mpc import OptiObject, OptiCollection, BSplineBasis

# Define a spline basis for the own ship
os_basis = BSplineBasis.Uniform(degree=2, n_control_points=12)

# Create own-ship object
own_ship     = OptiObject('own_ship')
os_pos       = own_ship.declare_spline_variable('pos', n_dim=2, basis=os_basis)
os_pos_init  = own_ship.declare_parameter('pos_init',  n_dim=2)
os_pos_final = own_ship.declare_parameter('pos_final',  n_dim=2)

# Boundary constraints
own_ship.declare_constraint(os_pos(0) - os_pos_init, lower_bound=0, upper_bound=0)
own_ship.declare_constraint(os_pos(1) - os_pos_final, lower_bound=0, upper_bound=0)

# Objective: minimize end-point error
end_err = os_pos - os_pos_final
own_ship.declare_objective(end_err.dot(end_err).definite_integral(0,1))

# Create a target-ship object
target_ship = OptiObject('target_ship')
ts_basis    = BSplineBasis.Uniform(degree=1, n_control_points=2)
ts_pos      = target_ship.declare_spline_parameter('pos', n_dim=2, basis=ts_basis)
min_dist    = target_ship.declare_parameter('min_dist', n_dim=1)

# Collision-avoidance constraint
ts_err = os_pos - ts_pos
target_ship.declare_constraint(ts_err.dot(ts_err) - min_dist**2, lower_bound=0)

# Set paramter values and solve
opti = OptiCollection([own_ship, target_ship])
own_ship.set_value('pos_init', [0,0])
own_ship.set_value('pos_final', [1000,0])
target_ship.set_value('pos', [[500,200],[500,-200]])
target_ship.set_value('min_dist', 50)
solution = opti.solve()
\end{python}
\end{example}






\section{Summary}
This chapter presents a unified B-spline MINMPC framework built on four key components:

\begin{enumerate}
  \item \textbf{Double‐integrator spline model}  
    Using the continuous-time double-integrator model
    \begin{equation}
      \dot{\mathbf p}(t)=\mathbf v(t),\quad
      \dot{\mathbf v}(t)=\mathbf a(t),\quad
      \|\mathbf v\|\le v_{\max},\;\|\mathbf a\|\le a_{\max},
    \end{equation}
    a time-parameter spline $t(x)$ (piecewise linear) is introduced so that the B-spline relaxation of the model becomes 
    \begin{equation}
        \begin{aligned}
            \mathbf p(x)&=\sum_i\mathbf p_iB_i(x), \\
            \mathbf p'(x)^\top\mathbf p'(x)&\le (t'(x)v_{\max})^2, \\
            \mathbf p''(x)^\top\mathbf p''(x)&\le (t'(x)^2a_{\max})^2.
        \end{aligned}
    \end{equation}
    All state and timing constraints become polynomial inequalities in the spline coefficients (\cref{sec:double-integrator}).

  \item \textbf{Mixed‐integer COLREGS constraints}  
    Collision avoidance is enforced by separating own‐ship (OS) and target‐ship (TS) via moving hyperplanes. Two hyperplanes per TS encode port/starboard passage, and a binary $z_j\in\{0,1\}$ activates the appropriate side through Big–M inequalities. This MIP formulation allows for handling the non-convexity of the collision constraints in a robust and efficient manner. These constraints are collected in the set $\mathcal{O}_j$, defined in \cref{sec:collision-constraints}, and are applied to the OS B-spline trajectory (\cref{sec:collision-constraints}).

  \item \textbf{Structured cost function}  
    The objective function is designed to minimize a combination of factors, with a novel emphasis on suppressing high-frequency oscillations through the application of an inverse-Gramian norm (\cref{sec:oscillations}). The composite cost function is structured as follows:
        \begin{equation}
          J = J_\text{ref} + J_\text{mv} + J_\text{acc} + J_\text{end} + w_\text{time}\,t(1),
        \end{equation}
        This function comprises four distinct terms:
        \begin{itemize}
          \item \(J_\text{ref}\): A reference-tracking term that penalizes deviations from a desired trajectory.
          \item \(J_\text{mv}\) and \(J_\text{acc}\): Terms that penalize course deviations and accelerations, respectively, within predefined maneuver windows. These penalties are modulated by weighting functions defined as splines, allowing for precise control over the maneuver dynamics.
          \item \(J_\text{end}\): A terminal cost that encourages convergence towards the reference trajectory's endpoint.
          \item \(w_\text{time}\,t(1)\): A minimum-time objective, where the final time \(t(1)\) is penalized by a weight \(w_\text{time}\), promoting faster trajectories.
        \end{itemize}

        The complete B-spline MINMPC problem can then be written compactly as
        \begin{equation}
        \label{eq:minmpc-compact}
            \begin{aligned}
                \min
                \quad & J
                \\
                \text{subject to}\quad
                & \mathcal{D}_{DI}(\mathbf{p}_\text{OS})
                \\
                & \mathcal{O}_j,
                \quad \forall j\in \{1,\ldots,N_\text{TS}\},
            \end{aligned}
        \end{equation}
    where \(\mathcal{D}_{DI}(\mathbf{p}_\text{OS})\) represents the double-integrator spline model constraints, and \(\mathcal{O}_j\) are the collision avoidance constraints for each target ship \(j\).

  \item \textbf{Python implementation}  
    A custom Python/CasADi library encapsulates B-spline operations. Central to this is the basis‐transformation algorithm (\cref{sec:basis-transformation}), which computes coefficient mappings between spline spaces. This underlies differentiation, integration, products, inner‐products (Gramian), and the construction of all spline‐based constraints and costs in a seamless, symbolic manner.
\end{enumerate}